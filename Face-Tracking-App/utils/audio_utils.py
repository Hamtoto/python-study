"""
ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ - FFmpeg ìµœëŒ€ ì„±ëŠ¥ ìµœì í™”
"""
import wave
import subprocess
import webrtcvad
import multiprocessing
import re
from moviepy import AudioFileClip
from config import AUDIO_SAMPLE_RATE, AUDIO_FRAME_DURATION


def get_optimal_cpu_threads():
    """ë™ì  CPU ìŠ¤ë ˆë“œ ìˆ˜ ê³„ì‚° (ì „ì²´ ì½”ì–´ - 1)"""
    total_cores = multiprocessing.cpu_count()
    optimal_threads = max(1, total_cores - 1)  # ìµœì†Œ 1ê°œ, ìµœëŒ€ ì „ì²´-1
    print(f"ğŸ–¥ï¸ CPU ìµœì í™”: {total_cores}ì½”ì–´ ì¤‘ {optimal_threads}ê°œ ì‚¬ìš©")
    return optimal_threads


def get_voice_segments(video_path: str, sample_rate=AUDIO_SAMPLE_RATE, frame_duration=AUDIO_FRAME_DURATION):
    """
    ë¹„ë””ì˜¤ì—ì„œ ìŒì„± êµ¬ê°„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        sample_rate: ì˜¤ë””ì˜¤ ìƒ˜í”Œ ë ˆì´íŠ¸
        frame_duration: í”„ë ˆì„ ì§€ì†ì‹œê°„ (ms)
    
    Returns:
        list: (ì‹œì‘ì‹œê°„, ì¢…ë£Œì‹œê°„) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    wav_path = video_path.replace('.mp4', '_audio.wav')
    
    # ë™ì  CPU ìŠ¤ë ˆë“œ ìˆ˜ ì ìš©
    threads = get_optimal_cpu_threads()
    
    # FFmpeg ìµœëŒ€ ì„±ëŠ¥ ëª…ë ¹ì–´ (ì‹¤ì‹œê°„ ì¶œë ¥)
    ffmpeg_cmd = [
        'ffmpeg', '-y', 
        '-threads', str(threads),  # ë™ì  CPU ìŠ¤ë ˆë“œ
        '-i', video_path, 
        '-ar', str(sample_rate), 
        '-ac', '1',  # ëª¨ë…¸
        '-sample_fmt', 's16',
        '-progress', 'pipe:1',  # ì‹¤ì‹œê°„ ì§„í–‰ë¥ 
        wav_path
    ]
    
    try:
        print(f"ğŸš€ FFmpeg ìµœëŒ€ ì„±ëŠ¥ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œì‘ ({threads}ìŠ¤ë ˆë“œ)")
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
        process = subprocess.Popen(
            ffmpeg_cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        # ì‹¤ì‹œê°„ ì§„í–‰ë¥  íŒŒì‹±
        while True:
            line = process.stdout.readline()
            if not line:
                break
            if 'time=' in line:
                # ì§„í–‰ë¥  í‘œì‹œ (ì„ íƒì )
                time_match = re.search(r'time=(\d+:\d+:\d+\.\d+)', line)
                if time_match:
                    print(f"â³ ì²˜ë¦¬ ì¤‘: {time_match.group(1)}", end='\r')
        
        # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
        return_code = process.wait()
        if return_code == 0:
            print("\nğŸ¯ FFmpeg ìµœëŒ€ ì„±ëŠ¥ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ!")
        else:
            raise subprocess.CalledProcessError(return_code, ffmpeg_cmd)
            
    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:
        # Fallback to MoviePy
        print(f"\nâš ï¸ FFmpeg ì‹¤íŒ¨ ({e}), MoviePy fallbackìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ")
        AudioFileClip(video_path).write_audiofile(wav_path, fps=sample_rate, nbytes=2, codec='pcm_s16le')
    vad = webrtcvad.Vad(3)
    wf = wave.open(wav_path, 'rb')
    frames = wf.readframes(wf.getnframes())
    segment_length = int(sample_rate * frame_duration / 1000) * 2
    voice_times = []
    is_speech = False
    t = 0.0
    for offset in range(0, len(frames), segment_length):
        chunk = frames[offset:offset+segment_length]
        
        # WebRTC VADëŠ” ì •í™•í•œ ì²­í¬ ê¸¸ì´ë¥¼ ìš”êµ¬í•¨ (10ms, 20ms, 30ms)
        if len(chunk) != segment_length:
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° íŒ¨ë”© ë˜ëŠ” ìŠ¤í‚µ
            if len(chunk) < segment_length // 2:  # ì ˆë°˜ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
                break
            # ì ˆë°˜ ì´ìƒì´ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
            chunk = chunk + b'\x00' * (segment_length - len(chunk))
        
        try:
            speech = vad.is_speech(chunk, sample_rate)
        except Exception as e:
            print(f"âš ï¸ VAD í”„ë ˆì„ ìŠ¤í‚µ: {len(chunk)} bytes, ì˜¤ë¥˜: {e}")
            speech = False  # ì˜¤ë¥˜ ì‹œ ìŒì„± ì•„ë‹˜ìœ¼ë¡œ ì²˜ë¦¬
        if speech and not is_speech:
            start = t; is_speech = True
        if not speech and is_speech:
            voice_times.append((start, t)); is_speech = False
        t += frame_duration / 1000
    if is_speech:
        voice_times.append((start, t))
    wf.close()
    return voice_times