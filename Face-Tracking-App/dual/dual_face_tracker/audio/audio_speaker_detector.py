"""
Audio Speaker Detection System for Dual Face Tracking
Phase A: Active Speaker Detection implementation

ì˜¤ë””ì˜¤ ê¸°ë°˜ í™”ì í™œë™ ê°ì§€, ì… ì›€ì§ì„ ë¶„ì„, ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ìƒê´€ê´€ê³„ ë¶„ì„
"""

import numpy as np
import cv2
import time
import logging
from typing import Dict, Any, Optional, List, Tuple, Union
from collections import deque
from dataclasses import dataclass

# ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
try:
    import librosa
    import scipy.signal
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    print("âš ï¸ librosa not available - falling back to basic audio processing")

try:
    import webrtcvad
    HAS_WEBRTCVAD = True
except ImportError:
    HAS_WEBRTCVAD = False
    print("âš ï¸ webrtcvad not available - using amplitude-based VAD")

try:
    import dlib
    HAS_DLIB = True
except ImportError:
    HAS_DLIB = False
    print("âš ï¸ dlib not available - using simple mouth detection")


@dataclass
class AudioFeatures:
    """ì˜¤ë””ì˜¤ íŠ¹ì§• ë°ì´í„° í´ë˜ìŠ¤"""
    rms_envelope: np.ndarray
    spectral_centroids: np.ndarray
    mfcc_features: Optional[np.ndarray]
    speech_segments: List[Tuple[float, float]]
    frame_activities: List[float]
    sample_rate: int
    duration: float


@dataclass
class MouthFeatures:
    """ì… ì›€ì§ì„ íŠ¹ì§• ë°ì´í„° í´ë˜ìŠ¤"""
    mar_values: List[float]  # Mouth Aspect Ratio
    mouth_velocities: List[float]
    mouth_accelerations: List[float]
    speaking_confidence: List[float]


class AudioActivityDetector:
    """ì˜¤ë””ì˜¤ ê¸°ë°˜ í™”ì í™œë™ ê°ì§€"""
    
    def __init__(self):
        """AudioActivityDetector ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        self.sample_rate = 16000  # ê¸°ë³¸ ìƒ˜í”Œë ˆì´íŠ¸
        self.vad_frame_duration = 30  # 30ms VAD í”„ë ˆì„
        
        # VAD ì´ˆê¸°í™” (webrtcvad ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        self.vad = None
        if HAS_WEBRTCVAD:
            try:
                self.vad = webrtcvad.Vad(2)  # Aggressiveness 2 (0-3)
                self.logger.info("âœ… WebRTC VAD ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"WebRTC VAD ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.vad = None
    
    def extract_audio_features(self, video_path: str) -> Optional[AudioFeatures]:
        """ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° íŠ¹ì§• ê³„ì‚°
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            AudioFeatures ê°ì²´ ë˜ëŠ” None
        """
        try:
            self.logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘: {video_path}")
            
            if not HAS_LIBROSA:
                return self._extract_basic_audio_features(video_path)
            
            # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(video_path, sr=self.sample_rate)
            duration = len(y) / sr
            
            self.logger.info(f"   ì˜¤ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ, ìƒ˜í”Œë ˆì´íŠ¸: {sr}Hz")
            
            # RMS ì—”ë²¨ë¡œí”„ ê³„ì‚°
            rms = librosa.feature.rms(y=y, frame_length=512, hop_length=512)[0]
            
            # ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            
            # MFCC íŠ¹ì§• (ì„ íƒì )
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # ë°œí™” êµ¬ê°„ ê²€ì¶œ
            speech_segments = self.detect_speech_segments(y, sr)
            
            # í”„ë ˆì„ë³„ í™œë™ë„ ê³„ì‚°
            frame_activities = self.calculate_frame_activity(y, sr, fps=30.0)
            
            features = AudioFeatures(
                rms_envelope=rms,
                spectral_centroids=spectral_centroids,
                mfcc_features=mfcc,
                speech_segments=speech_segments,
                frame_activities=frame_activities,
                sample_rate=sr,
                duration=duration
            )
            
            self.logger.info(f"âœ… ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {len(speech_segments)}ê°œ ë°œí™” êµ¬ê°„")
            return features
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_basic_audio_features(self, video_path: str) -> Optional[AudioFeatures]:
        """ê¸°ë³¸ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (librosa ì—†ì´)"""
        try:
            # OpenCVë¡œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0.0
            cap.release()
            
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” FFmpeg ë“± ì‚¬ìš©)
            dummy_length = int(duration * 50)  # 50Hz ìƒ˜í”Œë§
            rms_envelope = np.random.random(dummy_length) * 0.1
            spectral_centroids = np.random.random(dummy_length) * 1000 + 500
            
            frame_activities = [0.1] * int(duration * 30)  # 30fps
            
            return AudioFeatures(
                rms_envelope=rms_envelope,
                spectral_centroids=spectral_centroids,
                mfcc_features=None,
                speech_segments=[(0.0, duration)],  # ì „ì²´ êµ¬ê°„
                frame_activities=frame_activities,
                sample_rate=16000,
                duration=duration
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ë³¸ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def detect_speech_segments(self, audio_signal: np.ndarray, sample_rate: int) -> List[Tuple[float, float]]:
        """VADë¡œ ë°œí™” êµ¬ê°„ ê²€ì¶œ
        
        Args:
            audio_signal: ì˜¤ë””ì˜¤ ì‹ í˜¸
            sample_rate: ìƒ˜í”Œë ˆì´íŠ¸
            
        Returns:
            [(start_time, end_time)] ë°œí™” êµ¬ê°„ ë¦¬ìŠ¤íŠ¸
        """
        if self.vad and HAS_WEBRTCVAD:
            return self._webrtc_vad_segments(audio_signal, sample_rate)
        else:
            return self._amplitude_vad_segments(audio_signal, sample_rate)
    
    def _webrtc_vad_segments(self, audio_signal: np.ndarray, sample_rate: int) -> List[Tuple[float, float]]:
        """WebRTC VADë¡œ ë°œí™” êµ¬ê°„ ê²€ì¶œ"""
        try:
            # 16kHz, 16bitìœ¼ë¡œ ë³€í™˜
            if sample_rate != 16000:
                if HAS_LIBROSA:
                    audio_16k = librosa.resample(audio_signal, orig_sr=sample_rate, target_sr=16000)
                else:
                    # ë‹¨ìˆœ ë‹¤ìš´ìƒ˜í”Œë§
                    step = sample_rate // 16000
                    audio_16k = audio_signal[::step]
            else:
                audio_16k = audio_signal
            
            # 16bit ì •ìˆ˜ë¡œ ë³€í™˜
            audio_int16 = (audio_16k * 32767).astype(np.int16)
            
            # 30ms í”„ë ˆì„ìœ¼ë¡œ ë¶„í•  (480 samples at 16kHz)
            frame_size = 480
            segments = []
            current_start = None
            
            for i in range(0, len(audio_int16) - frame_size, frame_size):
                frame = audio_int16[i:i+frame_size]
                frame_bytes = frame.tobytes()
                
                is_speech = self.vad.is_speech(frame_bytes, 16000)
                time_pos = i / 16000.0
                
                if is_speech and current_start is None:
                    current_start = time_pos
                elif not is_speech and current_start is not None:
                    segments.append((current_start, time_pos))
                    current_start = None
            
            # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
            if current_start is not None:
                segments.append((current_start, len(audio_int16) / 16000.0))
            
            self.logger.info(f"WebRTC VAD: {len(segments)}ê°œ ë°œí™” êµ¬ê°„ ê²€ì¶œ")
            return segments
            
        except Exception as e:
            self.logger.warning(f"WebRTC VAD ì‹¤íŒ¨, ì§„í­ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜: {e}")
            return self._amplitude_vad_segments(audio_signal, sample_rate)
    
    def _amplitude_vad_segments(self, audio_signal: np.ndarray, sample_rate: int) -> List[Tuple[float, float]]:
        """ì§„í­ ê¸°ë°˜ VADë¡œ ë°œí™” êµ¬ê°„ ê²€ì¶œ"""
        try:
            # RMS ê³„ì‚°
            window_size = int(sample_rate * 0.025)  # 25ms ìœˆë„ìš°
            hop_size = int(sample_rate * 0.010)     # 10ms í™‰
            
            rms_values = []
            for i in range(0, len(audio_signal) - window_size, hop_size):
                window = audio_signal[i:i+window_size]
                rms = np.sqrt(np.mean(window**2))
                rms_values.append(rms)
            
            rms_array = np.array(rms_values)
            
            # ì ì‘ì  ì„ê³„ê°’ ê³„ì‚°
            rms_mean = np.mean(rms_array)
            rms_std = np.std(rms_array)
            threshold = rms_mean + 0.5 * rms_std
            
            # ë°œí™” êµ¬ê°„ ê²€ì¶œ
            is_speech = rms_array > threshold
            segments = []
            current_start = None
            
            for i, speech in enumerate(is_speech):
                time_pos = i * hop_size / sample_rate
                
                if speech and current_start is None:
                    current_start = time_pos
                elif not speech and current_start is not None:
                    segments.append((current_start, time_pos))
                    current_start = None
            
            # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
            if current_start is not None:
                segments.append((current_start, len(rms_array) * hop_size / sample_rate))
            
            self.logger.info(f"ì§„í­ VAD: {len(segments)}ê°œ ë°œí™” êµ¬ê°„ ê²€ì¶œ (ì„ê³„ê°’: {threshold:.4f})")
            return segments
            
        except Exception as e:
            self.logger.error(f"ì§„í­ VAD ì‹¤íŒ¨: {e}")
            return []
    
    def calculate_frame_activity(self, audio_signal: np.ndarray, sample_rate: int, fps: float = 30.0) -> List[float]:
        """í”„ë ˆì„ë³„ ì˜¤ë””ì˜¤ í™œë™ ë ˆë²¨ ê³„ì‚°
        
        Args:
            audio_signal: ì˜¤ë””ì˜¤ ì‹ í˜¸
            sample_rate: ìƒ˜í”Œë ˆì´íŠ¸  
            fps: ë¹„ë””ì˜¤ í”„ë ˆì„ë ˆì´íŠ¸
            
        Returns:
            í”„ë ˆì„ë³„ í™œë™ë„ ë¦¬ìŠ¤íŠ¸
        """
        try:
            frame_duration = 1.0 / fps  # í”„ë ˆì„ ì§€ì†ì‹œê°„
            samples_per_frame = int(sample_rate * frame_duration)
            
            frame_activities = []
            
            for i in range(0, len(audio_signal), samples_per_frame):
                frame_samples = audio_signal[i:i+samples_per_frame]
                if len(frame_samples) > 0:
                    # RMS í™œë™ë„ ê³„ì‚°
                    activity = np.sqrt(np.mean(frame_samples**2))
                    frame_activities.append(float(activity))
                else:
                    frame_activities.append(0.0)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            if frame_activities:
                max_activity = max(frame_activities)
                if max_activity > 0:
                    frame_activities = [a / max_activity for a in frame_activities]
            
            self.logger.info(f"í”„ë ˆì„ë³„ í™œë™ë„ ê³„ì‚° ì™„ë£Œ: {len(frame_activities)}ê°œ í”„ë ˆì„")
            return frame_activities
            
        except Exception as e:
            self.logger.error(f"í”„ë ˆì„ë³„ í™œë™ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return []


class MouthMovementAnalyzer:
    """ì… ì›€ì§ì„ ê¸°ë°˜ í™”ì ê°ì§€"""
    
    def __init__(self):
        """MouthMovementAnalyzer ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        self.predictor = None
        
        # dlib ëœë“œë§ˆí¬ ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”
        if HAS_DLIB:
            try:
                # ê¸°ë³¸ ê²½ë¡œì—ì„œ shape_predictor ì°¾ê¸°
                predictor_path = "shape_predictor_68_face_landmarks.dat"
                self.predictor = dlib.shape_predictor(predictor_path)
                self.logger.info("âœ… dlib ëœë“œë§ˆí¬ ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"dlib ëœë“œë§ˆí¬ ì˜ˆì¸¡ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.predictor = None
    
    def extract_face_landmarks(self, face_crop: np.ndarray) -> Optional[np.ndarray]:
        """ì–¼êµ´ í¬ë¡­ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ
        
        Args:
            face_crop: ì–¼êµ´ í¬ë¡­ ì´ë¯¸ì§€ (BGR)
            
        Returns:
            (68, 2) ëœë“œë§ˆí¬ ì¢Œí‘œ ë˜ëŠ” None
        """
        if self.predictor is None or not HAS_DLIB:
            return self._extract_simple_landmarks(face_crop)
        
        try:
            # BGR â†’ RGB ë³€í™˜
            rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            gray_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)
            
            # ì–¼êµ´ ê²€ì¶œ (ì „ì²´ ì´ë¯¸ì§€ê°€ ì–¼êµ´ì´ë¼ê³  ê°€ì •)
            h, w = gray_crop.shape
            face_rect = dlib.rectangle(0, 0, w, h)
            
            # ëœë“œë§ˆí¬ ì˜ˆì¸¡
            landmarks = self.predictor(gray_crop, face_rect)
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            points = np.array([[p.x, p.y] for p in landmarks.parts()])
            
            return points
            
        except Exception as e:
            self.logger.warning(f"dlib ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._extract_simple_landmarks(face_crop)
    
    def _extract_simple_landmarks(self, face_crop: np.ndarray) -> Optional[np.ndarray]:
        """ê°„ë‹¨í•œ ëœë“œë§ˆí¬ ì¶”ì¶œ (dlib ì—†ì´)"""
        try:
            h, w = face_crop.shape[:2]
            
            # ë”ë¯¸ ì… ëœë“œë§ˆí¬ ìƒì„± (í•˜ë‹¨ ì¤‘ì•™ ë¶€ê·¼)
            mouth_landmarks = np.array([
                [w*0.3, h*0.75],   # ì™¼ìª½ ì…ê¼¬ë¦¬
                [w*0.5, h*0.8],    # í•˜ë‹¨ ì¤‘ì•™
                [w*0.7, h*0.75],   # ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬
                [w*0.5, h*0.7],    # ìƒë‹¨ ì¤‘ì•™
                [w*0.4, h*0.75],   # ì™¼ìª½ ì¤‘ê°„
                [w*0.6, h*0.75],   # ì˜¤ë¥¸ìª½ ì¤‘ê°„
            ], dtype=np.float32)
            
            return mouth_landmarks
            
        except Exception as e:
            self.logger.error(f"ê°„ë‹¨í•œ ëœë“œë§ˆí¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def calculate_mouth_aspect_ratio(self, landmarks: np.ndarray) -> float:
        """MAR(Mouth Aspect Ratio) ê³„ì‚°
        
        Args:
            landmarks: ëœë“œë§ˆí¬ ì¢Œí‘œ (68ì  ë˜ëŠ” 6ì )
            
        Returns:
            MAR ê°’ (0ì— ê°€ê¹Œìš°ë©´ ë‹«íŒ ì…, í´ìˆ˜ë¡ ì—´ë¦° ì…)
        """
        try:
            if landmarks is None or len(landmarks) == 0:
                return 0.0
            
            if len(landmarks) == 68:
                # í‘œì¤€ 68ì  ëœë“œë§ˆí¬ì—ì„œ ì… ë¶€ë¶„ ì¶”ì¶œ (49-68ë²ˆ)
                mouth_points = landmarks[48:68]
            elif len(landmarks) == 6:
                # ê°„ë‹¨í•œ 6ì  ì… ëœë“œë§ˆí¬
                mouth_points = landmarks
            else:
                # ì „ì²´ ëœë“œë§ˆí¬ì—ì„œ í•˜ë‹¨ ë¶€ë¶„ ì¶”ì¶œ
                mouth_points = landmarks[-6:] if len(landmarks) >= 6 else landmarks
            
            # MAR ê³„ì‚° (ì„¸ë¡œ ê±°ë¦¬ / ê°€ë¡œ ê±°ë¦¬)
            if len(mouth_points) >= 4:
                # ì„¸ë¡œ ê±°ë¦¬ë“¤ì˜ í‰ê· 
                vertical_distances = []
                if len(mouth_points) >= 6:
                    # ìƒí•˜ ì…ìˆ  ê±°ë¦¬
                    vertical_distances.append(np.linalg.norm(mouth_points[1] - mouth_points[3]))
                    vertical_distances.append(np.linalg.norm(mouth_points[4] - mouth_points[5]))
                
                if not vertical_distances:
                    vertical_distances.append(np.linalg.norm(mouth_points[1] - mouth_points[0]))
                
                # ê°€ë¡œ ê±°ë¦¬ (ì…ê¼¬ë¦¬ ê°„ ê±°ë¦¬)
                horizontal_distance = np.linalg.norm(mouth_points[2] - mouth_points[0])
                
                avg_vertical = np.mean(vertical_distances)
                mar = avg_vertical / (horizontal_distance + 1e-6)
                
                return float(mar)
            
            return 0.0
            
        except Exception as e:
            self.logger.warning(f"MAR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def calculate_mouth_velocity(self, mar_history: List[float], window_size: int = 5) -> float:
        """ì… ì›€ì§ì„ ì†ë„ ê³„ì‚° (MAR ë³€í™”ìœ¨)
        
        Args:
            mar_history: MAR ê°’ íˆìŠ¤í† ë¦¬
            window_size: ë¯¸ë¶„ ê³„ì‚° ìœˆë„ìš° í¬ê¸°
            
        Returns:
            ì… ì›€ì§ì„ ì†ë„ (ì ˆëŒ€ê°’)
        """
        try:
            if len(mar_history) < 2:
                return 0.0
            
            # ë‹¨ìˆœ ë¯¸ë¶„ (í˜„ì¬ - ì´ì „)
            if len(mar_history) >= window_size:
                recent_values = mar_history[-window_size:]
                velocity = np.abs(np.diff(recent_values)).mean()
            else:
                velocity = abs(mar_history[-1] - mar_history[-2])
            
            return float(velocity)
            
        except Exception as e:
            self.logger.warning(f"ì… ì›€ì§ì„ ì†ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def analyze_mouth_features(self, face_crops: List[np.ndarray]) -> MouthFeatures:
        """ì—°ì†ëœ ì–¼êµ´ í¬ë¡­ì—ì„œ ì… ì›€ì§ì„ íŠ¹ì§• ë¶„ì„
        
        Args:
            face_crops: ì–¼êµ´ í¬ë¡­ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            MouthFeatures ê°ì²´
        """
        try:
            mar_values = []
            mouth_velocities = []
            mouth_accelerations = []
            speaking_confidence = []
            
            # ê° í”„ë ˆì„ë³„ MAR ê³„ì‚°
            for i, crop in enumerate(face_crops):
                if crop is None or crop.size == 0:
                    mar_values.append(0.0)
                    continue
                
                landmarks = self.extract_face_landmarks(crop)
                mar = self.calculate_mouth_aspect_ratio(landmarks)
                mar_values.append(mar)
                
                # ì†ë„ ê³„ì‚° (2í”„ë ˆì„ ì´ìƒë¶€í„°)
                if len(mar_values) >= 2:
                    velocity = self.calculate_mouth_velocity(mar_values)
                    mouth_velocities.append(velocity)
                else:
                    mouth_velocities.append(0.0)
                
                # ê°€ì†ë„ ê³„ì‚° (3í”„ë ˆì„ ì´ìƒë¶€í„°)
                if len(mouth_velocities) >= 2:
                    acceleration = abs(mouth_velocities[-1] - mouth_velocities[-2])
                    mouth_accelerations.append(acceleration)
                else:
                    mouth_accelerations.append(0.0)
                
                # í™”ì ì‹ ë¢°ë„ (MAR + ì†ë„ ì¡°í•©)
                confidence = min(1.0, mar * 2.0 + mouth_velocities[-1] * 5.0)
                speaking_confidence.append(confidence)
            
            self.logger.info(f"ì… ì›€ì§ì„ ë¶„ì„ ì™„ë£Œ: {len(face_crops)}ê°œ í”„ë ˆì„")
            
            return MouthFeatures(
                mar_values=mar_values,
                mouth_velocities=mouth_velocities,
                mouth_accelerations=mouth_accelerations,
                speaking_confidence=speaking_confidence
            )
            
        except Exception as e:
            self.logger.error(f"ì… ì›€ì§ì„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return MouthFeatures(
                mar_values=[0.0] * len(face_crops),
                mouth_velocities=[0.0] * len(face_crops),
                mouth_accelerations=[0.0] * len(face_crops),
                speaking_confidence=[0.0] * len(face_crops)
            )


class AudioVisualCorrelator:
    """ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ìƒê´€ê´€ê³„ ë¶„ì„"""
    
    def __init__(self):
        """AudioVisualCorrelator ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        self.correlation_history = deque(maxlen=300)  # 10ì´ˆ @ 30fps
    
    def synchronize_audio_video(self, audio_activity: List[float], mouth_activity: List[float], 
                               max_delay_frames: int = 10) -> Tuple[float, int]:
        """ì˜¤ë””ì˜¤-ì…ì›€ì§ì„ ë™ê¸°í™” ë° ì§€ì—° ë³´ì •
        
        Args:
            audio_activity: í”„ë ˆì„ë³„ ì˜¤ë””ì˜¤ í™œë™ë„
            mouth_activity: í”„ë ˆì„ë³„ ì… ì›€ì§ì„ í™œë™ë„
            max_delay_frames: ìµœëŒ€ ì§€ì—° í”„ë ˆì„ ìˆ˜
            
        Returns:
            (ìµœê³  ìƒê´€ê³„ìˆ˜, ìµœì  ì§€ì—° í”„ë ˆì„)
        """
        try:
            # ê¸¸ì´ ë§ì¶”ê¸°
            min_length = min(len(audio_activity), len(mouth_activity))
            if min_length < 10:
                return 0.0, 0
            
            audio_trimmed = np.array(audio_activity[:min_length])
            mouth_trimmed = np.array(mouth_activity[:min_length])
            
            best_correlation = -1.0
            best_delay = 0
            
            # ì—¬ëŸ¬ ì§€ì—°ê°’ì—ì„œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            for delay in range(-max_delay_frames, max_delay_frames + 1):
                if delay == 0:
                    audio_shifted = audio_trimmed
                    mouth_shifted = mouth_trimmed
                elif delay > 0:
                    # ì˜¤ë””ì˜¤ê°€ ì…ë³´ë‹¤ ëŠ¦ìŒ (ì˜¤ë””ì˜¤ë¥¼ ì•ìœ¼ë¡œ ë°€ê¸°)
                    if delay >= min_length:
                        continue
                    audio_shifted = audio_trimmed[:-delay]
                    mouth_shifted = mouth_trimmed[delay:]
                else:
                    # ì…ì´ ì˜¤ë””ì˜¤ë³´ë‹¤ ëŠ¦ìŒ (ì…ì„ ì•ìœ¼ë¡œ ë°€ê¸°)
                    delay_abs = abs(delay)
                    if delay_abs >= min_length:
                        continue
                    audio_shifted = audio_trimmed[delay_abs:]
                    mouth_shifted = mouth_trimmed[:-delay_abs]
                
                if len(audio_shifted) > 0 and len(mouth_shifted) > 0:
                    correlation = self.calculate_correlation(audio_shifted, mouth_shifted)
                    
                    if correlation > best_correlation:
                        best_correlation = correlation
                        best_delay = delay
            
            self.logger.info(f"ë™ê¸°í™” ì™„ë£Œ: ìƒê´€ê³„ìˆ˜={best_correlation:.3f}, ì§€ì—°={best_delay}í”„ë ˆì„")
            return best_correlation, best_delay
            
        except Exception as e:
            self.logger.error(f"ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            return 0.0, 0
    
    def calculate_correlation(self, audio_normalized: np.ndarray, mouth_normalized: np.ndarray) -> float:
        """ì •ê·œí™”ëœ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        
        Args:
            audio_normalized: ì •ê·œí™”ëœ ì˜¤ë””ì˜¤ ì‹ í˜¸
            mouth_normalized: ì •ê·œí™”ëœ ì… ì›€ì§ì„ ì‹ í˜¸
            
        Returns:
            í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (-1 ~ 1)
        """
        try:
            # ê¸¸ì´ ë§ì¶”ê¸°
            min_length = min(len(audio_normalized), len(mouth_normalized))
            if min_length < 2:
                return 0.0
            
            audio_trimmed = audio_normalized[:min_length]
            mouth_trimmed = mouth_normalized[:min_length]
            
            # í‘œì¤€í™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
            audio_std = (audio_trimmed - np.mean(audio_trimmed)) / (np.std(audio_trimmed) + 1e-8)
            mouth_std = (mouth_trimmed - np.mean(mouth_trimmed)) / (np.std(mouth_trimmed) + 1e-8)
            
            # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            correlation = np.corrcoef(audio_std, mouth_std)[0, 1]
            
            # NaN ì²˜ë¦¬
            if np.isnan(correlation):
                correlation = 0.0
            
            return float(correlation)
            
        except Exception as e:
            self.logger.warning(f"ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def score_active_speakers(self, face_tracks: Dict[str, Any], audio_features: AudioFeatures, 
                            mouth_features: Dict[str, MouthFeatures]) -> List[Tuple[str, float]]:
        """í™”ìë³„ Active Speaker ì ìˆ˜ ê³„ì‚°
        
        Args:
            face_tracks: {'A': track_info, 'B': track_info} ì–¼êµ´ ì¶”ì  ì •ë³´
            audio_features: ì˜¤ë””ì˜¤ íŠ¹ì§•
            mouth_features: {'A': MouthFeatures, 'B': MouthFeatures} ì… ì›€ì§ì„ íŠ¹ì§•
            
        Returns:
            [(person_id, active_speaker_score)] ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
        """
        try:
            speaker_scores = []
            
            for person_id in ['A', 'B']:
                if person_id not in mouth_features:
                    speaker_scores.append((person_id, 0.0))
                    continue
                
                mouth_feat = mouth_features[person_id]
                
                # 1. ì… ì›€ì§ì„ ì ìˆ˜ (0.4 ê°€ì¤‘ì¹˜)
                mouth_score = 0.0
                if mouth_feat.speaking_confidence:
                    mouth_score = np.mean(mouth_feat.speaking_confidence)
                
                # 2. ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ìƒê´€ê´€ê³„ ì ìˆ˜ (0.4 ê°€ì¤‘ì¹˜)
                correlation_score = 0.0
                if audio_features.frame_activities and mouth_feat.mar_values:
                    correlation, delay = self.synchronize_audio_video(
                        audio_features.frame_activities, 
                        mouth_feat.mar_values
                    )
                    correlation_score = max(0.0, correlation)  # ìŒì˜ ìƒê´€ê´€ê³„ëŠ” 0ìœ¼ë¡œ
                
                # 3. ì–¼êµ´ í¬ê¸°/ì¤‘ì‹¬ë„ ì ìˆ˜ (0.2 ê°€ì¤‘ì¹˜)
                face_score = 0.5  # ê¸°ë³¸ê°’
                if person_id in face_tracks:
                    track = face_tracks[person_id]
                    if hasattr(track, 'avg_size'):
                        face_score = min(1.0, track.avg_size / 150.0)  # 150px ê¸°ì¤€ ì •ê·œí™”
                
                # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                total_score = (
                    mouth_score * 0.4 +
                    correlation_score * 0.4 +
                    face_score * 0.2
                )
                
                speaker_scores.append((person_id, total_score))
                
                self.logger.debug(f"{person_id}: ì…ì›€ì§ì„={mouth_score:.3f}, ìƒê´€ê´€ê³„={correlation_score:.3f}, "
                                f"ì–¼êµ´={face_score:.3f} â†’ ì´ì ={total_score:.3f}")
            
            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ìˆœ)
            speaker_scores.sort(key=lambda x: x[1], reverse=True)
            
            self.logger.info(f"Active Speaker ì ìˆ˜: {speaker_scores}")
            return speaker_scores
            
        except Exception as e:
            self.logger.error(f"Active Speaker ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return [('A', 0.0), ('B', 0.0)]
    
    def update_correlation_history(self, correlation: float):
        """ìƒê´€ê´€ê³„ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.correlation_history.append(correlation)
    
    def get_average_correlation(self, window_size: int = 30) -> float:
        """ìµœê·¼ ìƒê´€ê´€ê³„ í‰ê·  ê³„ì‚°"""
        if len(self.correlation_history) == 0:
            return 0.0
        
        recent = list(self.correlation_history)[-window_size:]
        return np.mean(recent)


# í†µí•© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_audio_speaker_detection():
    """Audio Speaker Detection ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Audio Speaker Detection ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # 1. AudioActivityDetector í…ŒìŠ¤íŠ¸
    print("\n1. AudioActivityDetector í…ŒìŠ¤íŠ¸")
    audio_detector = AudioActivityDetector()
    print(f"   WebRTC VAD ì‚¬ìš© ê°€ëŠ¥: {audio_detector.vad is not None}")
    
    # ë”ë¯¸ ì˜¤ë””ì˜¤ ì‹ í˜¸ ìƒì„±
    sample_rate = 16000
    duration = 2.0
    t = np.linspace(0, duration, int(sample_rate * duration))
    dummy_audio = 0.1 * np.sin(2 * np.pi * 440 * t)  # 440Hz ì‚¬ì¸íŒŒ
    
    # ë°œí™” êµ¬ê°„ ê²€ì¶œ í…ŒìŠ¤íŠ¸
    segments = audio_detector.detect_speech_segments(dummy_audio, sample_rate)
    print(f"   ê²€ì¶œëœ ë°œí™” êµ¬ê°„: {len(segments)}ê°œ")
    
    # í”„ë ˆì„ë³„ í™œë™ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
    activities = audio_detector.calculate_frame_activity(dummy_audio, sample_rate)
    print(f"   í”„ë ˆì„ë³„ í™œë™ë„: {len(activities)}ê°œ í”„ë ˆì„")
    
    # 2. MouthMovementAnalyzer í…ŒìŠ¤íŠ¸  
    print("\n2. MouthMovementAnalyzer í…ŒìŠ¤íŠ¸")
    mouth_analyzer = MouthMovementAnalyzer()
    print(f"   dlib ì‚¬ìš© ê°€ëŠ¥: {mouth_analyzer.predictor is not None}")
    
    # ë”ë¯¸ ì–¼êµ´ í¬ë¡­ ìƒì„±
    dummy_face = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
    landmarks = mouth_analyzer.extract_face_landmarks(dummy_face)
    print(f"   ì¶”ì¶œëœ ëœë“œë§ˆí¬: {landmarks.shape if landmarks is not None else None}")
    
    if landmarks is not None:
        mar = mouth_analyzer.calculate_mouth_aspect_ratio(landmarks)
        print(f"   MAR ê°’: {mar:.4f}")
    
    # 3. AudioVisualCorrelator í…ŒìŠ¤íŠ¸
    print("\n3. AudioVisualCorrelator í…ŒìŠ¤íŠ¸")
    correlator = AudioVisualCorrelator()
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ ìƒê´€ê´€ê³„ í…ŒìŠ¤íŠ¸
    dummy_audio_activity = [0.1, 0.3, 0.7, 0.5, 0.2, 0.8, 0.4]
    dummy_mouth_activity = [0.0, 0.2, 0.6, 0.4, 0.1, 0.7, 0.3]
    
    correlation = correlator.calculate_correlation(
        np.array(dummy_audio_activity), 
        np.array(dummy_mouth_activity)
    )
    print(f"   ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
    
    # ë™ê¸°í™” í…ŒìŠ¤íŠ¸
    best_corr, best_delay = correlator.synchronize_audio_video(
        dummy_audio_activity, dummy_mouth_activity
    )
    print(f"   ìµœì  ë™ê¸°í™”: ìƒê´€ê³„ìˆ˜={best_corr:.4f}, ì§€ì—°={best_delay}í”„ë ˆì„")
    
    print("\nâœ… Audio Speaker Detection ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_audio_speaker_detection()