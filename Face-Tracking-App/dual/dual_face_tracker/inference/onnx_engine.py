"""
ONNX Runtime inference engine for optimized GPU acceleration.

ì´ ëª¨ë“ˆì€ ONNX Runtimeì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ GPU ì¶”ë¡  ì—”ì§„ì„ ì œê³µí•©ë‹ˆë‹¤.
RTX 5090ì—ì„œ CUDA Execution Providerë¥¼ í†µí•´ ìµœì í™”ëœ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import time
import psutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import numpy as np

try:
    import onnxruntime as ort
    ORT_AVAILABLE = True
except ImportError:
    ORT_AVAILABLE = False

from ..utils.logger import UnifiedLogger
from ..utils.exceptions import InferenceError, ModelLoadError


class ONNXRuntimeEngine:
    """
    ONNX Runtime GPU ì¶”ë¡  ì—”ì§„.
    
    Context7 best practicesë¥¼ ì ìš©í•œ ê³ ì„±ëŠ¥ GPU ì¶”ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤.
    CUDAExecutionProviderë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë©°, CPU fallbackì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self, 
        model_path: Union[str, Path],
        providers: Optional[List[str]] = None,
        provider_options: Optional[Dict] = None,
        enable_profiling: bool = False,
        enable_optimization: bool = True
    ):
        """
        ONNX Runtime ì—”ì§„ ì´ˆê¸°í™”.
        
        Args:
            model_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            providers: ì‹¤í–‰ ì œê³µìž ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: CUDA â†’ CPU)
            provider_options: ì œê³µìžë³„ ì˜µì…˜ ì„¤ì •
            enable_profiling: í”„ë¡œíŒŒì¼ë§ í™œì„±í™” ì—¬ë¶€
            enable_optimization: ëª¨ë¸ ìµœì í™” í™œì„±í™” ì—¬ë¶€
        """
        self.logger = UnifiedLogger("ONNXRuntimeEngine")
        
        if not ORT_AVAILABLE:
            raise ImportError("onnxruntime not available. Please install onnxruntime-gpu")
        
        self.model_path = Path(model_path)
        if not self.model_path.exists():
            raise ModelLoadError(f"Model file not found: {self.model_path}")
        
        # ê¸°ë³¸ ì œê³µìž ì„¤ì •
        if providers is None:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        
        self.providers = providers
        self.provider_options = provider_options or {}
        self.enable_profiling = enable_profiling
        self.enable_optimization = enable_optimization
        
        # ì„¸ì…˜ ë³€ìˆ˜
        self.session: Optional[ort.InferenceSession] = None
        self.input_names: List[str] = []
        self.output_names: List[str] = []
        self.input_shapes: Dict[str, Tuple[int, ...]] = {}
        self.output_shapes: Dict[str, Tuple[int, ...]] = {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.warmup_completed = False
        
        # ì´ˆê¸°í™”
        self._initialize_session()
        self.logger.success(f"ONNXRuntimeEngine initialized: {self.model_path.name}")
    
    def _initialize_session(self):
        """ONNX Runtime ì„¸ì…˜ ì´ˆê¸°í™”."""
        try:
            # Context7 best practices: ì„¸ì…˜ ì˜µì…˜ ìµœì í™”
            sess_options = ort.SessionOptions()
            sess_options.intra_op_num_threads = psutil.cpu_count(logical=True)
            
            if self.enable_optimization:
                # ê·¸ëž˜í”„ ìµœì í™” ë ˆë²¨ ì„¤ì • (99: ëª¨ë“  ìµœì í™”)
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                # ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™”
                sess_options.enable_mem_pattern = True
                sess_options.enable_mem_reuse = True
            
            if self.enable_profiling:
                sess_options.enable_profiling = True
                prof_file = f"onnx_profile_{int(time.time())}.json"
                sess_options.profile_file_prefix = prof_file
                self.logger.debug(f"Profiling enabled: {prof_file}")
            
            # ì œê³µìž ê°€ìš©ì„± í™•ì¸
            available_providers = ort.get_available_providers()
            self.logger.debug(f"Available providers: {available_providers}")
            
            # ìš”ì²­ëœ ì œê³µìž ì¤‘ ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒë§Œ í•„í„°ë§
            filtered_providers = []
            for provider in self.providers:
                if provider in available_providers:
                    filtered_providers.append(provider)
                    self.logger.debug(f"âœ… Provider available: {provider}")
                else:
                    self.logger.warning(f"âš ï¸ Provider not available: {provider}")
            
            if not filtered_providers:
                raise InferenceError("No valid execution providers available")
            
            # ì„¸ì…˜ ìƒì„±
            self.session = ort.InferenceSession(
                str(self.model_path),
                sess_options=sess_options,
                providers=filtered_providers
            )
            
            # ìž…ì¶œë ¥ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            self._collect_model_metadata()
            
            # ì‹¤ì œ ì‚¬ìš©ëœ ì œê³µìž ë¡œê¹…
            used_providers = self.session.get_providers()
            self.logger.stage(f"Active providers: {used_providers}")
            
            # CUDA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if 'CUDAExecutionProvider' in used_providers:
                self.logger.success("ðŸš€ CUDA acceleration enabled")
            else:
                self.logger.warning("ðŸ’» Running on CPU only")
                
        except Exception as e:
            self.logger.error(f"Session initialization failed: {e}")
            raise InferenceError(f"Failed to initialize ONNX Runtime session: {e}")
    
    def _collect_model_metadata(self):
        """ëª¨ë¸ ìž…ì¶œë ¥ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘."""
        # ìž…ë ¥ ì •ë³´
        for input_meta in self.session.get_inputs():
            name = input_meta.name
            shape = input_meta.shape
            self.input_names.append(name)
            self.input_shapes[name] = shape
            
        # ì¶œë ¥ ì •ë³´  
        for output_meta in self.session.get_outputs():
            name = output_meta.name
            shape = output_meta.shape
            self.output_names.append(name)
            self.output_shapes[name] = shape
            
        self.logger.debug(f"Model inputs: {self.input_shapes}")
        self.logger.debug(f"Model outputs: {self.output_shapes}")
    
    def warmup(self, num_runs: int = 3) -> float:
        """
        ëª¨ë¸ ì›Œë°ì—…ìœ¼ë¡œ ì´ˆê¸° ì§€ì—°ì‹œê°„ ì œê±°.
        
        Args:
            num_runs: ì›Œë°ì—… ì‹¤í–‰ íšŸìˆ˜
            
        Returns:
            í‰ê·  ì›Œë°ì—… ì‹œê°„ (ms)
        """
        if self.warmup_completed:
            self.logger.debug("Warmup already completed")
            return 0.0
            
        self.logger.stage("ðŸ”¥ Starting model warmup...")
        
        # ë”ë¯¸ ìž…ë ¥ ìƒì„±
        dummy_inputs = {}
        for name, shape in self.input_shapes.items():
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì²˜ë¦¬ (ì²« ë²ˆì§¸ ì°¨ì›ì´ -1ì´ë©´ 1ë¡œ ì„¤ì •)
            actual_shape = [1 if dim == -1 else dim for dim in shape]
            dummy_inputs[name] = np.random.randn(*actual_shape).astype(np.float32)
        
        warmup_times = []
        
        for i in range(num_runs):
            start_time = time.time()
            
            try:
                _ = self.session.run(self.output_names, dummy_inputs)
                elapsed = (time.time() - start_time) * 1000  # ms
                warmup_times.append(elapsed)
                self.logger.debug(f"Warmup {i+1}/{num_runs}: {elapsed:.2f}ms")
                
            except Exception as e:
                self.logger.error(f"Warmup failed at run {i+1}: {e}")
                raise InferenceError(f"Model warmup failed: {e}")
        
        avg_warmup_time = np.mean(warmup_times)
        self.warmup_completed = True
        
        self.logger.success(f"âœ… Warmup completed: {avg_warmup_time:.2f}ms average")
        return avg_warmup_time
    
    def run(
        self, 
        inputs: Dict[str, np.ndarray],
        output_names: Optional[List[str]] = None
    ) -> Dict[str, np.ndarray]:
        """
        ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰.
        
        Args:
            inputs: ìž…ë ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ {input_name: numpy_array}
            output_names: ë°˜í™˜í•  ì¶œë ¥ ì´ë¦„ë“¤ (Noneì´ë©´ ëª¨ë“  ì¶œë ¥)
            
        Returns:
            ì¶œë ¥ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ {output_name: numpy_array}
        """
        if self.session is None:
            raise InferenceError("Session not initialized")
        
        # ìž…ë ¥ ê²€ì¦
        self._validate_inputs(inputs)
        
        # ì¶œë ¥ ì´ë¦„ ê²°ì •
        if output_names is None:
            output_names = self.output_names
        
        start_time = time.time()
        
        try:
            # ì¶”ë¡  ì‹¤í–‰
            raw_outputs = self.session.run(output_names, inputs)
            
            # ì¶œë ¥ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
            outputs = {}
            for i, name in enumerate(output_names):
                outputs[name] = raw_outputs[i]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            inference_time = (time.time() - start_time) * 1000  # ms
            self.inference_count += 1
            self.total_inference_time += inference_time
            
            return outputs
            
        except Exception as e:
            self.logger.error(f"Inference failed: {e}")
            raise InferenceError(f"Model inference failed: {e}")
    
    def _validate_inputs(self, inputs: Dict[str, np.ndarray]):
        """ìž…ë ¥ ë°ì´í„° ê²€ì¦."""
        for name in self.input_names:
            if name not in inputs:
                raise InferenceError(f"Missing required input: {name}")
            
            input_array = inputs[name]
            expected_shape = self.input_shapes[name]
            
            # ë™ì  ë°°ì¹˜ í¬ê¸° ê³ ë ¤í•œ shape ê²€ì¦
            if len(input_array.shape) != len(expected_shape):
                raise InferenceError(
                    f"Input '{name}' shape mismatch: "
                    f"expected {len(expected_shape)}D, got {len(input_array.shape)}D"
                )
            
            # ê° ì°¨ì› ê²€ì¦ (ë™ì  ì°¨ì› -1ì€ ì œì™¸)
            for i, (actual, expected) in enumerate(zip(input_array.shape, expected_shape)):
                if expected != -1 and actual != expected:
                    raise InferenceError(
                        f"Input '{name}' dimension {i} mismatch: "
                        f"expected {expected}, got {actual}"
                    )
    
    @property
    def average_inference_time(self) -> float:
        """í‰ê·  ì¶”ë¡  ì‹œê°„ (ms) ë°˜í™˜."""
        if self.inference_count == 0:
            return 0.0
        return self.total_inference_time / self.inference_count
    
    @property 
    def fps(self) -> float:
        """ì´ˆë‹¹ í”„ë ˆìž„ ìˆ˜ ê³„ì‚°."""
        if self.average_inference_time == 0:
            return 0.0
        return 1000.0 / self.average_inference_time
    
    def get_performance_stats(self) -> Dict[str, Union[int, float]]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜."""
        return {
            'inference_count': self.inference_count,
            'total_time_ms': self.total_inference_time,
            'average_time_ms': self.average_inference_time,
            'fps': self.fps,
            'warmup_completed': self.warmup_completed,
            'providers': self.session.get_providers() if self.session else []
        }
    
    def reset_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì´ˆê¸°í™”."""
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.logger.debug("Performance statistics reset")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬."""
        if self.session is not None:
            self.session = None
        self.logger.debug("ONNXRuntimeEngine closed")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()