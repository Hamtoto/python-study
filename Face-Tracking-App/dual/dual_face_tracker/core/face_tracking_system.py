#!/usr/bin/env python3
"""
Phase 5: Dual-Face Tracking System
ì™„ì „í•œ ì–¼êµ´ ê²€ì¶œ, ì¶”ì , í¬ë¡­ íŒŒì´í”„ë¼ì¸

ê¸°ëŠ¥:
- 2ëª…ì˜ ì–¼êµ´ ê²€ì¶œ ë° ID í• ë‹¹
- ì‹¤ì‹œê°„ ì–¼êµ´ ì¶”ì  (OpenCV CSRT)
- ì–¼êµ´ ì¤‘ì‹¬ ê¸°ì¤€ í¬ë¡­ (2.5ë°° ë§ˆì§„)
- 1920x1080 ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ì¶œë ¥

Author: Dual-Face System v5.0
Date: 2025.08.16
"""

import argparse
import sys
import time
import os
import subprocess
import tempfile
from pathlib import Path
from typing import Tuple, Optional, List, Dict, Any
import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent / "src"))

# AutoSpeakerDetector import
from .auto_speaker_detector import AutoSpeakerDetector

# GPU ì„¤ì • ë° ë”¥ëŸ¬ë‹ ëª¨ë¸
import torch
import torch.nn.functional as F
from torchvision import transforms
if torch.cuda.is_available():
    torch.cuda.set_device(0)
    print(f"ğŸ–¥ï¸ GPU ì„¤ì •: {torch.cuda.get_device_name(0)}")

# ì„¤ì • import
from ..config.dual_config import (
    PRESERVE_AUDIO, ENABLE_FFMPEG_POST_PROCESSING,
    TRIM_UNDETECTED_SEGMENTS, UNDETECTED_THRESHOLD_SECONDS, 
    TRIM_BUFFER_SECONDS, REQUIRE_BOTH_PERSONS,
    FFMPEG_PRESET, FFMPEG_CRF, VIDEO_CODEC, AUDIO_CODEC
)

# í”„ë¡œì íŠ¸ ëª¨ë¸ import (conditional) 
MODEL_MANAGER_AVAILABLE = True
print("âœ… DUAL ë…ë¦½ ë²„ì „ ëª¨ë“œ (ModelManager í™œì„±í™”)")


class FaceDetection:
    """ì–¼êµ´ ê²€ì¶œ ê²°ê³¼ (ì„ë² ë”© ì§€ì›)"""
    def __init__(self, bbox: Tuple[int, int, int, int], confidence: float):
        self.x1, self.y1, self.x2, self.y2 = bbox
        self.confidence = confidence
        self.width = self.x2 - self.x1
        self.height = self.y2 - self.y1
        self.center_x = (self.x1 + self.x2) / 2
        self.center_y = (self.y1 + self.y2) / 2
        self.area = self.width * self.height
        
        # ì–¼êµ´ ì¸ì‹ìš© ì„ë² ë”© (FaceNet)
        self.embedding = None  # torch.Tensor
        self.p1_score = 0.0    # Person1ê³¼ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        self.p2_score = 0.0    # Person2ì™€ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        
    @property
    def bbox(self) -> Tuple[int, int, int, int]:
        return (self.x1, self.y1, self.x2, self.y2)
    
    @property
    def center(self) -> Tuple[float, float]:
        return (self.center_x, self.center_y)


class FaceTracker:
    """ê²€ì¶œ ê¸°ë°˜ ì–¼êµ´ íŠ¸ë˜ì»¤ (OpenCV 4.13 í˜¸í™˜)"""
    
    def __init__(self, person_id: str, smoothing_alpha: float = 0.15):
        self.person_id = person_id
        self.smoothing_alpha = smoothing_alpha  # 0.3 â†’ 0.15ë¡œ ë” ë¶€ë“œëŸ½ê²Œ
        
        # ê²€ì¶œ ê¸°ë°˜ ìƒíƒœ
        self.face_detection: Optional[FaceDetection] = None
        self.face_center: Optional[Tuple[float, float]] = None
        self.smooth_center: Optional[Tuple[float, float]] = None
        self.last_good_bbox: Optional[Tuple[int, int, int, int]] = None
        self.tracking_confidence = 0.0
        
        # ê²€ì¶œ íˆìŠ¤í† ë¦¬ (ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì )
        self.detection_history: List[Tuple[float, float]] = []  # (center_x, center_y) íˆìŠ¤í† ë¦¬
        self.max_history = 30  # ìµœê·¼ 30ê°œ ìœ„ì¹˜ ê¸°ì–µ (10 â†’ 30ìœ¼ë¡œ ì¦ê°€)
        self.position_threshold = 50  # ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ì¸ì‹í•  ê±°ë¦¬ ì„ê³„ê°’ (ë” ì—„ê²©í•˜ê²Œ)
        
        # í¬ë¡­ í¬ê¸° ê´€ë ¨ - ì™„ì „ ê³ ì •
        self.fixed_crop_size = None  # ì²« í”„ë ˆì„ì—ì„œ ì„¤ì • í›„ ë¶ˆë³€
        
        # í†µê³„
        self.detection_success_count = 0
        self.detection_fail_count = 0
        self.total_frames = 0
        
        # ë™ì  ì¬í• ë‹¹ ì‹œìŠ¤í…œ (ì‹ ë¢°ë„ ê¸°ë°˜)
        self.confidence_history = []  # ìµœê·¼ ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬
        self.max_confidence_history = 20
        self.low_confidence_threshold = 0.6  # ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’
        self.reassignment_trigger_count = 10  # ì¬í• ë‹¹ ê²€í† ë¥¼ ìœ„í•œ ë‚®ì€ ì‹ ë¢°ë„ ì—°ì† íšŸìˆ˜
        
    def update_detection(self, detection: Optional[FaceDetection]) -> bool:
        """ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸ (íŠ¸ë˜ì»¤ ì—†ìŒ)"""
        self.total_frames += 1
        
        if detection is None:
            self.detection_fail_count += 1
            return False
            
        # ê²€ì¶œ ì„±ê³µ
        self.face_detection = detection
        self.face_center = detection.center
        self.last_good_bbox = detection.bbox
        self.tracking_confidence = detection.confidence
        
        # ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ì¬í• ë‹¹ ê²€í† ìš©)
        self.confidence_history.append(detection.confidence)
        if len(self.confidence_history) > self.max_confidence_history:
            self.confidence_history.pop(0)
        
        # íˆìŠ¤í† ë¦¬ì— ìœ„ì¹˜ ì¶”ê°€
        self.detection_history.append(self.face_center)
        if len(self.detection_history) > self.max_history:
            self.detection_history.pop(0)
        
        # ìŠ¤ë¬´ë”© ì ìš©
        if self.smooth_center is None:
            self.smooth_center = self.face_center
        else:
            self.smooth_center = self._apply_smoothing(self.face_center)
        
        self.detection_success_count += 1
        return True
    
    def get_distance_to_detection(self, detection: FaceDetection) -> float:
        """ê²€ì¶œëœ ì–¼êµ´ê³¼ì˜ ê±°ë¦¬ ê³„ì‚° (ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­ìš©, íˆìŠ¤í† ë¦¬ ê°•í™”)"""
        if not self.detection_history:
            return float('inf')
        
        # ìµœê·¼ ìœ„ì¹˜ë“¤ê³¼ì˜ ê°€ì¤‘ í‰ê·  ê±°ë¦¬ ê³„ì‚° (ë” ë§ì€ íˆìŠ¤í† ë¦¬ ì‚¬ìš©)
        det_center = detection.center
        distances = []
        weights = []
        
        # ìµœê·¼ 5ê°œ ìœ„ì¹˜ ì‚¬ìš© (3 â†’ 5ë¡œ ì¦ê°€), ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        recent_history = self.detection_history[-5:]
        for i, hist_center in enumerate(recent_history):
            dist = ((det_center[0] - hist_center[0]) ** 2 + 
                   (det_center[1] - hist_center[1]) ** 2) ** 0.5
            distances.append(dist)
            # ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜: 0.1, 0.15, 0.2, 0.25, 0.3
            weight = 0.1 + (i * 0.05)
            weights.append(weight)
        
        if distances:
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weighted_sum = sum(d * w for d, w in zip(distances, weights))
            total_weight = sum(weights)
            return weighted_sum / total_weight
        else:
            return float('inf')
    
    def _get_fixed_crop_size(self, frame: np.ndarray, detected_face_size: Optional[int] = None) -> int:
        """í”„ë ˆì„ í¬ê¸° ê¸°ë°˜ ì™„ì „ ê³ ì • í¬ë¡­ í¬ê¸°"""
        h, w = frame.shape[:2]
        
        # ì™„ì „ ê³ ì • í¬ê¸°: í”„ë ˆì„ ë†’ì´ì˜ 45%ë¡œ ê³ ì •
        if self.fixed_crop_size is None:
            self.fixed_crop_size = int(h * 0.45)  # ì´ˆê¸°í™”ì‹œ í•œë²ˆë§Œ ì„¤ì •
        
        return self.fixed_crop_size  # í•­ìƒ ë™ì¼í•œ ê°’ ë°˜í™˜
    
    def get_crop_region(self, frame: np.ndarray, margin_factor: float = 2.5) -> np.ndarray:
        """ì™„ì „ ê³ ì • í¬ê¸° í¬ë¡­ ì˜ì—­ ë°˜í™˜"""
        h, w = frame.shape[:2]
        
        # ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì˜ì—­
        if self.smooth_center is None or self.last_good_bbox is None:
            if self.person_id == "Person1":
                crop = frame[0:h, 0:w//2]
            else:
                crop = frame[0:h, w//2:w]
            return cv2.resize(crop, (960, 1080))
        
        # ì™„ì „ ê³ ì • í¬ë¡­ í¬ê¸° (ì²« í”„ë ˆì„ì—ì„œ í•œë²ˆë§Œ ì„¤ì •)
        if self.fixed_crop_size is None:
            self.fixed_crop_size = int(h * 0.45)
        
        crop_size = self.fixed_crop_size  # í•­ìƒ ë™ì¼
        
        # í¬ë¡­ ì¤‘ì‹¬ì  (ë¶€ë“œëŸ½ê²Œ ìŠ¤ë¬´ë”©ëœ ì–¼êµ´ ì¤‘ì‹¬)
        center_x, center_y = self.smooth_center
        
        # í¬ë¡­ ì˜ì—­ ê³„ì‚°
        crop_x1 = max(0, int(center_x - crop_size // 2))
        crop_y1 = max(0, int(center_y - crop_size // 2))
        crop_x2 = min(w, crop_x1 + crop_size)
        crop_y2 = min(h, crop_y1 + crop_size)
        
        # ê²½ê³„ ì¡°ì • (ì •ì‚¬ê°í˜• ìœ ì§€)
        if crop_x2 - crop_x1 < crop_size:
            crop_x1 = max(0, crop_x2 - crop_size)
        if crop_y2 - crop_y1 < crop_size:
            crop_y1 = max(0, crop_y2 - crop_size)
        
        # í¬ë¡­ ì¶”ì¶œ
        cropped = frame[crop_y1:crop_y2, crop_x1:crop_x2]
        
        # ì •ì‚¬ê°í˜• íŒ¨ë”© (í•„ìš”ì‹œ) - ê³ ì • í¬ê¸° ë³´ì¥
        ch, cw = cropped.shape[:2]
        if ch != cw:
            max_size = max(ch, cw)
            pad_h = (max_size - ch) // 2
            pad_w = (max_size - cw) // 2
            cropped = cv2.copyMakeBorder(
                cropped, pad_h, max_size - ch - pad_h, 
                pad_w, max_size - cw - pad_w, 
                cv2.BORDER_CONSTANT, value=[0, 0, 0]
            )
        
        # 960x1080ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ìµœì¢… ê³ ì • í¬ê¸°)
        return cv2.resize(cropped, (960, 1080))
    
    def get_adaptive_crop_region(self, frame: np.ndarray, face_size: float) -> np.ndarray:
        """ì–¼êµ´ í¬ê¸°ì— ë”°ë¼ ì ì‘ì  í¬ë¡­ ì˜ì—­ ìƒì„±"""
        # ì–¼êµ´ í¬ê¸°ì— ë”°ë¼ ë§ˆì§„ ë™ì  ì¡°ì •
        if face_size < 5000:  # ì‘ì€ ì–¼êµ´ (ë©€ë¦¬ ìˆìŒ)
            margin_factor = 3.5  # ë” í° ë§ˆì§„ìœ¼ë¡œ ì£¼ë³€ ì •ë³´ í¬í•¨
        elif face_size < 15000:  # ì¤‘ê°„ ì–¼êµ´
            margin_factor = 2.5  # ê¸°ë³¸ ë§ˆì§„
        else:  # í° ì–¼êµ´ (ê°€ê¹Œì´ ìˆìŒ)
            margin_factor = 2.0  # ë” ì‘ì€ ë§ˆì§„ìœ¼ë¡œ ì–¼êµ´ì— ì§‘ì¤‘
        
        # ê¸°ì¡´ get_crop_regionì„ ë™ì  ë§ˆì§„ìœ¼ë¡œ í˜¸ì¶œ
        h, w = frame.shape[:2]
        
        if self.smooth_center is None or self.last_good_bbox is None:
            # ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì˜ì—­ ë°˜í™˜ (ì¢Œìš° ë¶„í• )
            if self.person_id == "Person1":
                crop = frame[0:h, 0:w//2]
            else:
                crop = frame[0:h, w//2:w]
                
            # 960x1080ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            return cv2.resize(crop, (960, 1080))
        
        # ì–¼êµ´ í¬ê¸° ê³„ì‚° (ê³ ì • í¬ê¸° ê³„ì‚°ìš©)
        x1, y1, x2, y2 = self.last_good_bbox
        face_width = x2 - x1
        face_height = y2 - y1
        calculated_face_size = max(face_width, face_height)
        
        # ì ì‘ì  í¬ë¡­ í¬ê¸° ê³„ì‚° (ì–¼êµ´ í¬ê¸°ì— ë”°ë¼ ë§ˆì§„ ì¡°ì •)
        adaptive_crop_size = int(calculated_face_size * margin_factor)
        adaptive_crop_size = max(100, min(min(w, h), adaptive_crop_size))  # ìµœì†Œ/ìµœëŒ€ í¬ê¸° ì œí•œ
        
        # í¬ë¡­ ì¤‘ì‹¬ì  (ìŠ¤ë¬´ë”©ëœ ì–¼êµ´ ì¤‘ì‹¬)
        center_x, center_y = self.smooth_center
        
        # í¬ë¡­ ì˜ì—­ ê³„ì‚°
        crop_x1 = max(0, int(center_x - adaptive_crop_size // 2))
        crop_y1 = max(0, int(center_y - adaptive_crop_size // 2))
        crop_x2 = min(w, crop_x1 + adaptive_crop_size)
        crop_y2 = min(h, crop_y1 + adaptive_crop_size)
        
        # ê²½ê³„ ì¡°ì • (ì •ì‚¬ê°í˜• ìœ ì§€)
        if crop_x2 - crop_x1 < adaptive_crop_size:
            crop_x1 = max(0, crop_x2 - adaptive_crop_size)
        if crop_y2 - crop_y1 < adaptive_crop_size:
            crop_y1 = max(0, crop_y2 - adaptive_crop_size)
        
        # í¬ë¡­ ì¶”ì¶œ
        cropped = frame[crop_y1:crop_y2, crop_x1:crop_x2]
        
        # ì •ì‚¬ê°í˜• íŒ¨ë”© (í•„ìš”ì‹œ) - ê³ ì • í¬ê¸° ë³´ì¥
        ch, cw = cropped.shape[:2]
        if ch != cw:
            max_size = max(ch, cw)
            pad_h = (max_size - ch) // 2
            pad_w = (max_size - cw) // 2
            cropped = cv2.copyMakeBorder(
                cropped, pad_h, max_size - ch - pad_h, 
                pad_w, max_size - cw - pad_w, 
                cv2.BORDER_CONSTANT, value=[0, 0, 0]
            )
        
        # 960x1080ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ìµœì¢… ê³ ì • í¬ê¸°)
        return cv2.resize(cropped, (960, 1080))
    
    def _apply_smoothing(self, center: Tuple[float, float]) -> Tuple[float, float]:
        """EMA ìŠ¤ë¬´ë”© ì ìš© (ë” ë¶€ë“œëŸ¬ìš´ ì›€ì§ì„)"""
        if self.smooth_center is None:
            return center
            
        # alphaë¥¼ 0.15ë¡œ ë‚®ì¶°ì„œ ë” ë¶€ë“œëŸ½ê²Œ (ê¸°ì¡´ 0.2)
        alpha = 0.15  # self.smoothing_alpha â†’ 0.15 ê³ ì •
        smooth_x = alpha * center[0] + (1 - alpha) * self.smooth_center[0]
        smooth_y = alpha * center[1] + (1 - alpha) * self.smooth_center[1]
        
        return (smooth_x, smooth_y)
    
    def should_consider_reassignment(self) -> bool:
        """ì¬í• ë‹¹ì„ ê²€í† í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨ (ì‹ ë¢°ë„ ê¸°ë°˜)"""
        if len(self.confidence_history) < self.reassignment_trigger_count:
            return False
        
        # ìµœê·¼ Nê°œ í”„ë ˆì„ì˜ ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ì§€ í™•ì¸
        recent_confidences = self.confidence_history[-self.reassignment_trigger_count:]
        low_confidence_count = sum(1 for conf in recent_confidences 
                                 if conf < self.low_confidence_threshold)
        
        # 80% ì´ìƒì´ ë‚®ì€ ì‹ ë¢°ë„ë©´ ì¬í• ë‹¹ ê²€í† 
        return low_confidence_count >= (self.reassignment_trigger_count * 0.8)
    
    def get_average_confidence(self) -> float:
        """í‰ê·  ì‹ ë¢°ë„ ë°˜í™˜"""
        if not self.confidence_history:
            return 0.0
        return sum(self.confidence_history) / len(self.confidence_history)
    
    def get_stats(self) -> Dict[str, Any]:
        """ê²€ì¶œ í†µê³„ ë°˜í™˜"""
        total_detections = self.detection_success_count + self.detection_fail_count
        success_rate = (self.detection_success_count / max(1, total_detections)) * 100
        
        return {
            'person_id': self.person_id,
            'total_frames': self.total_frames,
            'detection_success': self.detection_success_count,
            'detection_fail': self.detection_fail_count,
            'success_rate': success_rate,
            'has_detection': self.face_detection is not None,
            'detection_active': len(self.detection_history) > 0,
            'smooth_center': self.smooth_center,
            'history_length': len(self.detection_history),
            'average_confidence': self.get_average_confidence(),
            'should_reassign': self.should_consider_reassignment(),
            'fixed_crop_size': self.fixed_crop_size
        }


class FaceEmbeddingTracker(FaceTracker):
    """ê³ ê¸‰ ì–¼êµ´ ì„ë² ë”© ê¸°ë°˜ íŠ¸ë˜ì»¤ (SmartEmbeddingManager + ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ í™œìš©)"""
    
    def __init__(self, person_id: str, smoothing_alpha: float = 0.3):
        super().__init__(person_id, smoothing_alpha)
        
        # ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ì (SmartEmbeddingManager ë¹„í™œì„±í™”)
        self.smart_embedding_manager = None
        print(f"âš ï¸ {person_id}: SmartEmbeddingManager ë¹„í™œì„±í™”")
        
        # ê°œë³„ ì„ë² ë”© ì¶”ì  (ë””ë²„ê¹…ìš©)
        self.face_embeddings = []  # ë°±ì—… íˆìŠ¤í† ë¦¬
        self.reference_embedding = None  # ëŒ€í‘œ ì„ë² ë”© (í‰ê· )
        self.max_embeddings = 10  # ìµœëŒ€ 10ê°œ ì„ë² ë”© ìœ ì§€
        self.embedding_threshold = 0.35  # Phase 1: 0.75 â†’ 0.35 (Identity-based ê°•í™”)
        
        # ê³ ê¸‰ í†µê³„
        self.embedding_updates = 0
        self.similarity_scores = []  # ìµœê·¼ ìœ ì‚¬ë„ ì ìˆ˜ë“¤
        self.l2_normalization_enabled = True  # L2 ì •ê·œí™” ì‚¬ìš©
        
    def add_face_embedding(self, embedding: torch.Tensor) -> None:
        """ìƒˆ ì„ë² ë”© ì¶”ê°€ ë° ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ì ì—…ë°ì´íŠ¸"""
        if embedding is None:
            return
            
        # L2 ì •ê·œí™” (ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš©)
        normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=-1)
        
        # ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ìì— ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.smart_embedding_manager is not None:
            track_id = f"{self.person_id}_{self.embedding_updates}"
            self.smart_embedding_manager.add_embedding(track_id, normalized_embedding)
        
        # ë°±ì—… íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€ (í˜¸í™˜ì„±)
        self.face_embeddings.append(normalized_embedding)
        if len(self.face_embeddings) > self.max_embeddings:
            self.face_embeddings.pop(0)  # ì˜¤ë˜ëœ ê²ƒ ì œê±°
        
        # ëŒ€í‘œ ì„ë² ë”© ì—…ë°ì´íŠ¸ (í‰ê· )
        if len(self.face_embeddings) > 0:
            stacked_embeddings = torch.stack(self.face_embeddings)
            self.reference_embedding = torch.mean(stacked_embeddings, dim=0)
            self.reference_embedding = torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1)
            
        self.embedding_updates += 1
        
    def compute_face_similarity(self, new_embedding: torch.Tensor) -> float:
        """ìƒˆ ì–¼êµ´ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš©)"""
        if self.reference_embedding is None or new_embedding is None:
            return 0.0
        
        # ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš© (MODEL_MANAGER_AVAILABLE í™•ì¸)
        if MODEL_MANAGER_AVAILABLE:
            try:
                # calculate_face_similarity í•¨ìˆ˜ ì‚¬ìš© (L2 ì •ê·œí™” ìë™ ì ìš©)
                score = calculate_face_similarity(
                    self.reference_embedding.unsqueeze(0), 
                    new_embedding.unsqueeze(0), 
                    use_l2_norm=self.l2_normalization_enabled
                )
            except Exception as e:
                print(f"âš ï¸ ê³ ê¸‰ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë°©ë²• ì‚¬ìš©")
                # ë°±ì—… ë°©ë²•: ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                score = torch.cosine_similarity(
                    torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1).unsqueeze(0),
                    torch.nn.functional.normalize(new_embedding, p=2, dim=-1).unsqueeze(0),
                    dim=-1
                ).item()
        else:
            # ê¸°ë³¸ ë°©ë²•: L2 ì •ê·œí™” + ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            score = torch.cosine_similarity(
                torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1).unsqueeze(0),
                torch.nn.functional.normalize(new_embedding, p=2, dim=-1).unsqueeze(0),
                dim=-1
            ).item()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.similarity_scores.append(score)
        if len(self.similarity_scores) > 50:  # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
            self.similarity_scores.pop(0)
            
        return score
    
    def compute_hybrid_score(self, face_detection: 'FaceDetection') -> float:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì„ë² ë”© ìœ ì‚¬ë„ 70% + ìœ„ì¹˜ ê±°ë¦¬ 30%)"""
        if face_detection is None:
            return 0.0
            
        # 1. ì–¼êµ´ ìœ ì‚¬ë„ (0.0 ~ 1.0)
        face_similarity = 0.0
        if face_detection.embedding is not None and self.reference_embedding is not None:
            face_similarity = self.compute_face_similarity(face_detection.embedding)
        
        # 2. ìœ„ì¹˜ ê±°ë¦¬ (0.0 ~ 1.0, ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        position_distance = self.get_distance_to_detection(face_detection)
        position_score = 1.0 / (1.0 + position_distance / 100.0)  # 100px ê¸°ì¤€ ì •ê·œí™”
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        # ì„ë² ë”©ì´ ì—†ê±°ë‚˜ referenceê°€ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ ì ìˆ˜ë§Œ ì‚¬ìš©
        if face_detection.embedding is not None and self.reference_embedding is not None:
            hybrid_score = face_similarity * 0.7 + position_score * 0.3
        else:
            hybrid_score = position_score  # ìœ„ì¹˜ ê¸°ë°˜ë§Œ ì‚¬ìš©
        
        return hybrid_score
    
    def is_same_person(self, face_detection: 'FaceDetection') -> bool:
        """ê°™ì€ ì‚¬ëŒì¸ì§€ íŒë‹¨ (ì„ë² ë”© ê¸°ë°˜)"""
        if face_detection.embedding is None:
            # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
            distance = self.get_distance_to_detection(face_detection)
            return distance <= self.position_threshold
            
        # ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ íŒë‹¨
        similarity = self.compute_face_similarity(face_detection.embedding)
        return similarity >= self.embedding_threshold
    
    def update_with_embedding(self, detection: Optional['FaceDetection']) -> bool:
        """ì„ë² ë”©ì„ í¬í•¨í•œ ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸"""
        success = self.update_detection(detection)
        
        # ì„ë² ë”© ì¶”ê°€ (ê²€ì¶œ ì„±ê³µ ì‹œì—ë§Œ)
        if success and detection is not None and detection.embedding is not None:
            self.add_face_embedding(detection.embedding)
            
        return success
    
    def get_embedding_stats(self) -> Dict[str, Any]:
        """ê³ ê¸‰ ì„ë² ë”© ê´€ë ¨ í†µê³„ ë°˜í™˜ (SmartEmbeddingManager í¬í•¨)"""
        avg_similarity = 0.0
        if len(self.similarity_scores) > 0:
            avg_similarity = sum(self.similarity_scores) / len(self.similarity_scores)
        
        # SmartEmbeddingManager í†µê³„ ì¶”ê°€
        smart_stats = {}
        if self.smart_embedding_manager is not None:
            smart_stats = self.smart_embedding_manager.get_stats()
            
        return {
            'embeddings_count': len(self.face_embeddings),
            'embedding_updates': self.embedding_updates,
            'has_reference_embedding': self.reference_embedding is not None,
            'avg_similarity': avg_similarity,
            'recent_similarities': self.similarity_scores[-5:],  # ìµœê·¼ 5ê°œ
            'embedding_threshold': self.embedding_threshold,
            'l2_normalization_enabled': self.l2_normalization_enabled,
            'smart_embedding_manager_enabled': self.smart_embedding_manager is not None,
            'smart_embedding_stats': smart_stats  # SmartEmbeddingManager í†µê³„
        }


class SimplePreScanner:
    """ë¹ ë¥¸ ì‚¬ì „ ìŠ¤ìº”ìœ¼ë¡œ ì£¼ìš” 2ëª… ì°¾ê¸°"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.face_positions = []  # [(x, y, area, frame_idx), ...]
        
    def quick_scan(self, video_path: str, sample_rate: int = 30) -> Optional[Tuple[Tuple[float, float], Tuple[float, float]]]:
        """
        ë¹ ë¥¸ ì‚¬ì „ ìŠ¤ìº”ìœ¼ë¡œ ì£¼ìš” 2ëª…ì˜ í‰ê·  ìœ„ì¹˜ ì°¾ê¸°
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ 30í”„ë ˆì„ë§ˆë‹¤)
            
        Returns:
            (person1_avg_pos, person2_avg_pos) ë˜ëŠ” None
        """
        import cv2
        
        print(f"ğŸ” ì‚¬ì „ ë¶„ì„ ì‹œì‘: {sample_rate}í”„ë ˆì„ë§ˆë‹¤ ìƒ˜í”Œë§")
        
        # ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ ê²½ë¡œ ì‚¬ìš©)
        face_cascade = cv2.CascadeClassifier('/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml')
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
            return None
            
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        print(f"   ğŸ“¹ ì´ {total_frames}í”„ë ˆì„, {total_frames/fps:.1f}ì´ˆ")
        print(f"   ğŸ¯ ë¶„ì„í•  í”„ë ˆì„: {total_frames//sample_rate}ê°œ")
        
        self.face_positions = []
        frame_idx = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ìƒ˜í”Œë§: sample_rateë§ˆë‹¤ë§Œ ì²˜ë¦¬
                if frame_idx % sample_rate == 0:
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    faces = face_cascade.detectMultiScale(
                        gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
                    )
                    
                    for (x, y, w, h) in faces:
                        center_x = x + w / 2
                        center_y = y + h / 2
                        area = w * h
                        self.face_positions.append((center_x, center_y, area, frame_idx))
                
                frame_idx += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ (ë§¤ 300í”„ë ˆì„ë§ˆë‹¤)
                if frame_idx % 300 == 0:
                    progress = (frame_idx / total_frames) * 100
                    print(f"   ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({len(self.face_positions)}ê°œ ì–¼êµ´ ë°œê²¬)")
        
        finally:
            cap.release()
        
        print(f"âœ… ìŠ¤ìº” ì™„ë£Œ: {len(self.face_positions)}ê°œ ì–¼êµ´ ë°œê²¬")
        
        if len(self.face_positions) < 10:
            print("âš ï¸ ì¶©ë¶„í•œ ì–¼êµ´ ë°ì´í„° ì—†ìŒ")
            return None
        
        # ì¢Œìš° ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        return self._cluster_by_position()
    
    def _cluster_by_position(self) -> Optional[Tuple[Tuple[float, float], Tuple[float, float]]]:
        """ìœ„ì¹˜ ê¸°ë°˜ ë‹¨ìˆœ í´ëŸ¬ìŠ¤í„°ë§"""
        if not self.face_positions:
            return None
        
        # X ì¢Œí‘œë¡œ ì •ë ¬
        sorted_faces = sorted(self.face_positions, key=lambda x: x[0])
        
        # ì¤‘ê°„ê°’ìœ¼ë¡œ ì¢Œìš° êµ¬ë¶„
        mid_x = sorted_faces[len(sorted_faces)//2][0]
        
        left_faces = [f for f in self.face_positions if f[0] < mid_x]
        right_faces = [f for f in self.face_positions if f[0] >= mid_x]
        
        if len(left_faces) < 3 or len(right_faces) < 3:
            print("âš ï¸ ì¢Œìš° ì–¼êµ´ ë°ì´í„° ë¶€ì¡±")
            return None
        
        # ê° ê·¸ë£¹ì˜ í‰ê·  ìœ„ì¹˜ ê³„ì‚°
        left_avg_x = sum(f[0] for f in left_faces) / len(left_faces)
        left_avg_y = sum(f[1] for f in left_faces) / len(left_faces)
        
        right_avg_x = sum(f[0] for f in right_faces) / len(right_faces)
        right_avg_y = sum(f[1] for f in right_faces) / len(right_faces)
        
        person1_pos = (left_avg_x, left_avg_y)
        person2_pos = (right_avg_x, right_avg_y)
        
        if self.debug_mode:
            print(f"ğŸ“ ì‚¬ì „ ë¶„ì„ ê²°ê³¼:")
            print(f"   ì™¼ìª½ ê·¸ë£¹: {len(left_faces)}ê°œ, í‰ê·  ìœ„ì¹˜={person1_pos}")
            print(f"   ì˜¤ë¥¸ìª½ ê·¸ë£¹: {len(right_faces)}ê°œ, í‰ê·  ìœ„ì¹˜={person2_pos}")
        
        return person1_pos, person2_pos


class StablePositionTracker:
    """ìœ„ì¹˜ ì—°ì†ì„± ê¸°ë°˜ ì•ˆì •ì ì¸ ì–¼êµ´ ì¶”ì  (ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ ì¤€ë¹„ë¨)"""
    
    def __init__(self, debug_mode: bool = False, prescan_profiles: Optional[Tuple[Tuple[float, float], Tuple[float, float]]] = None):
        # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬
        self.person1_history = []  # [(x, y), ...] ìµœê·¼ ìœ„ì¹˜ë“¤
        self.person2_history = []
        self.history_size = 10  # ìµœëŒ€ 10ê°œ ìœ„ì¹˜ ì €ì¥
        
        # ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬ (ìŠ¤ë¬´ë”©ìš©)
        self.person1_confidence_history = []  # List[float]
        self.person2_confidence_history = []  # List[float]
        
        # ì´ˆê¸°í™” ê´€ë¦¬
        self.init_frames = 0
        self.init_threshold = 30  # 30í”„ë ˆì„ ë™ì•ˆ í¬ê¸° ê¸°ë°˜ ì´ˆê¸°í™”
        self.is_initialized = False
        
        # ë””ë²„ê·¸
        self.debug_mode = debug_mode
        
        # í† ë ˆë£¨ìŠ¤ ì„¤ì •
        self.min_confidence = 0.3  # ìµœì†Œ ì‹ ë¢°ë„
        self.smoothing_factor = 0.7  # ìœ„ì¹˜ ìŠ¤ë¬´ë”© (0.7 = 70% ì´ì „, 30% ìƒˆë¡œìš´)
        self.max_distance_threshold = 200  # ìµœëŒ€ í—ˆìš© ê±°ë¦¬ (í”½ì…€) - ê¸°ì¡´ 100ì—ì„œ 200ìœ¼ë¡œ ì™„í™”
        
        # Speaker reference ì„ë² ë”© (ì™¸ë¶€ì—ì„œ ì„¤ì •)
        self.speaker1_reference = None
        self.speaker2_reference = None
        self.model_manager = None  # ModelManager ì ‘ê·¼ìš©
        
        # ì‚¬ì „ ìŠ¤ìº” ê²°ê³¼ë¡œ ì´ˆê¸°í™”
        if prescan_profiles:
            person1_pos, person2_pos = prescan_profiles
            # íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ì „ ìŠ¤ìº” ê²°ê³¼ë¡œ ë¯¸ë¦¬ ì±„ìš°ê¸°
            self.person1_history = [person1_pos] * 5
            self.person2_history = [person2_pos] * 5
            self.is_initialized = True
            self.init_frames = self.init_threshold  # ì´ˆê¸°í™” ìŠ¤í‚µ
            print(f"âœ… ì‚¬ì „ ìŠ¤ìº” í”„ë¡œíŒŒì¼ë¡œ ì´ˆê¸°í™”: P1={person1_pos}, P2={person2_pos}")
        else:
            print(f"ğŸ¯ StablePositionTracker ì´ˆê¸°í™” (ì´ˆê¸°í™” {self.init_threshold}í”„ë ˆì„, íˆìŠ¤í† ë¦¬ {self.history_size}ê°œ)")
    
    def track_faces(self, faces: List[FaceDetection], frame_idx: int, frame: np.ndarray = None) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ë©”ì¸ ì¶”ì  ë©”ì„œë“œ: ì´ˆê¸°í™” ë˜ëŠ” ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì """
        
        if not faces:
            return None, None
        
        # ì´ˆê¸°í™” ë‹¨ê³„: í¬ê¸° ê¸°ë°˜ ì•ˆì •í™”
        if self.init_frames < self.init_threshold:
            return self._initialize_tracking(faces, frame_idx, frame)
        
        # ì¶”ì  ë‹¨ê³„: ìœ„ì¹˜ ì—°ì†ì„± ê¸°ë°˜
        return self._track_by_position(faces, frame_idx)
    
    def _initialize_tracking(self, faces: List[FaceDetection], frame_idx: int, frame: np.ndarray = None) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ì´ˆê¸° 30í”„ë ˆì„: í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì•ˆì •ì ì¸ ì´ˆê¸°í™”"""
        self.init_frames += 1
        
        # ì‚¬ì „ ìŠ¤ìº” í”„ë¡œíŒŒì¼ì´ ìˆìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ í• ë‹¹ ìš°ì„ 
        if self.person1_history and self.person2_history and len(self.person1_history) >= 5:
            person1_face, person2_face = self._assign_by_expected_position(faces)
        elif self.speaker1_reference is not None and self.speaker2_reference is not None:
            # ì„ë² ë”© ê¸°ë°˜ ìˆœìˆ˜ í• ë‹¹ (ìœ„ì¹˜ ë¬´ê´€)
            person1_face, person2_face = self._assign_by_embedding_only(faces, frame)
        else:
            # í´ë°±: ì¤‘ìš”ë„ ê¸°ë°˜ í• ë‹¹ (ì¤‘ì•™ + í¬ê¸° + ì‹ ë¢°ë„)
            person1_face, person2_face = self._assign_by_importance(faces)
        
        # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ êµ¬ì¶•
        if person1_face:
            self._update_person_history(1, person1_face)
        
        if person2_face:
            self._update_person_history(2, person2_face)
        
        # ì´ˆê¸°í™” ì™„ë£Œ ì²´í¬
        if self.init_frames >= self.init_threshold:
            self.is_initialized = True
            if self.debug_mode:
                print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ (í”„ë ˆì„ {frame_idx}): P1 íˆìŠ¤í† ë¦¬={len(self.person1_history)}, P2 íˆìŠ¤í† ë¦¬={len(self.person2_history)}")
        
        if self.debug_mode and frame_idx % 10 == 0:
            size1 = person1_face.area if person1_face else 0
            size2 = person2_face.area if person2_face else 0
            print(f"ğŸ“Š ì´ˆê¸°í™” {self.init_frames}/{self.init_threshold}: P1=ìœ„ì¹˜{person1_face.center if person1_face else None}, P2=ìœ„ì¹˜{person2_face.center if person2_face else None}")
        
        return person1_face, person2_face
    
    def _track_by_position(self, faces: List[FaceDetection], frame_idx: int) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ìœ„ì¹˜ ì—°ì†ì„± ê¸°ë°˜ ì¶”ì  (ë‚˜ì¤‘ì— ì„ë² ë”© í•˜ì´ë¸Œë¦¬ë“œ ê°€ëŠ¥)"""
        
        # TODO: ì„ë² ë”© ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ í•˜ì´ë¸Œë¦¬ë“œ
        # if self.embedding_enabled:
        #     embedding_scores = self._calculate_embedding_scores(faces)
        #     position_scores = self._calculate_position_scores(faces)
        #     final_scores = 0.7 * position_scores + 0.3 * embedding_scores
        # else:
        #     final_scores = self._calculate_position_scores(faces)
        
        # í˜„ì¬ëŠ” ìœ„ì¹˜ë§Œ ì‚¬ìš©
        return self._assign_by_position(faces, frame_idx)
    
    def _assign_by_position(self, faces: List[FaceDetection], frame_idx: int) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ìœ„ì¹˜ ê¸°ë°˜ ì–¼êµ´ ë§¤ì¹­"""
        
        # ì‚¬ìš©ëœ ì–¼êµ´ ì¶”ì 
        used_faces = set()
        person1_face = None
        person2_face = None
        
        # Person1 ì°¾ê¸°: ê°€ì¥ ê°€ê¹Œìš´ ì–¼êµ´
        if self.person1_history:
            predicted_p1_pos = self._get_predicted_position(self.person1_history)
            min_dist = float('inf')
            best_face_idx = -1
            
            for i, face in enumerate(faces):
                if face.confidence < self.min_confidence:
                    continue
                
                dist = self._calculate_distance(face.center, predicted_p1_pos)
                if dist < min_dist:
                    min_dist = dist
                    best_face_idx = i
            
            if best_face_idx >= 0 and min_dist <= self.max_distance_threshold:
                person1_face = faces[best_face_idx]
                used_faces.add(best_face_idx)
                
                # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_person_history(1, person1_face)
                    
                if self.debug_mode:
                    print(f"âœ… P1 ê±°ë¦¬ ë§¤ì¹­: {min_dist:.1f}px <= {self.max_distance_threshold}px")
            elif self.debug_mode:
                print(f"âŒ P1 ê±°ë¦¬ ì´ˆê³¼: {min_dist:.1f}px > {self.max_distance_threshold}px")
        
        # Person2 ì°¾ê¸°: ë‚¨ì€ ì–¼êµ´ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´
        if self.person2_history:
            predicted_p2_pos = self._get_predicted_position(self.person2_history)
            min_dist = float('inf')
            best_face_idx = -1
            
            for i, face in enumerate(faces):
                if i in used_faces or face.confidence < self.min_confidence:
                    continue
                
                dist = self._calculate_distance(face.center, predicted_p2_pos)
                if dist < min_dist:
                    min_dist = dist
                    best_face_idx = i
            
            if best_face_idx >= 0 and min_dist <= self.max_distance_threshold:
                person2_face = faces[best_face_idx]
                
                # ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_person_history(2, person2_face)
                    
                if self.debug_mode:
                    print(f"âœ… P2 ê±°ë¦¬ ë§¤ì¹­: {min_dist:.1f}px <= {self.max_distance_threshold}px")
            elif self.debug_mode:
                print(f"âŒ P2 ê±°ë¦¬ ì´ˆê³¼: {min_dist:.1f}px > {self.max_distance_threshold}px")
        
        # ë‹¨ì¼ ì–¼êµ´ ì²˜ë¦¬: Person2 ê°€ëŠ¥ì„± ì²´í¬
        if len(faces) == 1 and person1_face and not person2_face:
            face = faces[0]
            if self._is_closer_to_person2(face):
                if self.debug_mode:
                    print(f"ğŸ”„ ë‹¨ì¼ì–¼êµ´ì„ P2ë¡œ ì¬í• ë‹¹: {face.center}")
                person1_face = None
                person2_face = face
                # Person2 íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_person_history(2, person2_face)
        
        # Person2 í´ë°± ë©”ì»¤ë‹ˆì¦˜: ë‘ ë²ˆì§¸ë¡œ í° ì–¼êµ´ ìë™ í• ë‹¹
        if not person2_face and len(faces) >= 2 and person1_face:
            # Person1ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì–¼êµ´ë“¤ ì¤‘ ê°€ì¥ í° ê²ƒì„ Person2ë¡œ
            unused_faces = [(i, face) for i, face in enumerate(faces) if i not in used_faces and face.confidence >= self.min_confidence]
            if unused_faces:
                # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
                unused_faces.sort(key=lambda x: x[1].area, reverse=True)
                person2_face = unused_faces[0][1]
                
                # Person2 íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_person_history(2, person2_face)
                
                if self.debug_mode:
                    print(f"ğŸ”„ í´ë°± ë©”ì»¤ë‹ˆì¦˜: P2ì— ë‘ ë²ˆì§¸ í° ì–¼êµ´ í• ë‹¹ {person2_face.center}")
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        if self.debug_mode and frame_idx % 30 == 0:
            p1_info = f"pos={person1_face.center}, size={person1_face.area:.0f}" if person1_face else "ì—†ìŒ"
            p2_info = f"pos={person2_face.center}, size={person2_face.area:.0f}" if person2_face else "ì—†ìŒ"
            print(f"ğŸ¯ í”„ë ˆì„ {frame_idx} ì¶”ì : P1={p1_info}, P2={p2_info}")
            
            if person1_face and person2_face:
                dist_p1 = self._calculate_distance(person1_face.center, self._get_predicted_position(self.person1_history[:-1]))
                dist_p2 = self._calculate_distance(person2_face.center, self._get_predicted_position(self.person2_history[:-1]))
                print(f"ğŸ“ ì´ë™ ê±°ë¦¬: P1={dist_p1:.1f}px, P2={dist_p2:.1f}px")
        
        return person1_face, person2_face
    
    def _assign_by_expected_position(self, faces: List[FaceDetection]) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ì‚¬ì „ ìŠ¤ìº” ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–¼êµ´ì„ ì˜ˆìƒ ìœ„ì¹˜ì— í• ë‹¹"""
        if not faces:
            if self.debug_mode:
                print(f"ğŸ” _assign_by_expected_position: ì–¼êµ´ ì—†ìŒ")
            return None, None
        
        person1_face = None
        person2_face = None
        used_faces = set()
        
        if self.debug_mode:
            print(f"ğŸ” _assign_by_expected_position: {len(faces)}ê°œ ì–¼êµ´, P1íˆìŠ¤í† ë¦¬={len(self.person1_history)}, P2íˆìŠ¤í† ë¦¬={len(self.person2_history)}")
        
        # Person1 í• ë‹¹ (ì™¼ìª½, ì²« ë²ˆì§¸ ì‚¬ëŒ)
        if self.person1_history:
            predicted_p1_pos = self._get_predicted_position(self.person1_history)
            min_dist = float('inf')
            best_face_idx = -1
            
            if self.debug_mode:
                print(f"ğŸ” P1 ì˜ˆìƒìœ„ì¹˜: {predicted_p1_pos}")
            
            for i, face in enumerate(faces):
                if face.confidence < self.min_confidence:
                    if self.debug_mode:
                        print(f"   P1 ì–¼êµ´{i}: ì‹ ë¢°ë„ ë¶€ì¡± {face.confidence} < {self.min_confidence}")
                    continue
                
                dist = self._calculate_distance(face.center, predicted_p1_pos)
                if self.debug_mode:
                    print(f"   P1 ì–¼êµ´{i}: pos={face.center}, ê±°ë¦¬={dist:.1f}px")
                if dist < min_dist:
                    min_dist = dist
                    best_face_idx = i
            
            if best_face_idx >= 0:
                person1_face = faces[best_face_idx]
                used_faces.add(best_face_idx)
                if self.debug_mode:
                    print(f"âœ… P1 í• ë‹¹: ì–¼êµ´{best_face_idx}, ê±°ë¦¬={min_dist:.1f}px")
            elif self.debug_mode:
                print(f"âŒ P1 í• ë‹¹ ì‹¤íŒ¨: ëª¨ë“  ì–¼êµ´ì´ ì¡°ê±´ ë¶ˆë§Œì¡±")
        
        # Person2 í• ë‹¹ (ì˜¤ë¥¸ìª½, ë‘ ë²ˆì§¸ ì‚¬ëŒ)
        if self.person2_history:
            predicted_p2_pos = self._get_predicted_position(self.person2_history)
            min_dist = float('inf')
            best_face_idx = -1
            
            if self.debug_mode:
                print(f"ğŸ” P2 ì˜ˆìƒìœ„ì¹˜: {predicted_p2_pos}")
            
            for i, face in enumerate(faces):
                if i in used_faces:
                    if self.debug_mode:
                        print(f"   P2 ì–¼êµ´{i}: ì´ë¯¸ P1ì—ì„œ ì‚¬ìš©ë¨")
                    continue
                if face.confidence < self.min_confidence:
                    if self.debug_mode:
                        print(f"   P2 ì–¼êµ´{i}: ì‹ ë¢°ë„ ë¶€ì¡± {face.confidence} < {self.min_confidence}")
                    continue
                
                dist = self._calculate_distance(face.center, predicted_p2_pos)
                if self.debug_mode:
                    print(f"   P2 ì–¼êµ´{i}: pos={face.center}, ê±°ë¦¬={dist:.1f}px")
                if dist < min_dist:
                    min_dist = dist
                    best_face_idx = i
            
            if best_face_idx >= 0:
                person2_face = faces[best_face_idx]
                if self.debug_mode:
                    print(f"âœ… P2 í• ë‹¹: ì–¼êµ´{best_face_idx}, ê±°ë¦¬={min_dist:.1f}px")
            else:
                if self.debug_mode:
                    print(f"âŒ P2 í• ë‹¹ ì‹¤íŒ¨: ì‚¬ìš©ê°€ëŠ¥í•œ ì–¼êµ´ ì—†ìŒ (ì´ {len(faces)}ê°œ, ì‚¬ìš©ë¨ {used_faces})")
        else:
            if self.debug_mode:
                print(f"âŒ P2 íˆìŠ¤í† ë¦¬ ì—†ìŒ: ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        return person1_face, person2_face
    
    def _assign_by_left_right_position(self, faces: List[FaceDetection]) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ì¢Œìš° ìœ„ì¹˜ ê¸°ë°˜ í• ë‹¹ (ì¢Œìš° ì„ì„ ë°©ì§€)"""
        if not faces:
            return None, None
        
        # 1920x1080 ê¸°ì¤€ ì¤‘ì•™ì„ (x=960)ìœ¼ë¡œ ì¢Œìš° êµ¬ë¶„
        left_faces = []  # ì™¼ìª½ (Person1 í›„ë³´)
        right_faces = []  # ì˜¤ë¥¸ìª½ (Person2 í›„ë³´)
        center_line = 960  # í™”ë©´ ì¤‘ì•™
        
        for face in faces:
            if face.confidence < self.min_confidence:
                continue
                
            if face.center_x < center_line:
                left_faces.append(face)
            else:
                right_faces.append(face)
        
        # ê° ì˜ì—­ì—ì„œ ê°€ì¥ í° ì–¼êµ´ ì„ íƒ (ì•ì‚¬ëŒ ìš°ì„ )
        person1_face = None
        person2_face = None
        
        if left_faces:
            # ì™¼ìª½ì—ì„œ ê°€ì¥ í° ì–¼êµ´ = Person1
            left_faces.sort(key=lambda f: f.area, reverse=True)
            person1_face = left_faces[0]
            
        if right_faces:
            # ì˜¤ë¥¸ìª½ì—ì„œ ê°€ì¥ í° ì–¼êµ´ = Person2
            right_faces.sort(key=lambda f: f.area, reverse=True)
            person2_face = right_faces[0]
        
        # í•œìª½ì—ë§Œ ì–¼êµ´ì´ ìˆëŠ” ê²½ìš° í¬ê¸°ìˆœìœ¼ë¡œ í• ë‹¹
        if person1_face is None and person2_face is None:
            # ëª¨ë“  ì–¼êµ´ì´ ì¤‘ì•™ì„  ê·¼ì²˜ -> í¬ê¸°ìˆœ í• ë‹¹
            all_valid_faces = [f for f in faces if f.confidence >= self.min_confidence]
            if len(all_valid_faces) >= 2:
                all_valid_faces.sort(key=lambda f: f.area, reverse=True)
                person1_face = all_valid_faces[0]
                person2_face = all_valid_faces[1]
            elif len(all_valid_faces) == 1:
                person1_face = all_valid_faces[0]
        elif person1_face is None and person2_face is not None:
            # ì˜¤ë¥¸ìª½ì—ë§Œ ìˆìŒ -> ê°€ì¥ í°ê²ƒì„ Person1ìœ¼ë¡œ, ë‘ë²ˆì§¸ë¥¼ Person2ë¡œ
            if len(right_faces) >= 2:
                person1_face = right_faces[0]  # ê°€ì¥ í°ê²ƒ
                person2_face = right_faces[1]  # ë‘ë²ˆì§¸
        elif person1_face is not None and person2_face is None:
            # ì™¼ìª½ì—ë§Œ ìˆìŒ -> ê°€ì¥ í°ê²ƒì„ Person1ìœ¼ë¡œ, ë‘ë²ˆì§¸ë¥¼ Person2ë¡œ
            if len(left_faces) >= 2:
                person1_face = left_faces[0]  # ê°€ì¥ í°ê²ƒ
                person2_face = left_faces[1]  # ë‘ë²ˆì§¸
        
        if self.debug_mode:
            print(f"ğŸ”„ ì¢Œìš° ê¸°ë°˜ í• ë‹¹: ì™¼ìª½={len(left_faces)}ê°œ, ì˜¤ë¥¸ìª½={len(right_faces)}ê°œ")
            if person1_face:
                print(f"   P1: ìœ„ì¹˜{person1_face.center}, í¬ê¸°{person1_face.area:.0f}")
            if person2_face:
                print(f"   P2: ìœ„ì¹˜{person2_face.center}, í¬ê¸°{person2_face.area:.0f}")
        
        return person1_face, person2_face
    
    def _assign_by_embedding_only(self, faces: List[FaceDetection], frame: np.ndarray = None) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ìˆœìˆ˜ ì„ë² ë”© ê¸°ë°˜ í• ë‹¹ (ìœ„ì¹˜ ë¬´ê´€)"""
        best_p1_face = None
        best_p2_face = None
        best_p1_score = -1
        best_p2_score = -1
        
        # Speaker reference ì„ë² ë”©ì´ í•„ìš”
        if not hasattr(self, 'speaker1_reference') or self.speaker1_reference is None:
            return None, None
        if not hasattr(self, 'speaker2_reference') or self.speaker2_reference is None:
            return None, None
        
        import torch.nn.functional as F
        
        for face in faces:
            if face.confidence < self.min_confidence:
                continue
            
            # ì–¼êµ´ì—ì„œ ì„ë² ë”© ì¶”ì¶œ (model_managerë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì¶”ì¶œ)
            face_embedding = None
            if frame is not None and hasattr(self, 'model_manager') and self.model_manager is not None:
                try:
                    face_crop = self.model_manager.extract_face_crop(face, frame)
                    if face_crop is not None:
                        face_embedding = self.model_manager.get_embedding(face_crop)
                except:
                    pass  # ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ê³„ì† ì§„í–‰
            
            if face_embedding is not None:
                # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                p1_sim = F.cosine_similarity(
                    face_embedding.unsqueeze(0), 
                    self.speaker1_reference.unsqueeze(0)
                ).item()
                p2_sim = F.cosine_similarity(
                    face_embedding.unsqueeze(0), 
                    self.speaker2_reference.unsqueeze(0)
                ).item()
                
                # ì •ê·œí™” (-1~1 â†’ 0~1)
                p1_sim = (p1_sim + 1) / 2
                p2_sim = (p2_sim + 1) / 2
                
                # ì„ê³„ê°’ 0.4 ì´ìƒì¸ ê²½ìš°ë§Œ ê³ ë ¤
                if p1_sim > best_p1_score and p1_sim > 0.4:
                    best_p1_face = face
                    best_p1_score = p1_sim
                    
                if p2_sim > best_p2_score and p2_sim > 0.4:
                    best_p2_face = face
                    best_p2_score = p2_sim
        
        # ë™ì¼ ì–¼êµ´ ì¤‘ë³µ ë°©ì§€
        if best_p1_face == best_p2_face and best_p1_face is not None:
            if best_p1_score > best_p2_score:
                best_p2_face = None
            else:
                best_p1_face = None
        
        if self.debug_mode:
            print(f"ğŸ” ì„ë² ë”© ê¸°ë°˜ í• ë‹¹: P1ì ìˆ˜={best_p1_score:.3f}, P2ì ìˆ˜={best_p2_score:.3f}")
        
        return best_p1_face, best_p2_face
    
    def _assign_by_importance(self, faces: List[FaceDetection]) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ì¤‘ìš”ë„ ê¸°ë°˜ í• ë‹¹ (í¬ê¸° + ì¤‘ì•™ + ì‹ ë¢°ë„)"""
        if not faces:
            return None, None
        
        scores = []
        for face in faces:
            if face.confidence < self.min_confidence:
                continue
            
            # ì¤‘ì•™ ê±°ë¦¬ ì ìˆ˜ (1920x1080 ê¸°ì¤€)
            center_dist = ((face.center_x - 960)**2 + (face.center_y - 540)**2)**0.5
            center_score = max(0, 1 - center_dist / 800)
            
            # í¬ê¸° ì ìˆ˜ (ì •ê·œí™”, 50000pxÂ² ê¸°ì¤€)
            size_score = min(face.area / 50000, 1.0)
            
            # ìµœì¢… ì ìˆ˜ (ì¤‘ì•™ 50% + í¬ê¸° 30% + ì‹ ë¢°ë„ 20%)
            importance = (0.5 * center_score + 
                         0.3 * size_score + 
                         0.2 * face.confidence)
            scores.append((face, importance))
        
        if not scores:
            return None, None
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        scores.sort(key=lambda x: x[1], reverse=True)
        
        person1_face = scores[0][0] if len(scores) >= 1 else None
        person2_face = scores[1][0] if len(scores) >= 2 else None
        
        if self.debug_mode:
            if len(scores) >= 2:
                print(f"ğŸ” ì¤‘ìš”ë„ ê¸°ë°˜ í• ë‹¹: P1ì ìˆ˜={scores[0][1]:.3f}, P2ì ìˆ˜={scores[1][1]:.3f}")
            elif len(scores) == 1:
                print(f"ğŸ” ì¤‘ìš”ë„ ê¸°ë°˜ í• ë‹¹: P1ì ìˆ˜={scores[0][1]:.3f}, P2ì—†ìŒ")
            else:
                print(f"ğŸ” ì¤‘ìš”ë„ ê¸°ë°˜ í• ë‹¹: ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
        
        return person1_face, person2_face
    
    def _is_closer_to_person2(self, face: FaceDetection) -> bool:
        """ë‹¨ì¼ ì–¼êµ´ì´ Person2ì— ë” ê°€ê¹Œìš´ì§€ íŒë‹¨"""
        if not self.person1_history or not self.person2_history:
            return False
        
        p1_pos = self._get_predicted_position(self.person1_history)
        p2_pos = self._get_predicted_position(self.person2_history)
        
        dist_to_p1 = self._calculate_distance(face.center, p1_pos)
        dist_to_p2 = self._calculate_distance(face.center, p2_pos)
        
        if self.debug_mode:
            print(f"ğŸ” ë‹¨ì¼ì–¼êµ´ ê±°ë¦¬ë¹„êµ: P1={dist_to_p1:.1f}px, P2={dist_to_p2:.1f}px")
        
        # Person2ê°€ ë” ê°€ê¹Œìš°ë©´ True
        return dist_to_p2 < dist_to_p1
    
    def _get_predicted_position(self, history: List[Tuple[float, float]]) -> Tuple[float, float]:
        """ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ë¡œ ë‹¤ìŒ ìœ„ì¹˜ ì˜ˆì¸¡ (ì†ë„ ë²¡í„° + ìŠ¤ë¬´ë”©)"""
        if not history:
            return (0, 0)
        
        if len(history) == 1:
            return history[0]
        
        # 1. ê¸°ë³¸ ê°€ì¤‘í‰ê·  ì˜ˆì¸¡
        recent = history[-3:] if len(history) >= 3 else history
        weights = [0.2, 0.3, 0.5] if len(recent) == 3 else [0.4, 0.6] if len(recent) == 2 else [1.0]
        
        weighted_x = sum(pos[0] * weight for pos, weight in zip(recent, weights[-len(recent):]))
        weighted_y = sum(pos[1] * weight for pos, weight in zip(recent, weights[-len(recent):]))
        
        # 2. ì†ë„ ë²¡í„° ê¸°ë°˜ ì˜ˆì¸¡ (5ê°œ ì´ìƒ íˆìŠ¤í† ë¦¬ê°€ ìˆì„ ë•Œ)
        if len(history) >= 5:
            # ìµœê·¼ 4ê°œ í”„ë ˆì„ì˜ ì†ë„ ë²¡í„° ê³„ì‚°
            recent_positions = history[-4:]
            velocities = []
            
            for i in range(1, len(recent_positions)):
                vx = recent_positions[i][0] - recent_positions[i-1][0]
                vy = recent_positions[i][1] - recent_positions[i-1][1]
                velocities.append((vx, vy))
            
            # í‰ê·  ì†ë„ ë²¡í„°
            if velocities:
                avg_vx = sum(v[0] for v in velocities) / len(velocities)
                avg_vy = sum(v[1] for v in velocities) / len(velocities)
                
                # ì†ë„ ê¸°ë°˜ ì˜ˆì¸¡ ìœ„ì¹˜
                last_pos = history[-1]
                velocity_predicted_x = last_pos[0] + avg_vx * 1.5  # 1.5í”„ë ˆì„ ì• ì˜ˆì¸¡
                velocity_predicted_y = last_pos[1] + avg_vy * 1.5
                
                # ê°€ì¤‘í‰ê·  50% + ì†ë„ì˜ˆì¸¡ 50% ê²°í•©
                final_x = 0.5 * weighted_x + 0.5 * velocity_predicted_x
                final_y = 0.5 * weighted_y + 0.5 * velocity_predicted_y
                
                return (final_x, final_y)
        
        # ê¸°ë³¸ ê°€ì¤‘í‰ê·  ë°˜í™˜
        return (weighted_x, weighted_y)
    
    def _update_person_history(self, person_id: int, face: FaceDetection):
        """ì‹ ë¢°ë„ì™€ í•¨ê»˜ ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        if person_id == 1:
            self.person1_history.append(face.center)
            self.person1_confidence_history.append(face.confidence)
            if len(self.person1_history) > self.history_size:
                self.person1_history.pop(0)
                self.person1_confidence_history.pop(0)
        elif person_id == 2:
            self.person2_history.append(face.center)
            self.person2_confidence_history.append(face.confidence)
            if len(self.person2_history) > self.history_size:
                self.person2_history.pop(0)
                self.person2_confidence_history.pop(0)
    
    def get_average_confidence(self, person_id: int) -> float:
        """ìµœê·¼ ì‹ ë¢°ë„ í‰ê·  ë°˜í™˜"""
        if person_id == 1 and self.person1_confidence_history:
            return sum(self.person1_confidence_history) / len(self.person1_confidence_history)
        elif person_id == 2 and self.person2_confidence_history:
            return sum(self.person2_confidence_history) / len(self.person2_confidence_history)
        return 0.5  # ê¸°ë³¸ê°’
    
    def _calculate_distance(self, pos1: Tuple[float, float], pos2: Tuple[float, float]) -> float:
        """ë‘ ìœ„ì¹˜ ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
        return ((pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2) ** 0.5
    
    def get_tracking_stats(self) -> Dict[str, Any]:
        """ì¶”ì  í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            'is_initialized': self.is_initialized,
            'init_frames': self.init_frames,
            'init_threshold': self.init_threshold,
            'person1_history_length': len(self.person1_history),
            'person2_history_length': len(self.person2_history),
            'person1_last_position': self.person1_history[-1] if self.person1_history else None,
            'person2_last_position': self.person2_history[-1] if self.person2_history else None
        }


class DualFaceTrackingSystem:
    """í†µí•© ì–¼êµ´ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, args):
        self.input_path = args.input
        self.output_path = args.output
        self.mode = args.mode
        self.gpu_id = args.gpu
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_path).parent.mkdir(exist_ok=True)
        
        # ì²˜ë¦¬ ì„¤ì •
        self.detection_interval = 1   # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì–¼êµ´ ê²€ì¶œ (íŠ¸ë˜ì»¤ ì—†ìœ¼ë¯€ë¡œ)
        self.margin_factor = 5.0      # ì–¼êµ´ í¬ê¸° ëŒ€ë¹„ í¬ë¡­ ë°°ìœ¨ (ìƒë°˜ì‹  í¬í•¨)
        self.confidence_threshold = 0.1  # 0.3â†’0.1 (ë§¤ìš° ê´€ëŒ€í•œ ì„ê³„ê°’)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.detection_method = None
        self._debug_detection_logged = False
        self._initialize_models()
        
        # íŠ¸ë˜ì»¤ ì´ˆê¸°í™” (ì„ë² ë”© ê¸°ë°˜ íŠ¸ë˜ì»¤ ì‚¬ìš©)
        self.person1_tracker = FaceEmbeddingTracker("Person1", smoothing_alpha=0.1)
        self.person2_tracker = FaceEmbeddingTracker("Person2", smoothing_alpha=0.1)
        
        # ìœ„ì¹˜ ê¸°ë°˜ ì•ˆì •ì  ì¶”ì  ì‹œìŠ¤í…œ
        self.debug_mode = False  # ë””ë²„ê·¸ ëª¨ë“œ (í¬ê¸° ì •ë³´ ì¶œë ¥)
        self.size_stabilize = False  # í¬ê¸° ê¸°ë°˜ ì•ˆì •í™” ì‚¬ìš© ì—¬ë¶€
        self.prescan_enabled = getattr(args, 'prescan', False)  # ì‚¬ì „ ìŠ¤ìº” ì˜µì…˜
        
        # AutoSpeakerDetector ì˜µì…˜ (ìƒˆë¡œìš´ ìë™ í™”ì ì„ ì • ì‹œìŠ¤í…œ)
        self.auto_speaker_enabled = getattr(args, 'auto_speaker', True)  # ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”
        
        # 1ë¶„ ë¶„ì„ ëª¨ë“œ ì˜µì…˜
        self.one_minute_mode = getattr(args, 'one_minute', False)  # 1ë¶„ ì§‘ì¤‘ ë¶„ì„ ëª¨ë“œ
        
        # Phase 3: Hungarian Matching ì˜µì…˜
        self.hungarian_mode = getattr(args, 'hungarian', False)  # Hungarian Matching ì‚¬ìš©
        
        # ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì ìëŠ” process()ì—ì„œ í”„ë¡œíŒŒì¼ê³¼ í•¨ê»˜ ì´ˆê¸°í™”
        self.position_tracker = None
        
        # ìë™ ì„ ì •ëœ í™”ì ì •ë³´ (Reference ì„ë² ë”©)
        self.speaker1_reference = None  # í™”ì1 ëŒ€í‘œ ì„ë² ë”©
        self.speaker2_reference = None  # í™”ì2 ëŒ€í‘œ ì„ë² ë”©
        self.speaker_similarity_threshold = 0.35  # Referenceì™€ ë§¤ì¹­ ì„ê³„ê°’ (0.5 â†’ 0.35, í€„ë¦¬í‹° ê°œì„ )
        
        # Phase 1: Identity-based tracking ê°•í™”
        self.MIN_FACE_SIZE = 120  # í”½ì…€ ë‹¨ìœ„, ì‘ì€ ì–¼êµ´ ë¬´ì‹œ (ë°°ê²½ ì¸ë¬¼ í•„í„°ë§)
        
        # FaceNet ëª¨ë¸ ì´ˆê¸°í™” (ì–¼êµ´ ì„ë² ë”©ìš©)
        self.model_manager = None
        self.resnet = None
        self.face_transform = None
        self._initialize_facenet()
        
        # í†µê³„
        self.stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'detection_calls': 0,
            'start_time': time.time(),
            'processing_time': 0.0
        }
        
        # ê²€ì¶œ íƒ€ì„ë¼ì¸ (ì˜¤ë””ì˜¤/íŠ¸ë¦¬ë°ìš©)
        self.detection_timeline = []  # [(frame_idx, has_person1, has_person2), ...]
        
        print(f"ğŸ—ï¸ DualFaceTrackingSystem ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ“¥ ì…ë ¥: {self.input_path}")
        print(f"   ğŸ“¤ ì¶œë ¥: {self.output_path}")
        print(f"   ğŸ”§ ê²€ì¶œ ê°„ê²©: {self.detection_interval}í”„ë ˆì„")
        print(f"   ğŸ“ í¬ë¡­ ë°°ìœ¨: {self.margin_factor}x")
        
    def _initialize_models(self):
        """ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ—ï¸ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°©ë²• 1: ìš°ë¦¬ì˜ ModelManager ì‚¬ìš© (ìµœìš°ì„ )
        if MODEL_MANAGER_AVAILABLE:
            try:
                from .model_manager import ModelManager
                model_manager = ModelManager()
                if model_manager.mtcnn is not None:
                    self.mtcnn = model_manager.mtcnn
                    self.detection_method = "mtcnn_manager"
                    print("âœ… ModelManager MTCNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    print(f"   ğŸ“ ë””ë°”ì´ìŠ¤: {model_manager.device}")
                    print("   ğŸ§  ê³ ì„±ëŠ¥ ì–¼êµ´ ê²€ì¶œ í™œì„±í™” (MTCNN)")
                    return
            except Exception as e:
                print(f"âš ï¸ ModelManager MTCNN ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: ì§ì ‘ MTCNN ë¡œë“œ
        try:
            from facenet_pytorch import MTCNN
            self.mtcnn = MTCNN(
                image_size=160,
                margin=0,
                min_face_size=20,
                thresholds=[0.6, 0.7, 0.7],
                factor=0.709,
                post_process=False,
                device='cuda' if torch.cuda.is_available() else 'cpu',
                keep_all=True,
                selection_method='largest_over_threshold'
            )
            self.detection_method = "mtcnn_direct"
            print("âœ… facenet-pytorch MTCNN ì§ì ‘ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ ì§ì ‘ MTCNN ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 3: ê¸°ì¡´ í”„ë¡œì íŠ¸ì˜ MTCNN ì‹œë„ (ìƒìœ„ ë””ë ‰í† ë¦¬)
        try:
            # ìƒìœ„ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
            parent_project = Path(__file__).parent.parent
            sys.path.insert(0, str(parent_project))
            sys.path.insert(0, str(parent_project / "src"))
            
            from face_tracker.core.models import ModelManager
            self.mtcnn, self.resnet = ModelManager.get_models()
            self.detection_method = "mtcnn"
            print("âœ… ìƒìœ„ í”„ë¡œì íŠ¸ MTCNN + FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ ìƒìœ„ í”„ë¡œì íŠ¸ MTCNN ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 4: OpenCV Haar Cascade (í´ë°±)
        try:
            # í™•ì¸ëœ ê²½ë¡œë¥¼ ì§ì ‘ ì‚¬ìš©
            cascade_path = "/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"
            
            if Path(cascade_path).exists():
                self.face_cascade = cv2.CascadeClassifier(cascade_path)
                if not self.face_cascade.empty():
                    self.detection_method = "haar"
                    print(f"âœ… OpenCV Haar Cascade ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (í´ë°±)")
                    print(f"   ğŸ“ ê²½ë¡œ: {cascade_path}")
                    print("   âš ï¸ ì„±ëŠ¥ ì œí•œ: MTCNN ëŒ€ì‹  Haar Cascade ì‚¬ìš©")
                    return
                else:
                    print(f"âš ï¸ Haar Cascade ìƒì„± ì‹¤íŒ¨: {cascade_path}")
            else:
                print(f"âš ï¸ Haar Cascade íŒŒì¼ ì—†ìŒ: {cascade_path}")
                
        except Exception as e:
            print(f"âš ï¸ Haar Cascade ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 5: MediaPipe ì–¼êµ´ ê²€ì¶œ
        try:
            import mediapipe as mp
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_drawing = mp.solutions.drawing_utils
            self.face_detection = self.mp_face_detection.FaceDetection(
                model_selection=1, min_detection_confidence=0.3)
            self.detection_method = "mediapipe"
            print("âœ… MediaPipe ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ MediaPipe ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        raise RuntimeError("âŒ ëª¨ë“  ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
    
    def _initialize_facenet(self):
        """FaceNet ëª¨ë¸ ì´ˆê¸°í™” (ì–¼êµ´ ì„ë² ë”©ìš©)"""
        print("ğŸ§  FaceNet ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if not MODEL_MANAGER_AVAILABLE:
            print("âš ï¸ ModelManagerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ. ì„ë² ë”© ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
            
        try:
            # ModelManagerë¡œ FaceNet ëª¨ë¸ ë¡œë“œ
            from .model_manager import ModelManager
            self.model_manager = ModelManager()
            self.resnet = self.model_manager.facenet
            
            # ì–¼êµ´ ì „ì²˜ë¦¬ ë³€í™˜ (FaceNetìš©)
            self.face_transform = transforms.Compose([
                transforms.Resize((160, 160)),  # FaceNet ì…ë ¥ í¬ê¸°
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            print("âœ… FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"   ğŸ“ ë””ë°”ì´ìŠ¤: {self.model_manager.device}")
            print("   ğŸ§  ì–¼êµ´ ì„ë² ë”© ê¸°ëŠ¥ í™œì„±í™”")
            
        except Exception as e:
            print(f"âš ï¸ FaceNet ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ğŸ”„ ì„ë² ë”© ì—†ì´ ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì ë§Œ ì‚¬ìš©")
            self.model_manager = None
            self.resnet = None
            self.face_transform = None
    
    def generate_face_embedding(self, face_crop: np.ndarray) -> Optional[torch.Tensor]:
        """ì–¼êµ´ í¬ë¡­ì—ì„œ ì„ë² ë”© ìƒì„±"""
        if self.resnet is None or self.face_transform is None:
            return None
            
        try:
            # OpenCV BGR -> PIL RGB ë³€í™˜
            rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_crop)
            
            # ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            face_tensor = self.face_transform(pil_image).unsqueeze(0)
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                face_tensor = face_tensor.cuda()
            
            # ì„ë² ë”© ìƒì„± (ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”)
            with torch.no_grad():
                embedding = self.resnet(face_tensor)
                # L2 ì •ê·œí™”
                embedding = F.normalize(embedding, p=2, dim=1)
            
            return embedding.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def detect_faces(self, frame: np.ndarray) -> List[FaceDetection]:
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        faces = []
        
        try:
            # ë””ë²„ê¹…: ê²€ì¶œ ë°©ë²• í™•ì¸
            if not self._debug_detection_logged:
                print(f"ğŸ”§ ê²€ì¶œ ë°©ë²•: {self.detection_method}")
                self._debug_detection_logged = True
            
            if self.detection_method == "mtcnn":
                faces = self._detect_faces_mtcnn(frame)
            elif self.detection_method == "mtcnn_manager":
                faces = self._detect_faces_mtcnn_direct(frame)
            elif self.detection_method == "mtcnn_direct":
                faces = self._detect_faces_mtcnn_direct(frame)
            elif self.detection_method == "haar":
                faces = self._detect_faces_haar(frame)
            elif self.detection_method == "mediapipe":
                faces = self._detect_faces_mediapipe(frame)
            elif self.detection_method == "dnn":
                faces = self._detect_faces_dnn(frame)
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê²€ì¶œ ë°©ë²•: {self.detection_method}")
                return []
            
            # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°)
            faces.sort(key=lambda x: x.confidence, reverse=True)
            
            # Phase 1: ì‘ì€ ì–¼êµ´ í•„í„°ë§ (ë°°ê²½ ì¸ë¬¼ ì œê±°)
            faces = [face for face in faces if face.width >= self.MIN_FACE_SIZE and face.height >= self.MIN_FACE_SIZE]
            
            # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì„ íƒ
            faces = faces[:2]
            
            # ê° ì–¼êµ´ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
            for face in faces:
                try:
                    # ì–¼êµ´ ì˜ì—­ í¬ë¡­
                    x1, y1, x2, y2 = face.bbox
                    face_crop = frame[y1:y2, x1:x2]
                    
                    # ìœ íš¨í•œ í¬ë¡­ì¸ì§€ í™•ì¸
                    if face_crop.size > 0:
                        # ì„ë² ë”© ìƒì„±
                        embedding = self.generate_face_embedding(face_crop)
                        face.embedding = embedding
                        
                except Exception as e:
                    print(f"âš ï¸ ì–¼êµ´ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    face.embedding = None
            
            return faces
            
        except Exception as e:
            print(f"âš ï¸ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _detect_faces_mtcnn(self, frame: np.ndarray) -> List[FaceDetection]:
        """MTCNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        boxes_list, probs_list = self.mtcnn.detect([pil_image])
        faces = []
        
        if boxes_list[0] is not None and probs_list[0] is not None:
            boxes = boxes_list[0]
            probs = probs_list[0]
            
            for box, prob in zip(boxes, probs):
                if prob > self.confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    faces.append(FaceDetection((x1, y1, x2, y2), prob))
        
        return faces
    
    def _detect_faces_haar(self, frame: np.ndarray) -> List[FaceDetection]:
        """Haar Cascadeë¡œ ì–¼êµ´ ê²€ì¶œ"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ê· í˜•ì¡íŒ íŒŒë¼ë¯¸í„°ë¡œ ì•ˆì •ì ì¸ ì–¼êµ´ ê²€ì¶œ
        detected_faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.08,    # 1.1â†’1.08 (ì¡°ê¸ˆ ë” ì„¸ë°€í•œ ê²€ì¶œ)
            minNeighbors=4,      # 5â†’4 (ì•½ê°„ ë” ê´€ëŒ€í•˜ê²Œ)
            minSize=(25, 25),    # 30â†’25 (ì¡°ê¸ˆ ë” ì‘ì€ ì–¼êµ´ë„ í¬í•¨)
            maxSize=(180, 180),  # 200â†’180 (ë” ì ì ˆí•œ ìµœëŒ€ í¬ê¸°)
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        faces = []
        for (x, y, w, h) in detected_faces:
            # ì–¼êµ´ í¬ê¸° ë° ì¢…íš¡ë¹„ ê²€ì¦
            if w > 20 and h > 20 and w < 150 and h < 150:
                # ì¢…íš¡ë¹„ ì²´í¬ (ì–¼êµ´ì€ ëŒ€ëµ ì •ì‚¬ê°í˜•)
                aspect_ratio = w / h
                if 0.7 < aspect_ratio < 1.3:
                    confidence = 0.9  # HaarëŠ” ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë¯€ë¡œ ê³ ì •ê°’
                    faces.append(FaceDetection((x, y, x + w, y + h), confidence))
        
        return faces
    
    def _detect_faces_dnn(self, frame: np.ndarray) -> List[FaceDetection]:
        """DNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ"""
        h, w = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123])
        self.dnn_net.setInput(blob)
        detections = self.dnn_net.forward()
        
        faces = []
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            
            if confidence > self.confidence_threshold:
                x1 = int(detections[0, 0, i, 3] * w)
                y1 = int(detections[0, 0, i, 4] * h)
                x2 = int(detections[0, 0, i, 5] * w)
                y2 = int(detections[0, 0, i, 6] * h)
                
                faces.append(FaceDetection((x1, y1, x2, y2), confidence))
        
        return faces
    
    def _detect_faces_mtcnn_direct(self, frame: np.ndarray) -> List[FaceDetection]:
        """facenet-pytorch MTCNNìœ¼ë¡œ ì§ì ‘ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # MTCNN ê²€ì¶œ
        boxes, probs = self.mtcnn.detect(pil_image)
        faces = []
        
        if boxes is not None and probs is not None:
            for box, prob in zip(boxes, probs):
                if prob > self.confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    faces.append(FaceDetection((x1, y1, x2, y2), prob))
        
        return faces
    
    def _detect_faces_mediapipe(self, frame: np.ndarray) -> List[FaceDetection]:
        """MediaPipeë¡œ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb_frame)
        
        faces = []
        if results.detections:
            h, w, _ = frame.shape
            
            for detection in results.detections:
                # MediaPipe bounding box (normalized coordinates)
                bbox = detection.location_data.relative_bounding_box
                x1 = int(bbox.xmin * w)
                y1 = int(bbox.ymin * h)
                x2 = int((bbox.xmin + bbox.width) * w)
                y2 = int((bbox.ymin + bbox.height) * h)
                
                # ì‹ ë¢°ë„ ì ìˆ˜
                confidence = detection.score[0]
                
                if confidence > self.confidence_threshold:
                    faces.append(FaceDetection((x1, y1, x2, y2), confidence))
        
        return faces
    
    def _assign_by_size(self, faces: List[FaceDetection]) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ Person1(ê°€ì¥ í° ì–¼êµ´), Person2(ë‘ ë²ˆì§¸ í° ì–¼êµ´) í• ë‹¹"""
        if not faces:
            return None, None
        
        # ì–¼êµ´ í¬ê¸°(area)ë¡œ ì •ë ¬ - í° ìˆœì„œëŒ€ë¡œ
        sorted_faces = sorted(faces, key=lambda f: f.area, reverse=True)
        
        # ê°€ì¥ í° ì–¼êµ´ â†’ Person1 (ì™¼ìª½)
        person1_face = sorted_faces[0] if len(sorted_faces) >= 1 else None
        
        # ë‘ ë²ˆì§¸ë¡œ í° ì–¼êµ´ â†’ Person2 (ì˜¤ë¥¸ìª½)
        person2_face = sorted_faces[1] if len(sorted_faces) >= 2 else None
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        if self.debug_mode and person1_face and person2_face:
            size_ratio = person1_face.area / person2_face.area
            print(f"ğŸ“Š í¬ê¸° ë¹„êµ: P1={person1_face.area:.0f}, P2={person2_face.area:.0f}, ë¹„ìœ¨={size_ratio:.2f}")
        
        return person1_face, person2_face

    def assign_face_ids(self, faces: List[FaceDetection], frame: np.ndarray, frame_idx: int) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­: ì„ë² ë”© ìœ ì‚¬ë„(70%) + ìœ„ì¹˜ ê¸°ë°˜(30%) í• ë‹¹"""
        if len(faces) == 0:
            return None, None
        
        # 1. Reference embeddingì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ ì‚¬ìš©
        if self.speaker1_reference is not None and self.speaker2_reference is not None:
            person1_face, person2_face = self._hybrid_face_matching(faces, frame, frame_idx)
        else:
            # 2. Reference embeddingì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì 
            person1_face, person2_face = self.position_tracker.track_faces(faces, frame_idx, frame)
        
        # ê° íŠ¸ë˜ì»¤ì— ì—…ë°ì´íŠ¸
        if person1_face:
            self.person1_tracker.update_detection(person1_face)
        if person2_face:
            self.person2_tracker.update_detection(person2_face)
        
        # ì¶”ê°€ ë””ë²„ê·¸ ì •ë³´ (StablePositionTracker ë‚´ë¶€ ë””ë²„ê·¸ì™€ ë³„ë„)
        if self.debug_mode and frame_idx % 30 == 0:
            tracking_stats = self.position_tracker.get_tracking_stats()
            print(f"ğŸ”„ ì¶”ì  ìƒíƒœ: ì´ˆê¸°í™”={tracking_stats['is_initialized']}, P1íˆìŠ¤í† ë¦¬={tracking_stats['person1_history_length']}, P2íˆìŠ¤í† ë¦¬={tracking_stats['person2_history_length']}")
            
            if person1_face and person2_face:
                size_ratio = person1_face.area / person2_face.area
                print(f"ğŸ“Š í”„ë ˆì„ {frame_idx}: P1=ìœ„ì¹˜{person1_face.center} í¬ê¸°{person1_face.area:.0f}, P2=ìœ„ì¹˜{person2_face.center} í¬ê¸°{person2_face.area:.0f}, ë¹„ìœ¨={size_ratio:.2f}")
            elif person1_face:
                print(f"ğŸ“Š í”„ë ˆì„ {frame_idx}: P1=ìœ„ì¹˜{person1_face.center} í¬ê¸°{person1_face.area:.0f}, P2=ì—†ìŒ")
        
        return person1_face, person2_face
    
    def _hybrid_face_matching(self, faces: List[FaceDetection], frame: np.ndarray, frame_idx: int) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­: ì„ë² ë”© ìœ ì‚¬ë„(90%) + ìœ„ì¹˜ ë§¤ì¹­(10%) - í€„ë¦¬í‹° ê°œì„ """
        import torch.nn.functional as F
        
        if len(faces) == 0:
            return None, None
        
        # ModelManagerê°€ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ í´ë°±
        if not hasattr(self, 'model_manager') or self.model_manager is None:
            return self.position_tracker.track_faces(faces, frame_idx, frame)
        
        # ê° ì–¼êµ´ì— ëŒ€í•´ ì„ë² ë”© ì¶”ì¶œ
        face_embeddings = []
        valid_faces = []
        
        for face in faces:
            try:
                # ì–¼êµ´ í¬ë¡­ ì¶”ì¶œ (í”„ë ˆì„ ì „ë‹¬)
                face_crop = self.model_manager.extract_face_crop(face, frame)
                if face_crop is not None:
                    # ì„ë² ë”© ê³„ì‚°
                    embedding = self.model_manager.get_embedding(face_crop)
                    if embedding is not None:
                        face_embeddings.append(embedding)
                        valid_faces.append(face)
            except Exception as e:
                if self.debug_mode:
                    print(f"âš ï¸ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨ (ì–¼êµ´ {face.center}): {e}")
                continue
        
        # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ í´ë°±
        if len(face_embeddings) == 0:
            return self.position_tracker.track_faces(faces, frame_idx, frame)
        
        # ê° ì–¼êµ´ì— ëŒ€í•´ Speaker1, Speaker2ì™€ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        speaker1_scores = []
        speaker2_scores = []
        
        for i, (face, embedding) in enumerate(zip(valid_faces, face_embeddings)):
            # 1. ì„ë² ë”© ìœ ì‚¬ë„ ì ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„, 0~1)
            embedding_sim1 = F.cosine_similarity(embedding.unsqueeze(0), self.speaker1_reference.unsqueeze(0)).item()
            embedding_sim2 = F.cosine_similarity(embedding.unsqueeze(0), self.speaker2_reference.unsqueeze(0)).item()
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™” (-1~1 â†’ 0~1)
            embedding_sim1 = (embedding_sim1 + 1) / 2
            embedding_sim2 = (embedding_sim2 + 1) / 2
            
            # 2. ìœ„ì¹˜ ìœ ì‚¬ë„ ì ìˆ˜ (ê±°ë¦¬ ê¸°ë°˜, 0~1) - ì§ì ‘ ê³„ì‚°
            # Person1, Person2 ì˜ˆìƒ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self.position_tracker, 'person1_history') and self.position_tracker.person1_history:
                predicted_p1_pos = self.position_tracker._get_predicted_position(self.position_tracker.person1_history)
                dist_to_p1 = self.position_tracker._calculate_distance(face.center, predicted_p1_pos)
                # ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜, 200px ê¸°ì¤€)
                position_score1 = max(0, 1 - dist_to_p1 / 200)
            else:
                position_score1 = 0.5
                
            if hasattr(self.position_tracker, 'person2_history') and self.position_tracker.person2_history:
                predicted_p2_pos = self.position_tracker._get_predicted_position(self.position_tracker.person2_history)
                dist_to_p2 = self.position_tracker._calculate_distance(face.center, predicted_p2_pos)
                # ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜, 200px ê¸°ì¤€)
                position_score2 = max(0, 1 - dist_to_p2 / 200)
            else:
                position_score2 = 0.5
            
            # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì„ë² ë”© 90% + ìœ„ì¹˜ 10%) - í€„ë¦¬í‹° ê°œì„ 
            hybrid_score1 = 0.9 * embedding_sim1 + 0.1 * position_score1
            hybrid_score2 = 0.9 * embedding_sim2 + 0.1 * position_score2
            
            speaker1_scores.append(hybrid_score1)
            speaker2_scores.append(hybrid_score2)
            
            if self.debug_mode and frame_idx % 60 == 0:  # 2ì´ˆë§ˆë‹¤ ì¶œë ¥
                print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ (í”„ë ˆì„ {frame_idx}): ì–¼êµ´{i} â†’ "
                      f"S1ì ìˆ˜={hybrid_score1:.3f}(ì„ë² ë”©={embedding_sim1:.3f}+ìœ„ì¹˜={position_score1:.3f}), "
                      f"S2ì ìˆ˜={hybrid_score2:.3f}(ì„ë² ë”©={embedding_sim2:.3f}+ìœ„ì¹˜={position_score2:.3f})")
        
        # ì„ê³„ê°’ ê¸°ë°˜ í•„í„°ë§
        threshold = self.speaker_similarity_threshold  # 0.5
        
        # Speaker1, Speaker2ì— ê°€ì¥ ì í•©í•œ ì–¼êµ´ ì„ íƒ
        person1_candidates = [(i, score) for i, score in enumerate(speaker1_scores) if score >= threshold]
        person2_candidates = [(i, score) for i, score in enumerate(speaker2_scores) if score >= threshold]
        
        # ìµœê³  ì ìˆ˜ ì–¼êµ´ ì„ íƒ
        person1_face = None
        person2_face = None
        
        if person1_candidates:
            best_idx = max(person1_candidates, key=lambda x: x[1])[0]
            person1_face = valid_faces[best_idx]
        
        if person2_candidates:
            # Person1ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì–¼êµ´ ì„ íƒ
            available_candidates = [c for c in person2_candidates if valid_faces[c[0]] != person1_face]
            if available_candidates:
                best_idx = max(available_candidates, key=lambda x: x[1])[0]
                person2_face = valid_faces[best_idx]
        
        if self.debug_mode and frame_idx % 60 == 0:
            p1_status = f"P1=ì„ê³„ê°’í†µê³¼" if person1_face else "P1=ì„ê³„ê°’ë¯¸ë‹¬"
            p2_status = f"P2=ì„ê³„ê°’í†µê³¼" if person2_face else "P2=ì„ê³„ê°’ë¯¸ë‹¬"
            print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼: {p1_status}, {p2_status} (ì„ê³„ê°’={threshold})")
        
        return person1_face, person2_face
    
    # ì»´íŒ©íŠ¸í•œ í¬ê¸° ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ êµì²´ëœ ë³µì¡í•œ ë¡œì§ë“¤ì€ ì œê±°ë¨
    
    def create_adaptive_split_screen(self, crop1: np.ndarray, crop2: np.ndarray, 
                                   face1_size: float, face2_size: float) -> Tuple[np.ndarray, float]:
        """ì–¼êµ´ í¬ê¸°ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìŠ¤í”Œë¦¿ ë¹„ìœ¨ ì¡°ì •"""
        # ì–¼êµ´ í¬ê¸° ë¹„ìœ¨ ê³„ì‚° (30:70 ~ 70:30 ì œí•œ)
        total_size = face1_size + face2_size
        if total_size > 0:
            ratio1 = face1_size / total_size
            ratio1 = max(0.3, min(0.7, ratio1))  # 30~70% ë²”ìœ„ ì œí•œ
        else:
            ratio1 = 0.5  # ê¸°ë³¸ê°’
        
        ratio2 = 1 - ratio1
        
        # í™”ë©´ ë„ˆë¹„ ë¶„í•  ê³„ì‚°
        width1 = int(1920 * ratio1)
        width2 = 1920 - width1
        
        # ë™ì  ë¦¬ì‚¬ì´ì¦ˆ (ì–¼êµ´ í¬ê¸°ì— ë”°ë¼ í™”ë©´ ì˜ì—­ í• ë‹¹)
        resized_crop1 = cv2.resize(crop1, (width1, 1080))
        resized_crop2 = cv2.resize(crop2, (width2, 1080))
        
        # ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ìƒì„±
        split_screen = np.zeros((1080, 1920, 3), dtype=np.uint8)
        split_screen[0:1080, 0:width1] = resized_crop1
        split_screen[0:1080, width1:1920] = resized_crop2
        
        # êµ¬ë¶„ì„  ì¶”ê°€ (ì‹œê°ì  êµ¬ë¶„)
        cv2.line(split_screen, (width1, 0), (width1, 1080), (128, 128, 128), 2)
        
        return split_screen, ratio1
    
    def create_split_screen(self, crop1: np.ndarray, crop2: np.ndarray) -> np.ndarray:
        """ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ìƒì„± (1920x1080) - 50:50 ê³ ì • ë¹„ìœ¨"""
        # 1920x1080 ìº”ë²„ìŠ¤ ìƒì„±
        split_screen = np.zeros((1080, 1920, 3), dtype=np.uint8)
        
        # ì™¼ìª½: Person1 (0:960)
        split_screen[0:1080, 0:960] = crop1
        
        # ì˜¤ë¥¸ìª½: Person2 (960:1920)
        split_screen[0:1080, 960:1920] = crop2
        
        return split_screen
    
    def add_overlay_info(self, frame: np.ndarray, frame_idx: int, fps: float) -> np.ndarray:
        """ì˜¤ë²„ë ˆì´ ì •ë³´ ì¶”ê°€"""
        # ì§„í–‰ë¥  ê³„ì‚°
        progress = (frame_idx / max(1, self.stats['total_frames'])) * 100
        
        # íŠ¸ë˜ì»¤ í†µê³„
        p1_stats = self.person1_tracker.get_stats()
        p2_stats = self.person2_tracker.get_stats()
        
        # í…ìŠ¤íŠ¸ ì •ë³´
        texts = [
            f"Frame: {frame_idx}/{self.stats['total_frames']}",
            f"Progress: {progress:.1f}%",
            f"FPS: {fps:.1f}",
            f"P1 Track: {p1_stats['success_rate']:.1f}%",
            f"P2 Track: {p2_stats['success_rate']:.1f}%",
            "Dual-Face Tracking v5.0"
        ]
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        for i, text in enumerate(texts):
            y_pos = 30 + i * 35
            cv2.putText(frame, text, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        return frame
    
    def process(self):
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘")
        
        # ğŸ†• ìƒˆë¡œìš´ 1ë‹¨ê³„: ìë™ í™”ì ì„ ì • (AutoSpeakerDetector or OneMinuteAnalyzer)
        prescan_profiles = None
        
        if self.one_minute_mode:
            print("\n" + "=" * 50)
            print("ğŸ¯ 1ë‹¨ê³„: 1ë¶„ ì§‘ì¤‘ ë¶„ì„ (OneMinuteAnalyzer)")
            print("=" * 50)
            
            from .auto_speaker_detector import OneMinuteAnalyzer
            analyzer = OneMinuteAnalyzer(debug_mode=self.debug_mode)
            person1_profile, person2_profile = analyzer.analyze_first_minute(self.input_path)
            
            if person1_profile and person2_profile:
                # í™”ì ì •ë³´ ì €ì¥
                self.speaker1_reference = person1_profile['reference_embedding']
                self.speaker2_reference = person2_profile['reference_embedding']
                
                # ìœ„ì¹˜ ì •ë³´ë¥¼ prescan_profilesë¡œ ë³€í™˜
                prescan_profiles = (
                    person1_profile['average_position'],
                    person2_profile['average_position']
                )
                
                # Phase 2: IdentityBank ì €ì¥ (SimpleConsistentTrackerì™€ ê³µìœ )
                self.identity_bank = analyzer.identity_bank
                
                # SimpleConsistentTrackerìš© í”„ë¡œíŒŒì¼ ì €ì¥
                self.person1_profile = person1_profile
                self.person2_profile = person2_profile
                
                print(f"âœ… 1ë¶„ ì§‘ì¤‘ ë¶„ì„ ì™„ë£Œ:")
                print(f"   Person1: {person1_profile['appearance_count']}ê°œ ì–¼êµ´, IdentityBank: {person1_profile['identity_bank_size']}ê°œ")
                print(f"   Person2: {person2_profile['appearance_count']}ê°œ ì–¼êµ´, IdentityBank: {person2_profile['identity_bank_size']}ê°œ") 
                print(f"   ìœ„ì¹˜: P1={prescan_profiles[0]}, P2={prescan_profiles[1]}")
                print(f"   ğŸ’ª IdentityBank ì¤€ë¹„: AìŠ¬ë¡¯={len(self.identity_bank.bank['A'])}ê°œ, BìŠ¬ë¡¯={len(self.identity_bank.bank['B'])}ê°œ")
                
            else:
                print("âš ï¸ 1ë¶„ ì§‘ì¤‘ ë¶„ì„ ì‹¤íŒ¨, í´ë°± ëª¨ë“œë¡œ ì²˜ë¦¬")
                self.one_minute_mode = False  # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë“œ ì‚¬ìš©
                
        elif self.auto_speaker_enabled:
            print("\n" + "=" * 50)
            print("ğŸ¯ 1ë‹¨ê³„: ìë™ í™”ì ì„ ì • (AutoSpeakerDetector)")
            print("=" * 50)
            
            auto_detector = AutoSpeakerDetector(debug_mode=self.debug_mode)
            speaker1_info, speaker2_info = auto_detector.analyze_video(self.input_path)
            
            if speaker1_info and speaker2_info:
                # í™”ì ì •ë³´ ì €ì¥
                self.speaker1_reference = speaker1_info['representative_embedding']
                self.speaker2_reference = speaker2_info['representative_embedding']
                
                # ìœ„ì¹˜ ì •ë³´ë¥¼ prescan_profilesë¡œ ë³€í™˜
                prescan_profiles = (
                    speaker1_info['average_position'],
                    speaker2_info['average_position']
                )
                
                print(f"âœ… ìë™ í™”ì ì„ ì • ì™„ë£Œ:")
                print(f"   í™”ì1: {speaker1_info['appearance_count']}íšŒ ë“±ì¥, ì ìˆ˜ {speaker1_info['importance_score']:.3f}")
                print(f"   í™”ì2: {speaker2_info['appearance_count']}íšŒ ë“±ì¥, ì ìˆ˜ {speaker2_info['importance_score']:.3f}")
                print(f"   ìœ„ì¹˜: P1={prescan_profiles[0]}, P2={prescan_profiles[1]}")
                
            else:
                print("âš ï¸ ìë™ í™”ì ì„ ì • ì‹¤íŒ¨, í´ë°± ëª¨ë“œë¡œ ì²˜ë¦¬")
                self.speaker1_reference = None
                self.speaker2_reference = None
        
        # í´ë°±: ê¸°ì¡´ ì‚¬ì „ ë¶„ì„ (prescan ì˜µì…˜ì´ ì¼œì ¸ìˆê±°ë‚˜ ìë™ í™”ì ì„ ì • ì‹¤íŒ¨ì‹œ)
        elif self.prescan_enabled or (self.auto_speaker_enabled and prescan_profiles is None):
            print("\n" + "=" * 50)
            print("ğŸ” í´ë°±: ê¸°ì¡´ ì‚¬ì „ ë¶„ì„ (SimplePreScanner)")
            print("=" * 50)
            scanner = SimplePreScanner(debug_mode=self.debug_mode)
            prescan_profiles = scanner.quick_scan(self.input_path, sample_rate=30)
            
            if prescan_profiles:
                p1, p2 = prescan_profiles
                print(f"âœ… íƒ€ê²Ÿ í”„ë¡œíŒŒì¼ í™•ì •:")
                print(f"   Person1: ìœ„ì¹˜ {p1}")
                print(f"   Person2: ìœ„ì¹˜ {p2}")
            else:
                print("âš ï¸ ì‚¬ì „ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì²˜ë¦¬")
        
        # ë©”ì¸ ì²˜ë¦¬ ë‹¨ê³„ í‘œì‹œ
        print("\n" + "=" * 50)
        print("ğŸ¬ 2ë‹¨ê³„: ë©”ì¸ ì²˜ë¦¬")
        print("=" * 50)
        
        # ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì ì ì´ˆê¸°í™” (í”„ë¡œíŒŒì¼ê³¼ í•¨ê»˜)
        self.position_tracker = StablePositionTracker(
            debug_mode=self.debug_mode,
            prescan_profiles=prescan_profiles
        )
        
        # Speaker reference ì„ë² ë”© ë° ModelManager ì „ë‹¬
        if hasattr(self, 'speaker1_reference') and hasattr(self, 'speaker2_reference'):
            self.position_tracker.speaker1_reference = self.speaker1_reference
            self.position_tracker.speaker2_reference = self.speaker2_reference
        if hasattr(self, 'model_manager'):
            self.position_tracker.model_manager = self.model_manager
        
        # ë¹„ë””ì˜¤ ì—´ê¸°
        cap = cv2.VideoCapture(self.input_path)
        if not cap.isOpened():
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {self.input_path}")
        
        # ë¹„ë””ì˜¤ ì •ë³´
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / fps if fps > 0 else 0
        
        self.stats['total_frames'] = total_frames
        
        print(f"   ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps:.1f}fps")
        print(f"   â±ï¸ ì§€ì†ì‹œê°„: {duration:.1f}ì´ˆ, {total_frames}í”„ë ˆì„")
        
        # SimpleConsistentTracker ì´ˆê¸°í™” (1ë¶„ ë¶„ì„ ëª¨ë“œ)
        simple_tracker = None
        if self.one_minute_mode and hasattr(self, 'person1_profile') and hasattr(self, 'person2_profile'):
            # Phase 2: IdentityBankë¥¼ SimpleConsistentTrackerì— ì „ë‹¬
            identity_bank = getattr(self, 'identity_bank', None)
            simple_tracker = SimpleConsistentTracker(
                self.person1_profile, 
                self.person2_profile, 
                debug_mode=self.debug_mode,
                identity_bank=identity_bank
            )
            print(f"âœ… SimpleConsistentTracker ì´ˆê¸°í™” ì™„ë£Œ (IdentityBank: {'ì—°ê²°ë¨' if identity_bank else 'ì—†ìŒ'})")
        
        # Phase 3: HungarianFaceAssigner ì´ˆê¸°í™”
        hungarian_assigner = None
        if self.hungarian_mode and hasattr(self, 'identity_bank') and self.identity_bank is not None:
            hungarian_assigner = HungarianFaceAssigner(
                self.identity_bank,
                debug_mode=self.debug_mode
            )
            print(f"âœ… HungarianFaceAssigner ì´ˆê¸°í™” ì™„ë£Œ (IdentityBank A:{len(self.identity_bank.bank['A'])}, B:{len(self.identity_bank.bank['B'])})")
        elif self.hungarian_mode:
            print(f"âš ï¸ Hungarian ëª¨ë“œ ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ IdentityBank ì—†ìŒ. ê¸°ë³¸ ëª¨ë“œë¡œ ì§„í–‰")
            self.hungarian_mode = False
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • (1920x1080 ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦°)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(self.output_path, fourcc, fps, (1920, 1080))
        
        if not writer.isOpened():
            cap.release()
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨: {self.output_path}")
        
        # ì§„í–‰ë¥  í‘œì‹œ
        pbar = tqdm(total=total_frames, desc="ğŸ¯ ì–¼êµ´ íŠ¸ë˜í‚¹", ncols=80, leave=True)
        
        try:
            frame_idx = 0
            start_time = time.time()
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_idx += 1
                
                # 1. ì–¼êµ´ ê²€ì¶œ (ë§¤ í”„ë ˆì„ ë˜ëŠ” Ní”„ë ˆì„ë§ˆë‹¤)
                if self.detection_interval == 1 or frame_idx % self.detection_interval == 0:
                    # ë””ë²„ê¹…: ê²€ì¶œ ì „ í”„ë ˆì„ ì •ë³´
                    if frame_idx <= 10:
                        print(f"ğŸ” í”„ë ˆì„ {frame_idx} ê²€ì¶œ ì‹œë„ - í”„ë ˆì„ í¬ê¸°: {frame.shape}")
                    
                    faces = self.detect_faces(frame)
                    self.stats['detection_calls'] += 1
                    
                    # ë””ë²„ê¹…: ì–¼êµ´ ê²€ì¶œ ê²°ê³¼ ë¡œê·¸ (ë§¤ í”„ë ˆì„ ì¶œë ¥)
                    if frame_idx <= 50 or frame_idx % 200 == 1:  # ì²˜ìŒ 50í”„ë ˆì„ê³¼ 200í”„ë ˆì„ë§ˆë‹¤
                        print(f"ğŸ” í”„ë ˆì„ {frame_idx}: {len(faces)}ê°œ ì–¼êµ´ ê²€ì¶œ (ë°©ë²•: {self.detection_method})")
                        for i, face in enumerate(faces):
                            print(f"   ì–¼êµ´ {i+1}: ({face.x1},{face.y1})-({face.x2},{face.y2}) conf={face.confidence:.2f}")
                    
                    # ì–¼êµ´ ID í• ë‹¹ (Hungarian > SimpleTracker > ê¸°ì¡´ ëª¨ë“œ ìˆœì„œ)
                    if hungarian_assigner:
                        # Phase 3: Hungarian Matching í• ë‹¹
                        person1_face, person2_face = hungarian_assigner.assign_faces(faces, frame)
                        if frame_idx <= 60:
                            print(f"   ğŸ§® Hungarian í• ë‹¹ (í”„ë ˆì„ {frame_idx}): P1={'âœ…' if person1_face else 'âŒ'}, P2={'âœ…' if person2_face else 'âŒ'}")
                    elif simple_tracker:
                        # 1ë¶„ ë¶„ì„ ê¸°ë°˜ ë‹¨ìˆœ ì¶”ì 
                        person1_face, person2_face = simple_tracker.track_frame(faces, frame)
                        if frame_idx <= 60:
                            print(f"   ğŸ¯ SimpleTracker í• ë‹¹ (í”„ë ˆì„ {frame_idx}): P1={'âœ…' if person1_face else 'âŒ'}, P2={'âœ…' if person2_face else 'âŒ'}")
                    else:
                        # ê¸°ì¡´ ë³µì¡í•œ í• ë‹¹ ì‹œìŠ¤í…œ
                        person1_face, person2_face = self.assign_face_ids(faces, frame, frame_idx)
                    
                    # ë””ë²„ê¹…: ID í• ë‹¹ ê²°ê³¼ (ì¼ê´€ì„± ì¶”ì )
                    if frame_idx <= 60:  # ì²˜ìŒ 60í”„ë ˆì„ ë™ì•ˆ ìƒì„¸ ë¡œê·¸
                        p1_assigned = person1_face is not None
                        p2_assigned = person2_face is not None
                        print(f"   ğŸ¯ ID í• ë‹¹ (í”„ë ˆì„ {frame_idx}): P1={p1_assigned}, P2={p2_assigned}")
                        
                        if person1_face:
                            print(f"     P1 ì–¼êµ´: center={person1_face.center} conf={person1_face.confidence:.2f}")
                        if person2_face:
                            print(f"     P2 ì–¼êµ´: center={person2_face.center} conf={person2_face.confidence:.2f}")
                            
                        # ê¸°ì¡´ ì´ˆê¸° ìœ„ì¹˜ ê¸°ë°˜ ì½”ë“œ ì œê±°ë¨ (í¬ê¸° ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    
                    # ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸ (íŠ¸ë˜ì»¤ ì—†ìŒ)
                    if person1_face:
                        result1 = self.person1_tracker.update_detection(person1_face)
                        if frame_idx <= 50:
                            print(f"   âœ… P1 ê²€ì¶œ ì—…ë°ì´íŠ¸: {result1}")
                    else:
                        self.person1_tracker.update_detection(None)
                        
                    if person2_face:
                        result2 = self.person2_tracker.update_detection(person2_face)
                        if frame_idx <= 50:
                            print(f"   âœ… P2 ê²€ì¶œ ì—…ë°ì´íŠ¸: {result2}")
                    else:
                        self.person2_tracker.update_detection(None)
                
                # ê²€ì¶œ íƒ€ì„ë¼ì¸ ê¸°ë¡ (ì˜¤ë””ì˜¤/íŠ¸ë¦¬ë°ìš©)
                has_person1 = person1_face is not None if 'person1_face' in locals() else False
                has_person2 = person2_face is not None if 'person2_face' in locals() else False
                self.detection_timeline.append((frame_idx, has_person1, has_person2))
                
                # 3. í¬ë¡­ ì˜ì—­ ìƒì„± (ê³ ì • ë§ˆì§„)
                crop1 = self.person1_tracker.get_crop_region(frame, self.margin_factor)
                crop2 = self.person2_tracker.get_crop_region(frame, self.margin_factor)
                
                # 4. ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ìƒì„± (50:50 ê³ ì • ë¹„ìœ¨)
                split_screen = self.create_split_screen(crop1, crop2)
                
                # 5. ì˜¤ë²„ë ˆì´ ì •ë³´ ì¶”ê°€
                current_fps = frame_idx / max(0.01, time.time() - start_time)
                split_screen = self.add_overlay_info(split_screen, frame_idx, current_fps)
                
                # 6. ì¶œë ¥
                writer.write(split_screen)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                self.stats['processed_frames'] = frame_idx
                pbar.update(1)
                
                # ì¤‘ê°„ ì§„í–‰ë¥  ì¶œë ¥ (100í”„ë ˆì„ë§ˆë‹¤)
                if frame_idx % 100 == 0:
                    p1_rate = self.person1_tracker.get_stats()['success_rate']
                    p2_rate = self.person2_tracker.get_stats()['success_rate']
                    pbar.set_postfix({
                        'P1': f'{p1_rate:.1f}%',
                        'P2': f'{p2_rate:.1f}%',
                        'FPS': f'{current_fps:.1f}'
                    })
        
        finally:
            cap.release()
            writer.release()
            pbar.close()
        
        # ìµœì¢… í†µê³„
        self.stats['processing_time'] = time.time() - start_time
        
        # FFmpeg í›„ì²˜ë¦¬ (ì˜¤ë””ì˜¤ ë³‘í•© + íŠ¸ë¦¬ë°)
        if ENABLE_FFMPEG_POST_PROCESSING:
            print(f"\nğŸ”„ FFmpeg í›„ì²˜ë¦¬ ì‹œì‘...")
            success = self._post_process_with_ffmpeg(fps)
            if success:
                print(f"âœ… FFmpeg í›„ì²˜ë¦¬ ì™„ë£Œ!")
            else:
                print(f"âš ï¸ FFmpeg í›„ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ë¹„ë””ì˜¤ ìœ ì§€")
        
        self._print_final_stats()
    
    def _post_process_with_ffmpeg(self, fps: float) -> bool:
        """FFmpegë¥¼ ì‚¬ìš©í•œ í›„ì²˜ë¦¬ (ì˜¤ë””ì˜¤ ë³‘í•© + íŠ¸ë¦¬ë°)
        
        Args:
            fps: ë¹„ë””ì˜¤ í”„ë ˆì„ ë ˆì´íŠ¸
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # 0. ì˜¤ë””ì˜¤ íŠ¸ë™ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            has_audio = self._check_audio_track(self.input_path)
            print(f"ğŸ”Š ì˜¤ë””ì˜¤ íŠ¸ë™: {'ìˆìŒ' if has_audio else 'ì—†ìŒ'}")
            
            # 1. íŠ¸ë¦¬ë° êµ¬ê°„ ê³„ì‚°
            keep_segments = self._calculate_trim_segments(fps)
            
            if not keep_segments:
                print("âš ï¸ ìœ ì§€í•  êµ¬ê°„ì´ ì—†ìŒ, í›„ì²˜ë¦¬ ê±´ë„ˆëœ€")
                return False
            
            # 2. ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            temp_video = self.output_path + ".temp_no_audio.mp4"
            final_output = self.output_path
            
            # 3. ê¸°ì¡´ ì¶œë ¥ì„ ì„ì‹œ íŒŒì¼ë¡œ ì´ë™
            if os.path.exists(final_output):
                os.rename(final_output, temp_video)
            else:
                print("âš ï¸ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŒ")
                return False
            
            # 4. FFmpeg ì²˜ë¦¬
            if not has_audio:
                # ì˜¤ë””ì˜¤ê°€ ì—†ëŠ” ê²½ìš° - ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬
                if TRIM_UNDETECTED_SEGMENTS and len(keep_segments) < len(self.detection_timeline):
                    success = self._ffmpeg_trim_video_only(temp_video, final_output, keep_segments)
                else:
                    # íŠ¸ë¦¬ë° ì—†ì´ ê·¸ëƒ¥ ì´ë™
                    os.rename(temp_video, final_output)
                    success = True
                    print("âœ… ì˜¤ë””ì˜¤ ì—†ëŠ” ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
            elif TRIM_UNDETECTED_SEGMENTS and len(keep_segments) < len(self.detection_timeline):
                # ì˜¤ë””ì˜¤ ìˆê³  íŠ¸ë¦¬ë°ì´ í•„ìš”í•œ ê²½ìš°
                success = self._ffmpeg_trim_and_merge_audio(temp_video, final_output, keep_segments)
            else:
                # ì˜¤ë””ì˜¤ ìˆê³  íŠ¸ë¦¬ë° ì—†ì´ ì˜¤ë””ì˜¤ë§Œ ë³‘í•©
                success = self._ffmpeg_merge_audio_only(temp_video, final_output)
            
            # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if success and os.path.exists(temp_video):
                try:
                    os.remove(temp_video)
                except:
                    pass
            
            return success
            
        except Exception as e:
            print(f"âŒ FFmpeg í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def _check_audio_track(self, video_path: str) -> bool:
        """ë¹„ë””ì˜¤ íŒŒì¼ì— ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            bool: ì˜¤ë””ì˜¤ íŠ¸ë™ ì¡´ì¬ ì—¬ë¶€
        """
        try:
            probe_cmd = [
                'ffprobe', '-v', 'error', '-select_streams', 'a:0', 
                '-show_entries', 'stream=codec_type', '-of', 'csv=p=0', video_path
            ]
            
            result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=10)
            return result.returncode == 0 and result.stdout.strip() == 'audio'
            
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False
    
    def _calculate_trim_segments(self, fps: float) -> List[Tuple[float, float]]:
        """ê²€ì¶œ íƒ€ì„ë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì§€í•  êµ¬ê°„ ê³„ì‚°
        
        Args:
            fps: ë¹„ë””ì˜¤ í”„ë ˆì„ ë ˆì´íŠ¸
            
        Returns:
            List[Tuple[float, float]]: [(ì‹œì‘_ì‹œê°„, ë_ì‹œê°„), ...] ìœ ì§€í•  êµ¬ê°„ë“¤
        """
        if not self.detection_timeline:
            return []
        
        print(f"ğŸ” íŠ¸ë¦¬ë° êµ¬ê°„ ê³„ì‚° ì¤‘... (ì´ {len(self.detection_timeline)}ê°œ í”„ë ˆì„)")
        
        segments_to_keep = []
        total_duration = len(self.detection_timeline) / fps
        
        # 1. ì²« í”„ë ˆì„ë¶€í„° ë¶„ì„í•˜ì—¬ ì´ˆê¸° êµ¬ê°„ ê²°ì •
        current_segment_start = 0  # í•­ìƒ 0ë¶€í„° ì‹œì‘
        current_undetected_start = None
        
        for i, (frame_idx, has_p1, has_p2) in enumerate(self.detection_timeline):
            current_time = frame_idx / fps
            
            # ê²€ì¶œ ì—¬ë¶€ í™•ì¸ (REQUIRE_BOTH_PERSONS ì„¤ì •ì— ë”°ë¼)
            if REQUIRE_BOTH_PERSONS:
                detected = has_p1 and has_p2  # ë‘˜ ë‹¤ ê²€ì¶œë˜ì–´ì•¼ í•¨
            else:
                detected = has_p1 or has_p2   # í•˜ë‚˜ë¼ë„ ê²€ì¶œë˜ë©´ ë¨
            
            if detected:
                # ê²€ì¶œëœ ìƒíƒœ
                if current_undetected_start is not None:
                    # ë¯¸ê²€ì¶œ êµ¬ê°„ ì¢…ë£Œ - 2ì´ˆ ì´ìƒì´ì—ˆëŠ”ì§€ í™•ì¸
                    undetected_duration = current_time - current_undetected_start
                    if undetected_duration >= UNDETECTED_THRESHOLD_SECONDS:
                        # 2ì´ˆ ì´ìƒ ë¯¸ê²€ì¶œì´ì—ˆìŒ -> ì´ì „ êµ¬ê°„ ì¢…ë£Œí•˜ê³  ìƒˆ êµ¬ê°„ ì‹œì‘
                        end_time = max(current_segment_start, current_undetected_start - TRIM_BUFFER_SECONDS)
                        if end_time > current_segment_start:
                            segments_to_keep.append((current_segment_start, end_time))
                        
                        # ìƒˆë¡œìš´ êµ¬ê°„ ì‹œì‘ (ë²„í¼ ê³ ë ¤)
                        current_segment_start = max(0, current_time - TRIM_BUFFER_SECONDS)
                    
                    # ë¯¸ê²€ì¶œ êµ¬ê°„ ì¢…ë£Œ (2ì´ˆ ë¯¸ë§Œì´ë©´ êµ¬ê°„ ìœ ì§€ë¨)
                    current_undetected_start = None
                
                # ê²€ì¶œ ì¤‘ì´ë¯€ë¡œ í˜„ì¬ êµ¬ê°„ ê³„ì† ìœ ì§€
            else:
                # ë¯¸ê²€ì¶œ ìƒíƒœ - ë¯¸ê²€ì¶œ êµ¬ê°„ ì‹œì‘ì  ê¸°ë¡
                if current_undetected_start is None:
                    current_undetected_start = current_time
        
        # 2. ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
        if current_undetected_start is not None:
            # ë§ˆì§€ë§‰ì— ë¯¸ê²€ì¶œ êµ¬ê°„ì´ ìˆìŒ
            final_undetected_duration = total_duration - current_undetected_start
            if final_undetected_duration >= UNDETECTED_THRESHOLD_SECONDS:
                # ë§ˆì§€ë§‰ ë¯¸ê²€ì¶œ êµ¬ê°„ì´ 2ì´ˆ ì´ìƒ -> ê·¸ ì „ê¹Œì§€ë§Œ í¬í•¨
                end_time = max(current_segment_start, current_undetected_start - TRIM_BUFFER_SECONDS)
                if end_time > current_segment_start:
                    segments_to_keep.append((current_segment_start, end_time))
            else:
                # ë§ˆì§€ë§‰ ë¯¸ê²€ì¶œ êµ¬ê°„ì´ 2ì´ˆ ë¯¸ë§Œ -> ëê¹Œì§€ í¬í•¨
                segments_to_keep.append((current_segment_start, total_duration))
        else:
            # ëê¹Œì§€ ê²€ì¶œë¨ -> í˜„ì¬ êµ¬ê°„ì„ ëê¹Œì§€ í¬í•¨
            segments_to_keep.append((current_segment_start, total_duration))
        
        # íŠ¹ë³„ ì²˜ë¦¬: ì „ì²´ê°€ ë¯¸ê²€ì¶œì¸ ê²½ìš° ì›ë³¸ ìœ ì§€
        if not segments_to_keep or (len(segments_to_keep) == 1 and segments_to_keep[0][1] - segments_to_keep[0][0] < 0.5):
            # ê²€ì¶œëœ ì–¼êµ´ì´ ì „í˜€ ì—†ê±°ë‚˜, ìœ ì§€í•  êµ¬ê°„ì´ 0.5ì´ˆ ë¯¸ë§Œì¸ ê²½ìš°
            total_detected = sum(1 for _, has_p1, has_p2 in self.detection_timeline 
                               if ((has_p1 or has_p2) if not REQUIRE_BOTH_PERSONS else (has_p1 and has_p2)))
            
            if total_detected == 0:
                print(f"âš ï¸ ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•ŠìŒ -> ì›ë³¸ ìœ ì§€")
                segments_to_keep = [(0, total_duration)]
            else:
                print(f"âš ï¸ ìœ ì§€í•  êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìŒ -> ì›ë³¸ ìœ ì§€")
                segments_to_keep = [(0, total_duration)]
        
        # 3. êµ¬ê°„ ì •ë¦¬ ë° ê²€ì¦
        cleaned_segments = []
        for start, end in segments_to_keep:
            # ìµœì†Œ ê¸¸ì´ ì²´í¬ (0.1ì´ˆ ì´ìƒ)
            if end - start >= 0.1:
                # ë²”ìœ„ ì œí•œ
                start = max(0, start)
                end = min(total_duration, end)
                if end > start:
                    cleaned_segments.append((start, end))
        
        print(f"âœ… ìœ ì§€í•  êµ¬ê°„: {len(cleaned_segments)}ê°œ")
        for i, (start, end) in enumerate(cleaned_segments):
            print(f"   êµ¬ê°„ {i+1}: {start:.1f}ì´ˆ - {end:.1f}ì´ˆ (ê¸¸ì´: {end-start:.1f}ì´ˆ)")
        
        return cleaned_segments
    
    def _ffmpeg_trim_and_merge_audio(self, temp_video: str, final_output: str, 
                                   segments: List[Tuple[float, float]]) -> bool:
        """FFmpegë¥¼ ì‚¬ìš©í•´ì„œ íŠ¸ë¦¬ë° + ì˜¤ë””ì˜¤ ë³‘í•©
        
        Args:
            temp_video: ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ (ì˜¤ë””ì˜¤ ì—†ìŒ)
            final_output: ìµœì¢… ì¶œë ¥ íŒŒì¼
            segments: ìœ ì§€í•  êµ¬ê°„ë“¤ [(ì‹œì‘, ë), ...]
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            if len(segments) == 1:
                # ë‹¨ì¼ êµ¬ê°„: ê°„ë‹¨í•œ trim + ì˜¤ë””ì˜¤ ë³‘í•©
                start, end = segments[0]
                cmd = [
                    'ffmpeg', '-y',
                    '-ss', str(start),
                    '-i', temp_video,
                    '-ss', str(start),
                    '-i', self.input_path,
                    '-t', str(end - start),
                    '-c:v', 'copy',  # ë¹„ë””ì˜¤ ë³µì‚¬ (ë¹ ë¥¸ ì²˜ë¦¬)
                    '-c:a', AUDIO_CODEC,
                    '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                    '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                    '-preset', FFMPEG_PRESET,
                    final_output
                ]
            else:
                # ë‹¤ì¤‘ êµ¬ê°„: filter_complex ì‚¬ìš©
                filter_parts = []
                
                # ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸
                for i, (start, end) in enumerate(segments):
                    filter_parts.append(f"[0:v]trim=start={start:.3f}:end={end:.3f},setpts=PTS-STARTPTS[v{i}]")
                
                # ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ (ì›ë³¸ì—ì„œ)
                for i, (start, end) in enumerate(segments):
                    filter_parts.append(f"[1:a]atrim=start={start:.3f}:end={end:.3f},asetpts=PTS-STARTPTS[a{i}]")
                
                # ë¹„ë””ì˜¤ concat
                video_inputs = "".join([f"[v{i}]" for i in range(len(segments))])
                filter_parts.append(f"{video_inputs}concat=n={len(segments)}:v=1:a=0[vout]")
                
                # ì˜¤ë””ì˜¤ concat
                audio_inputs = "".join([f"[a{i}]" for i in range(len(segments))])
                filter_parts.append(f"{audio_inputs}concat=n={len(segments)}:v=0:a=1[aout]")
                
                filter_complex = ";".join(filter_parts)
                
                cmd = [
                    'ffmpeg', '-y',
                    '-i', temp_video,  # ì²˜ë¦¬ëœ ë¹„ë””ì˜¤
                    '-i', self.input_path,  # ì›ë³¸ (ì˜¤ë””ì˜¤ìš©)
                    '-filter_complex', filter_complex,
                    '-map', '[vout]',
                    '-map', '[aout]',
                    '-c:v', VIDEO_CODEC,
                    '-c:a', AUDIO_CODEC,
                    '-preset', FFMPEG_PRESET,
                    '-crf', str(FFMPEG_CRF),
                    final_output
                ]
            
            print(f"ğŸ”„ FFmpeg íŠ¸ë¦¬ë° + ì˜¤ë””ì˜¤ ë³‘í•© ì‹¤í–‰ ì¤‘...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                if os.path.exists(final_output):
                    file_size = os.path.getsize(final_output) / 1024 / 1024
                    print(f"âœ… íŠ¸ë¦¬ë° + ì˜¤ë””ì˜¤ ë³‘í•© ì™„ë£Œ ({file_size:.1f}MB)")
                    return True
                else:
                    print("âŒ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    return False
            else:
                print(f"âŒ FFmpeg ì˜¤ë¥˜: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ íŠ¸ë¦¬ë° + ì˜¤ë””ì˜¤ ë³‘í•© ì˜¤ë¥˜: {e}")
            return False
    
    def _ffmpeg_trim_video_only(self, temp_video: str, final_output: str, 
                                keep_segments: List[Tuple[float, float]]) -> bool:
        """ë¹„ë””ì˜¤ë§Œ íŠ¸ë¦¬ë° (ì˜¤ë””ì˜¤ ì—†ìŒ)
        
        Args:
            temp_video: ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼
            final_output: ìµœì¢… ì¶œë ¥ íŒŒì¼
            keep_segments: ìœ ì§€í•  êµ¬ê°„ë“¤ [(ì‹œì‘ì‹œê°„, ëì‹œê°„), ...]
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            if len(keep_segments) == 1:
                # ë‹¨ì¼ êµ¬ê°„ íŠ¸ë¦¬ë°
                start_time, end_time = keep_segments[0]
                
                cmd = [
                    'ffmpeg', '-y',
                    '-i', temp_video,
                    '-ss', str(start_time),
                    '-t', str(end_time - start_time),
                    '-c', 'copy',  # ë¹ ë¥¸ ë³µì‚¬
                    final_output
                ]
            else:
                # ë‹¤ì¤‘ êµ¬ê°„ ë³‘í•©
                filter_parts = []
                for i, (start_time, end_time) in enumerate(keep_segments):
                    duration = end_time - start_time
                    filter_parts.append(f"[0:v]trim=start={start_time:.3f}:duration={duration:.3f},setpts=PTS-STARTPTS[v{i}]")
                
                concat_inputs = "".join(f"[v{i}]" for i in range(len(keep_segments)))
                filter_parts.append(f"{concat_inputs}concat=n={len(keep_segments)}:v=1:a=0[vout]")
                
                filter_complex = ";".join(filter_parts)
                
                cmd = [
                    'ffmpeg', '-y',
                    '-i', temp_video,
                    '-filter_complex', filter_complex,
                    '-map', '[vout]',
                    '-c:v', VIDEO_CODEC,
                    '-preset', FFMPEG_PRESET,
                    '-crf', str(FFMPEG_CRF),
                    final_output
                ]
            
            print(f"ğŸ”„ FFmpeg ë¹„ë””ì˜¤ íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘... ({len(keep_segments)}ê°œ êµ¬ê°„)")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                if os.path.exists(final_output):
                    file_size = os.path.getsize(final_output) / 1024 / 1024
                    print(f"âœ… ë¹„ë””ì˜¤ íŠ¸ë¦¬ë° ì™„ë£Œ ({file_size:.1f}MB)")
                    return True
                else:
                    print("âŒ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    return False
            else:
                print(f"âŒ FFmpeg ì˜¤ë¥˜: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            return False
    
    def _ffmpeg_merge_audio_only(self, temp_video: str, final_output: str) -> bool:
        """FFmpegë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤ë””ì˜¤ë§Œ ë³‘í•© (íŠ¸ë¦¬ë° ì—†ìŒ)
        
        Args:
            temp_video: ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ (ì˜¤ë””ì˜¤ ì—†ìŒ)
            final_output: ìµœì¢… ì¶œë ¥ íŒŒì¼
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not PRESERVE_AUDIO:
                # ì˜¤ë””ì˜¤ ë³´ì¡´ ë¹„í™œì„±í™”ì‹œ ê·¸ëƒ¥ ì´ë¦„ë§Œ ë³€ê²½
                os.rename(temp_video, final_output)
                return True
            
            cmd = [
                'ffmpeg', '-y',
                '-i', temp_video,      # ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ (ì˜¤ë””ì˜¤ ì—†ìŒ)
                '-i', self.input_path,  # ì›ë³¸ ë¹„ë””ì˜¤ (ì˜¤ë””ì˜¤ í¬í•¨)
                '-c:v', 'copy',        # ë¹„ë””ì˜¤ ë³µì‚¬ (ë¹ ë¥¸ ì²˜ë¦¬)
                '-c:a', AUDIO_CODEC,   # ì˜¤ë””ì˜¤ ì¸ì½”ë”©
                '-map', '0:v:0',       # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
                '-map', '1:a:0',       # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
                '-shortest',           # ë” ì§§ì€ ìŠ¤íŠ¸ë¦¼ì— ë§ì¶¤
                final_output
            ]
            
            print(f"ğŸ”„ FFmpeg ì˜¤ë””ì˜¤ ë³‘í•© ì‹¤í–‰ ì¤‘...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                if os.path.exists(final_output):
                    file_size = os.path.getsize(final_output) / 1024 / 1024
                    print(f"âœ… ì˜¤ë””ì˜¤ ë³‘í•© ì™„ë£Œ ({file_size:.1f}MB)")
                    return True
                else:
                    print("âŒ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                    return False
            else:
                print(f"âŒ FFmpeg ì˜¤ë¥˜: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë³‘í•© ì˜¤ë¥˜: {e}")
            return False
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Dual-Face Tracking ìµœì¢… ê²°ê³¼")
        print("=" * 60)
        
        # ì „ì²´ í†µê³„
        total_time = self.stats['processing_time']
        total_frames = self.stats['processed_frames']
        detection_calls = self.stats['detection_calls']
        avg_fps = total_frames / max(0.01, total_time)
        
        print(f"ğŸ¯ ì²˜ë¦¬ ì™„ë£Œ: {self.output_path}")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸ“Š í‰ê·  FPS: {avg_fps:.1f}")
        print(f"ğŸ” ì–¼êµ´ ê²€ì¶œ í˜¸ì¶œ: {detection_calls}íšŒ")
        
        # ê°œë³„ ê²€ì¶œ í†µê³„
        p1_stats = self.person1_tracker.get_stats()
        p2_stats = self.person2_tracker.get_stats()
        
        print(f"\nğŸ‘¤ Person1 ê²€ì¶œ:")
        print(f"   ì„±ê³µë¥ : {p1_stats['success_rate']:.1f}%")
        print(f"   ì„±ê³µ/ì‹¤íŒ¨: {p1_stats['detection_success']}/{p1_stats['detection_fail']}")
        print(f"   ê²€ì¶œ ìƒíƒœ: {'âœ…' if p1_stats['has_detection'] else 'âŒ'}")
        print(f"   íˆìŠ¤í† ë¦¬ ê¸¸ì´: {p1_stats['history_length']}")
        
        print(f"\nğŸ‘¤ Person2 ê²€ì¶œ:")
        print(f"   ì„±ê³µë¥ : {p2_stats['success_rate']:.1f}%")
        print(f"   ì„±ê³µ/ì‹¤íŒ¨: {p2_stats['detection_success']}/{p2_stats['detection_fail']}")
        print(f"   ê²€ì¶œ ìƒíƒœ: {'âœ…' if p2_stats['has_detection'] else 'âŒ'}")
        print(f"   íˆìŠ¤í† ë¦¬ ê¸¸ì´: {p2_stats['history_length']}")
        
        print(f"\nğŸ¬ ì¶œë ¥ ì •ë³´:")
        if Path(self.output_path).exists():
            file_size = Path(self.output_path).stat().st_size / 1024**2
            print(f"   ğŸ“„ íŒŒì¼: {self.output_path}")
            print(f"   ğŸ’¾ í¬ê¸°: {file_size:.1f}MB")
            print(f"   ğŸ“º í•´ìƒë„: 1920x1080 (ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦°)")
            print(f"   ğŸµ ì˜¤ë””ì˜¤: {'âœ… í¬í•¨' if PRESERVE_AUDIO else 'âŒ ì—†ìŒ'}")
            
            # íŠ¸ë¦¬ë° ì •ë³´
            if TRIM_UNDETECTED_SEGMENTS and self.detection_timeline:
                total_frames = len(self.detection_timeline)
                detected_frames = sum(1 for _, has_p1, has_p2 in self.detection_timeline 
                                    if ((has_p1 or has_p2) if not REQUIRE_BOTH_PERSONS else (has_p1 and has_p2)))
                detection_rate = detected_frames / total_frames * 100 if total_frames > 0 else 0
                print(f"   âœ‚ï¸ íŠ¸ë¦¬ë°: í™œì„±í™” (ì„ê³„ê°’: {UNDETECTED_THRESHOLD_SECONDS}ì´ˆ)")
                print(f"   ğŸ“Š ê²€ì¶œë¥ : {detection_rate:.1f}% ({detected_frames}/{total_frames} í”„ë ˆì„)")
            else:
                print(f"   âœ‚ï¸ íŠ¸ë¦¬ë°: ë¹„í™œì„±í™”")
        else:
            print(f"   âŒ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="Phase 5: Dual-Face Tracking System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (í¬ê¸° ê¸°ë°˜ ìë™ í• ë‹¹)
  python3 face_tracking_system.py
  
  # ë””ë²„ê·¸ ëª¨ë“œë¡œ í¬ê¸° ì •ë³´ í™•ì¸
  python3 face_tracking_system.py --debug
  
  # ì‚¬ìš©ì ì§€ì • ë¹„ë””ì˜¤
  python3 face_tracking_system.py --input input/sample.mp4 --output output/tracked.mp4 --debug
  
  # í¬ê¸° ê¸°ë°˜ ì•ˆì •í™” ì‚¬ìš©
  python3 face_tracking_system.py --size-stabilize --debug
        """
    )
    
    parser.add_argument("--input", 
                       default="input/2people_sample1.mp4",
                       help="ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ê¸°ë³¸ê°’: input/2people_sample1.mp4)")
    parser.add_argument("--output", 
                       default="output/2people_sample1_tracked.mp4",
                       help="ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ê¸°ë³¸ê°’: output/2people_sample1_tracked.mp4)")
    parser.add_argument("--mode", 
                       default="dual_face", 
                       choices=["dual_face", "single"],
                       help="ì²˜ë¦¬ ëª¨ë“œ (ê¸°ë³¸ê°’: dual_face)")
    parser.add_argument("--gpu", 
                       type=int, 
                       default=0,
                       help="GPU ID (ê¸°ë³¸ê°’: 0)")
    parser.add_argument("--debug", 
                       action="store_true",
                       help="ë””ë²„ê·¸ ëª¨ë“œ (í¬ê¸° ë¹„êµ ì •ë³´ ì¶œë ¥)")
    parser.add_argument("--size-stabilize", 
                       action="store_true",
                       help="í¬ê¸° ê¸°ë°˜ í• ë‹¹ ì•ˆì •í™” ì‚¬ìš©")
    parser.add_argument("--prescan", 
                       action="store_true",
                       help="ì‚¬ì „ ë¶„ì„ ëª¨ë“œ (ì •í™•ë„ í–¥ìƒ, 15ì´ˆ ì¶”ê°€)")
    parser.add_argument("--quick", 
                       action="store_true",
                       help="ë¹ ë¥¸ ëª¨ë“œ (ì‚¬ì „ ë¶„ì„ ìŠ¤í‚µ)")
    parser.add_argument("--auto-speaker", 
                       action="store_true",
                       default=True,
                       help="ìë™ í™”ì ì„ ì • (AutoSpeakerDetector, ê¸°ë³¸ê°’: True)")
    parser.add_argument("--one-minute", 
                       action="store_true",
                       help="1ë¶„ ì§‘ì¤‘ ë¶„ì„ ëª¨ë“œ (1ë¶„ ë¶„ì„ + ê°„ë‹¨ ì¶”ì )")
    parser.add_argument("--hungarian", 
                       action="store_true",
                       help="Phase 3: Hungarian Matching ì‚¬ìš© (ê³ ê¸‰ í• ë‹¹ ì‹œìŠ¤í…œ)")
    
    args = parser.parse_args()
    
    # prescanê³¼ quick ì˜µì…˜ ì¶©ëŒ ì²´í¬
    if args.prescan and args.quick:
        print("âŒ --prescanê³¼ --quick ì˜µì…˜ì€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        sys.exit(1)
    
    mode_str = "ì •í™• ëª¨ë“œ" if args.prescan else "ë¹ ë¥¸ ëª¨ë“œ" if args.quick else "ê¸°ë³¸ ëª¨ë“œ"
    print("")
    print(f"ğŸš€ Dual-Face Tracking System v6.1 ({mode_str})")
    print("=" * 50)
    print(f"   ğŸ“¥ ì…ë ¥: {args.input}")
    print(f"   ğŸ“¤ ì¶œë ¥: {args.output}")
    print(f"   ğŸ”§ ëª¨ë“œ: {args.mode}")
    print(f"   ğŸ–¥ï¸ GPU: {args.gpu}")
    print(f"   ğŸ” ë””ë²„ê·¸: {args.debug}")
    print(f"   âš™ï¸ ì•ˆì •í™”: {args.size_stabilize}")
    print(f"   ğŸ¯ ì‚¬ì „ ë¶„ì„: {args.prescan}")
    print(f"   âš¡ ë¹ ë¥¸ ëª¨ë“œ: {args.quick}")
    print(f"   ğŸ¤– ìë™ í™”ì: {args.auto_speaker}")
    print(f"   ğŸ¯ 1ë¶„ ë¶„ì„: {args.one_minute}")
    print(f"   ğŸ§® Hungarian: {args.hungarian}")
    print("=" * 50)
    print("")
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not Path(args.input).exists():
        print(f"âŒ ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {args.input}")
        sys.exit(1)
    
    # ì‹œìŠ¤í…œ ì‹¤í–‰
    try:
        system = DualFaceTrackingSystem(args)
        system.debug_mode = args.debug
        system.size_stabilize = args.size_stabilize
        
        system.process()
        
        # position_tracker ë””ë²„ê·¸ ëª¨ë“œ ì—…ë°ì´íŠ¸ (process() í›„ì—)
        if system.position_tracker:
            system.position_tracker.debug_mode = args.debug
        
        print("\nğŸ‰ Phase 5 ì™„ë£Œ: ì–¼êµ´ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


class SimpleConsistentTracker:
    """1ë¶„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ìˆœí•˜ê³  ì¼ê´€ëœ ì¶”ì """
    
    def __init__(self, person1_profile: Dict[str, Any], person2_profile: Dict[str, Any], 
                 debug_mode: bool = False, identity_bank=None):
        self.p1_profile = person1_profile
        self.p2_profile = person2_profile
        self.debug_mode = debug_mode
        
        # ë§¤ì¹­ ì„ê³„ê°’ ì„¤ì •
        self.embedding_threshold = 0.35  # Phase 1: 0.65 â†’ 0.35 (Identity-based ê°•í™”)
        self.position_tolerance = 200    # 200px ì´ë‚´ ìœ„ì¹˜ ë³€í™”
        self.size_tolerance = 0.5        # 50% í¬ê¸° ë³€í™” í—ˆìš©
        
        # í´ë°± ì¹´ìš´í„°
        self.p1_fallback_count = 0
        self.p2_fallback_count = 0
        
        # Phase 2: ì™¸ë¶€ IdentityBank ì‚¬ìš© (OneMinuteAnalyzerì—ì„œ ì „ë‹¬ë¨)
        if identity_bank is not None:
            self.identity_bank = identity_bank
            if self.debug_mode:
                print(f"âœ… Phase 2: ì™¸ë¶€ IdentityBank ì—°ê²°ë¨ (A:{len(identity_bank.bank['A'])}, B:{len(identity_bank.bank['B'])})")
        else:
            # Phase 1: ë¡œì»¬ Identity Bank (ì„ë² ë”© ë±…í¬) - í´ë°± ëª¨ë“œ
            from collections import deque
            max_embeddings = 64  # ìµœëŒ€ 64ê°œ ì„ë² ë”© ì €ì¥
            self.p1_embedding_bank = deque(maxlen=max_embeddings)
            self.p2_embedding_bank = deque(maxlen=max_embeddings)
            self.identity_bank = None
            
            # ì´ˆê¸° í”„ë¡œí† íƒ€ì… ì„¤ì •
            if person1_profile.get('reference_embedding') is not None:
                self.p1_embedding_bank.append(self._normalize_embedding(person1_profile['reference_embedding']))
            if person2_profile.get('reference_embedding') is not None:
                self.p2_embedding_bank.append(self._normalize_embedding(person2_profile['reference_embedding']))
            
            if self.debug_mode:
                print(f"âš ï¸ Phase 2: ë¡œì»¬ ì„ë² ë”© ë±…í¬ ì‚¬ìš© (í´ë°± ëª¨ë“œ)")
        
        # ModelManager ì´ˆê¸°í™” (ì„ë² ë”© ì¶”ì¶œìš©)
        self.model_manager = None
        try:
            from .model_manager import ModelManager
            self.model_manager = ModelManager()
            if self.debug_mode:
                print("âœ… SimpleConsistentTracker: ModelManager ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            if self.debug_mode:
                print("âš ï¸ SimpleConsistentTracker: ModelManager ì—†ìŒ (í´ë°± ëª¨ë“œ)")
    
    def _normalize_embedding(self, embedding):
        """L2 ì •ê·œí™”"""
        import torch
        if isinstance(embedding, torch.Tensor):
            norm = torch.norm(embedding) + 1e-8
            return embedding / norm
        else:
            import numpy as np
            norm = np.linalg.norm(embedding) + 1e-8
            return embedding / norm
    
    def _get_prototype_embedding(self, person_num: int):
        """ì¤‘ì•™ê°’ ê¸°ë°˜ í”„ë¡œí† íƒ€ì… ì„ë² ë”© ê³„ì‚° (ë…¸ì´ì¦ˆ ê°•ê±´)"""
        # Phase 2: IdentityBankê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if self.identity_bank is not None:
            slot = 'A' if person_num == 1 else 'B'
            return self.identity_bank.proto(slot)
        
        # Phase 1: ë¡œì»¬ ë±…í¬ ì‚¬ìš© (í´ë°±)
        bank = self.p1_embedding_bank if person_num == 1 else self.p2_embedding_bank
        
        if len(bank) == 0:
            return None
        elif len(bank) == 1:
            return bank[0]
        else:
            # ì¤‘ì•™ê°’ ê³„ì‚° (ë…¸ì´ì¦ˆì— ê°•ê±´)
            import torch
            import numpy as np
            
            if isinstance(bank[0], torch.Tensor):
                embeddings = torch.stack(list(bank))
                prototype = torch.median(embeddings, dim=0)[0]  # ì¤‘ì•™ê°’
            else:
                embeddings = np.array(list(bank))
                prototype = np.median(embeddings, axis=0)  # ì¤‘ì•™ê°’
            
            return self._normalize_embedding(prototype)
    
    def _update_embedding_bank(self, person_num: int, embedding):
        """ì„ë² ë”© ë±…í¬ ì—…ë°ì´íŠ¸ (ì„±ê³µí•œ ë§¤ì¹­ë§Œ)"""
        # Phase 2: IdentityBankê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if self.identity_bank is not None:
            slot = 'A' if person_num == 1 else 'B'
            self.identity_bank.update(slot, embedding)
            
            if self.debug_mode and len(self.identity_bank.bank[slot]) % 10 == 0:
                print(f"ğŸ”„ Phase 2: {slot} ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸: {len(self.identity_bank.bank[slot])}ê°œ")
            return
        
        # Phase 1: ë¡œì»¬ ë±…í¬ ì—…ë°ì´íŠ¸ (í´ë°±)
        normalized_emb = self._normalize_embedding(embedding)
        
        if person_num == 1:
            self.p1_embedding_bank.append(normalized_emb)
            if self.debug_mode and len(self.p1_embedding_bank) % 10 == 0:
                print(f"ğŸ”„ P1 ë¡œì»¬ ë±…í¬ ì—…ë°ì´íŠ¸: {len(self.p1_embedding_bank)}ê°œ")
        else:
            self.p2_embedding_bank.append(normalized_emb)
            if self.debug_mode and len(self.p2_embedding_bank) % 10 == 0:
                print(f"ğŸ”„ P2 ë¡œì»¬ ë±…í¬ ì—…ë°ì´íŠ¸: {len(self.p2_embedding_bank)}ê°œ")
    
    def track_frame(self, faces: List[FaceDetection], frame: np.ndarray) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """ë§¤ í”„ë ˆì„ ë‹¨ìˆœ ì¶”ì """
        
        # 1. ì¢Œìš° ë¶„ë¦¬
        left_faces = [f for f in faces if f.center_x < 960]
        right_faces = [f for f in faces if f.center_x >= 960]
        
        # 2. Person1 ì°¾ê¸° (ì™¼ìª½ì—ì„œ)
        person1, p1_embedding = self._find_best_match(left_faces, self.p1_profile, frame, person_num=1)
        
        # ì„±ê³µì  ë§¤ì¹­ì‹œ ì„ë² ë”© ë±…í¬ ì—…ë°ì´íŠ¸
        if person1 and p1_embedding is not None:
            self._update_embedding_bank(1, p1_embedding)
        
        # ëª» ì°¾ìœ¼ë©´ ì™¼ìª½ì—ì„œ ê°€ì¥ í° ì–¼êµ´ (í´ë°±)
        if person1 is None and left_faces:
            person1 = max(left_faces, key=lambda f: f.area)
            self.p1_fallback_count += 1
            if self.debug_mode and self.p1_fallback_count % 30 == 1:  # 1ì´ˆë§ˆë‹¤ í•œë²ˆ
                print(f"âš ï¸ Person1 í´ë°±: ê°€ì¥ í° ì–¼êµ´ ì‚¬ìš© ({self.p1_fallback_count}íšŒ)")
        
        # 3. Person2 ì°¾ê¸° (ì˜¤ë¥¸ìª½ì—ì„œ)
        person2, p2_embedding = self._find_best_match(right_faces, self.p2_profile, frame, person_num=2)
        
        # ì„±ê³µì  ë§¤ì¹­ì‹œ ì„ë² ë”© ë±…í¬ ì—…ë°ì´íŠ¸
        if person2 and p2_embedding is not None:
            self._update_embedding_bank(2, p2_embedding)
        
        # ëª» ì°¾ìœ¼ë©´ ì˜¤ë¥¸ìª½ì—ì„œ ê°€ì¥ í° ì–¼êµ´ (í´ë°±)
        if person2 is None and right_faces:
            person2 = max(right_faces, key=lambda f: f.area)
            self.p2_fallback_count += 1
            if self.debug_mode and self.p2_fallback_count % 30 == 1:  # 1ì´ˆë§ˆë‹¤ í•œë²ˆ
                print(f"âš ï¸ Person2 í´ë°±: ê°€ì¥ í° ì–¼êµ´ ì‚¬ìš© ({self.p2_fallback_count}íšŒ)")
        
        return person1, person2
    
    def _find_best_match(self, faces: List[FaceDetection], profile: Dict[str, Any], frame: np.ndarray, person_num: int) -> Tuple[Optional[FaceDetection], Optional[Any]]:
        """í”„ë¡œíŒŒì¼ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ì–¼êµ´ ì°¾ê¸° + ì„ë² ë”© ë°˜í™˜"""
        if not faces:
            return None, None
        
        best_face = None
        best_score = 0
        best_embedding = None
        
        # Phase 1: í”„ë¡œí† íƒ€ì… ì„ë² ë”© ì‚¬ìš© (ì¤‘ì•™ê°’ ê¸°ë°˜)
        prototype_embedding = self._get_prototype_embedding(person_num)
        
        for face in faces:
            total_score = 0
            score_count = 0
            face_embedding = None
            
            # 1. ì„ë² ë”© ìœ ì‚¬ë„ (60% ê°€ì¤‘ì¹˜) - í”„ë¡œí† íƒ€ì… ì‚¬ìš©
            if self.model_manager and prototype_embedding is not None:
                try:
                    face_crop = self.model_manager.extract_face_crop(face, frame)
                    if face_crop is not None:
                        face_embedding = self.model_manager.get_embedding(face_crop)
                        if face_embedding is not None:
                            face_embedding = self._normalize_embedding(face_embedding)  # ì •ê·œí™”
                            
                            import torch.nn.functional as F
                            if hasattr(face_embedding, 'unsqueeze') and hasattr(prototype_embedding, 'unsqueeze'):
                                similarity = F.cosine_similarity(
                                    face_embedding.unsqueeze(0), 
                                    prototype_embedding.unsqueeze(0)
                                ).item()
                            else:
                                # numpy ë°°ì—´ì˜ ê²½ìš°
                                import numpy as np
                                similarity = float(np.dot(face_embedding, prototype_embedding))
                            
                            # -1~1 â†’ 0~1 ì •ê·œí™”
                            similarity = (similarity + 1) / 2
                            
                            total_score += similarity * 0.6
                            score_count += 0.6
                except:
                    pass  # ì„ë² ë”© ì‹¤íŒ¨ì‹œ ë¬´ì‹œ
            
            # 2. í¬ê¸° ì¼ì¹˜ë„ (20% ê°€ì¤‘ì¹˜)
            if profile.get('average_size'):
                size_diff = abs(face.area - profile['average_size']) / profile['average_size']
                size_score = max(0, 1.0 - size_diff)  # ì°¨ì´ê°€ í´ìˆ˜ë¡ ì ìˆ˜ ë‚®ìŒ
                
                total_score += size_score * 0.2
                score_count += 0.2
            
            # 3. ìœ„ì¹˜ ì¼ì¹˜ë„ (20% ê°€ì¤‘ì¹˜)
            if profile.get('average_position') is not None:
                pos_distance = np.linalg.norm(
                    np.array(face.center) - np.array(profile['average_position'])
                )
                pos_score = max(0, 1.0 - pos_distance / self.position_tolerance)
                
                total_score += pos_score * 0.2
                score_count += 0.2
            
            # ìµœì†Œ í•˜ë‚˜ì˜ ì ìˆ˜ë¼ë„ ìˆì–´ì•¼ í•¨
            if score_count > 0:
                final_score = total_score / score_count  # ì •ê·œí™”
                
                if final_score > best_score and final_score > self.embedding_threshold:  # Phase 1: ë” ì—„ê²©í•œ ì„ê³„ê°’ ì ìš©
                    best_score = final_score
                    best_face = face
                    best_embedding = face_embedding  # ì„ë² ë”©ë„ í•¨ê»˜ ì €ì¥
        
        if self.debug_mode and best_face and best_score > 0.8:  # ê³ ì ìˆ˜ì¼ ë•Œë§Œ ì¶œë ¥
            print(f"ğŸ¯ ì¢‹ì€ ë§¤ì¹­: {profile.get('label', f'P{person_num}')} ì ìˆ˜={best_score:.3f}")
        
        return best_face, best_embedding
    
    def get_stats(self) -> Dict[str, Any]:
        """ì¶”ì  í†µê³„ ë°˜í™˜"""
        return {
            'p1_fallback_count': self.p1_fallback_count,
            'p2_fallback_count': self.p2_fallback_count,
            'embedding_threshold': self.embedding_threshold,
            'position_tolerance': self.position_tolerance
        }


class HungarianFaceAssigner:
    """Phase 3: Hungarian Matchingì„ ì‚¬ìš©í•œ A/B ì–¼êµ´ í• ë‹¹"""
    
    def __init__(self, identity_bank, debug_mode: bool = False):
        """
        Args:
            identity_bank: IdentityBank ì¸ìŠ¤í„´ìŠ¤
            debug_mode: ë””ë²„ê·¸ ëª¨ë“œ
        """
        self.identity_bank = identity_bank
        self.debug_mode = debug_mode
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weights = {
            'iou': 0.45,     # IoU (ìœ„ì¹˜ ì—°ì†ì„±)
            'emb': 0.45,     # ì„ë² ë”© ê±°ë¦¬ (ì •ì²´ì„±)
            'motion': 0.10   # ëª¨ì…˜ (í–¥í›„ í™•ì¥)
        }
        
        # ì„ê³„ê°’ ì„¤ì •
        self.identity_threshold = 0.45  # í”„ë¡œí† íƒ€ì…ê³¼ ê±°ë¦¬ ì„ê³„ê°’
        
        # ì´ì „ ë°•ìŠ¤ ì €ì¥ (ì—°ì†ì„±ì„ ìœ„í•´)
        self.prev_boxes = {'A': None, 'B': None}
    
    def assign_faces(self, faces: List[FaceDetection], frame: np.ndarray, 
                    predicted_boxes: Dict[str, Any] = None) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """Hungarian Matchingìœ¼ë¡œ A/B ì–¼êµ´ í• ë‹¹
        
        Args:
            faces: ê²€ì¶œëœ ì–¼êµ´ ë¦¬ìŠ¤íŠ¸
            frame: í˜„ì¬ í”„ë ˆì„
            predicted_boxes: {'A': bbox, 'B': bbox} ì˜ˆì¸¡ ë°•ìŠ¤ (ì˜µì…˜)
            
        Returns:
            (face_A, face_B) í• ë‹¹ëœ ì–¼êµ´ ë˜ëŠ” None
        """
        if not faces:
            return None, None
        
        if len(faces) == 1:
            # ì–¼êµ´ì´ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ë” ì í•©í•œ ìŠ¬ë¡¯ì— í• ë‹¹
            face = faces[0]
            best_slot, similarity, _ = self.identity_bank.get_best_match([face], [face.embedding if hasattr(face, 'embedding') else None])
            
            if best_slot == 'A':
                return face, None
            else:
                return None, face
        
        # ë¹„ìš© í–‰ë ¬ êµ¬ì„±
        cost_matrix = self._build_cost_matrix(faces, predicted_boxes)
        
        # Hungarian í• ë‹¹
        assignment = self._hungarian_assign(cost_matrix)
        
        # ê²°ê³¼ ìƒì„±
        face_A = faces[assignment['A']] if assignment['A'] != -1 else None
        face_B = faces[assignment['B']] if assignment['B'] != -1 else None
        
        # ì´ì „ ë°•ìŠ¤ ì—…ë°ì´íŠ¸
        if face_A:
            self.prev_boxes['A'] = face_A.bbox
        if face_B:
            self.prev_boxes['B'] = face_B.bbox
        
        if self.debug_mode:
            print(f"ğŸ¯ Hungarian í• ë‹¹: A={'âœ…' if face_A else 'âŒ'}, B={'âœ…' if face_B else 'âŒ'}")
        
        return face_A, face_B
    
    def _build_cost_matrix(self, faces: List[FaceDetection], predicted_boxes: Dict[str, Any] = None) -> np.ndarray:
        """ë¹„ìš© í–‰ë ¬ êµ¬ì„± (2Ã—N)"""
        N = len(faces)
        cost_matrix = np.zeros((2, N), dtype=np.float32)
        
        for j, face in enumerate(faces):
            # A/B ê°ê°ì— ëŒ€í•´ ë¹„ìš© ê³„ì‚°
            for i, slot in enumerate(['A', 'B']):
                total_cost = 0.0
                
                # 1. ì„ë² ë”© ë¹„ìš© (ì •ì²´ì„±)
                if hasattr(face, 'embedding') and face.embedding is not None:
                    emb_distance = self.identity_bank.dist(slot, face.embedding)
                    total_cost += emb_distance * self.weights['emb']
                
                # 2. IoU ë¹„ìš© (ìœ„ì¹˜ ì—°ì†ì„±)
                if predicted_boxes and slot in predicted_boxes:
                    iou_cost = 1.0 - self._calculate_iou(face, predicted_boxes[slot])
                    total_cost += iou_cost * self.weights['iou']
                elif self.prev_boxes[slot] is not None:
                    iou_cost = 1.0 - self._calculate_iou(face, self.prev_boxes[slot])
                    total_cost += iou_cost * self.weights['iou']
                
                # 3. ëª¨ì…˜ ë¹„ìš© (í–¥í›„ í™•ì¥)
                # motion_cost = 0.0
                # total_cost += motion_cost * self.weights['motion']
                
                cost_matrix[i, j] = total_cost
        
        # ì„ê³„ê°’ ê¸°ë°˜ íŒ¨ë„í‹° (í”„ë¡œí† íƒ€ì…ê³¼ ë„ˆë¬´ ë¨¼ ê²½ìš°)
        for j, face in enumerate(faces):
            if hasattr(face, 'embedding') and face.embedding is not None:
                for i, slot in enumerate(['A', 'B']):
                    if self.identity_bank.dist(slot, face.embedding) > self.identity_threshold:
                        cost_matrix[i, j] += 10.0  # í° íŒ¨ë„í‹°
        
        return cost_matrix
    
    def _calculate_iou(self, face: FaceDetection, box) -> float:
        """IoU ê³„ì‚°"""
        if not hasattr(face, 'bbox'):
            return 0.0
        
        x1, y1, x2, y2 = face.bbox
        
        if isinstance(box, (list, tuple)) and len(box) == 4:
            bx1, by1, bx2, by2 = box
        else:
            return 0.0
        
        # êµì§‘í•© ê³„ì‚°
        inter_x1 = max(x1, bx1)
        inter_y1 = max(y1, by1)
        inter_x2 = min(x2, bx2)
        inter_y2 = min(y2, by2)
        
        if inter_x2 <= inter_x1 or inter_y2 <= inter_y1:
            return 0.0
        
        inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
        
        # í•©ì§‘í•© ê³„ì‚°
        area1 = (x2 - x1) * (y2 - y1)
        area2 = (bx2 - bx1) * (by2 - by1)
        union_area = area1 + area2 - inter_area
        
        return inter_area / (union_area + 1e-6)
    
    def _hungarian_assign(self, cost_matrix: np.ndarray) -> Dict[str, int]:
        """2Ã—N í—ê°€ë¦¬ì–¸ í• ë‹¹"""
        if cost_matrix.shape[1] == 0:
            return {'A': -1, 'B': -1}
        
        try:
            from scipy.optimize import linear_sum_assignment
            
            # í—ê°€ë¦¬ì–¸ ì•Œê³ ë¦¬ì¦˜ ì ìš©
            row_indices, col_indices = linear_sum_assignment(cost_matrix)
            
            result = {'A': -1, 'B': -1}
            
            for row_idx, col_idx in zip(row_indices, col_indices):
                slot = 'A' if row_idx == 0 else 'B'
                result[slot] = col_idx
            
            return result
            
        except ImportError:
            # scipy ì—†ëŠ” ê²½ìš° ë‹¨ìˆœ íƒìš• ë§¤ì¹­
            return self._greedy_assign(cost_matrix)
    
    def _greedy_assign(self, cost_matrix: np.ndarray) -> Dict[str, int]:
        """íƒìš•ì  í• ë‹¹ (scipy ì—†ëŠ” ê²½ìš°)"""
        result = {'A': -1, 'B': -1}
        used_cols = set()
        
        # A ë¨¼ì € í• ë‹¹
        if cost_matrix.shape[1] > 0:
            best_col = np.argmin(cost_matrix[0, :])
            result['A'] = best_col
            used_cols.add(best_col)
        
        # B í• ë‹¹ (Aì™€ ë‹¤ë¥¸ ì»¬ëŸ¼)
        if cost_matrix.shape[1] > 1:
            available_cols = [i for i in range(cost_matrix.shape[1]) if i not in used_cols]
            if available_cols:
                costs_b = [cost_matrix[1, i] for i in available_cols]
                best_idx = np.argmin(costs_b)
                result['B'] = available_cols[best_idx]
        
        return result


if __name__ == "__main__":
    main()