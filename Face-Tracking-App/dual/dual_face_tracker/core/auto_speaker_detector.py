#!/usr/bin/env python3
"""
AutoSpeakerDetector: í†µê³„ ê¸°ë°˜ ìë™ í™”ì ì„ ì • ì‹œìŠ¤í…œ

ê¸°ëŠ¥:
- ì „ì²´ ì˜ìƒ 5% ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥¸ ìŠ¤ìº”
- ì–¼êµ´ ì„ë² ë”© ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (DBSCAN)
- ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (ë¹ˆë„, í¬ê¸°, ìœ„ì¹˜, ì§€ì†ì‹œê°„)
- ìƒìœ„ 2ëª… ì£¼ìš” í™”ì ìë™ ì„ ì •

Author: Auto Speaker Detection System v1.0
Date: 2025.08.17
"""

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
from PIL import Image
from torchvision import transforms
import time
import logging
from ..utils.logger import get_logger

# Logger ì„¤ì •
logger = get_logger(__name__, level=logging.INFO)

# í´ëŸ¬ìŠ¤í„°ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from sklearn.cluster import DBSCAN
    from sklearn.metrics.pairwise import cosine_similarity
    CLUSTERING_AVAILABLE = True
except ImportError:
    logger.warning("scikit-learn ì—†ìŒ. ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš©")
    CLUSTERING_AVAILABLE = False

# Dual ì‹œìŠ¤í…œ ì „ìš© ëª¨ë¸ import - models.py ì‚­ì œë¨
try:
    # from models import ModelManager  # models.py íŒŒì¼ì´ ì‚­ì œë¨
    MODEL_MANAGER_AVAILABLE = False  # í•­ìƒ Falseë¡œ ì„¤ì •
except ImportError:
    MODEL_MANAGER_AVAILABLE = False


class FaceCluster:
    """ì–¼êµ´ í´ëŸ¬ìŠ¤í„° (ê°™ì€ ì‚¬ëŒì˜ ì–¼êµ´ë“¤)"""
    
    def __init__(self, cluster_id: int):
        self.cluster_id = cluster_id
        self.detections = []  # List of face detections
        self.embeddings = []  # List of embeddings
        self.positions = []   # List of (x, y) positions
        self.sizes = []       # List of face areas
        self.timestamps = []  # List of timestamps
        self.confidences = [] # List of detection confidences
        
        # ê³„ì‚°ëœ í†µê³„
        self.representative_embedding = None
        self.average_position = None
        self.importance_score = 0.0
        
    def add_detection(self, detection_data: Dict[str, Any]):
        """ê²€ì¶œ ë°ì´í„° ì¶”ê°€"""
        self.detections.append(detection_data)
        
        if detection_data.get('embedding') is not None:
            self.embeddings.append(detection_data['embedding'])
        
        self.positions.append(detection_data['center'])
        self.sizes.append(detection_data['size'])
        self.timestamps.append(detection_data['timestamp'])
        self.confidences.append(detection_data['confidence'])
    
    def calculate_statistics(self):
        """í´ëŸ¬ìŠ¤í„° í†µê³„ ê³„ì‚°"""
        if not self.detections:
            return
            
        # ëŒ€í‘œ ì„ë² ë”© (í‰ê· )
        if self.embeddings:
            stacked_embeddings = torch.stack(self.embeddings)
            self.representative_embedding = torch.mean(stacked_embeddings, dim=0)
            self.representative_embedding = F.normalize(self.representative_embedding, p=2, dim=0)
        
        # í‰ê·  ìœ„ì¹˜
        if self.positions:
            avg_x = sum(pos[0] for pos in self.positions) / len(self.positions)
            avg_y = sum(pos[1] for pos in self.positions) / len(self.positions)
            self.average_position = (avg_x, avg_y)
    
    def get_stats(self) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„° í†µê³„ ë°˜í™˜"""
        return {
            'cluster_id': self.cluster_id,
            'appearance_count': len(self.detections),
            'average_position': self.average_position,
            'average_size': sum(self.sizes) / len(self.sizes) if self.sizes else 0,
            'time_span': max(self.timestamps) - min(self.timestamps) if len(self.timestamps) > 1 else 0,
            'has_embedding': self.representative_embedding is not None,
            'importance_score': self.importance_score
        }


class AutoSpeakerDetector:
    """í†µê³„ ê¸°ë°˜ ìë™ í™”ì ì„ ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        
        # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
        self.face_cascade = None
        self._initialize_face_detector()
        
        # FaceNet ëª¨ë¸ ì´ˆê¸°í™” (ì„ë² ë”©ìš©)
        self.model_manager = None
        self.resnet = None
        self.face_transform = None
        self._initialize_facenet()
        
        # ìŠ¤ìº” ì„¤ì • (ì „êµ¬ê°„ ë¶„ì„)
        self.sample_rate = 0.5  # 50% ìƒ˜í”Œë§ (20% â†’ 50%, ì „êµ¬ê°„ ë¶„ì„ìœ¼ë¡œ ê°œì„ )
        self.min_face_size = 30  # ìµœì†Œ ì–¼êµ´ í¬ê¸°
        self.clustering_threshold = 0.25  # í´ëŸ¬ìŠ¤í„°ë§ ì„ê³„ê°’ (0.35 â†’ 0.25, ë” ì„¸ë°€í•œ í´ëŸ¬ìŠ¤í„°ë§)
        self.min_cluster_size = 5  # ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (10 â†’ 5, ë” ìœ ì—°í•œ í´ëŸ¬ìŠ¤í„°ë§)
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê°€ì¤‘ì¹˜ (í™”ì ì¤‘ì‹¬ ê°œì„ )
        self.weights = {
            'frequency': 0.25,      # ë“±ì¥ ë¹ˆë„ (0.30 â†’ 0.25, ì›€ì§ì„ ì ìˆ˜ ì¶”ê°€ë¡œ ì¡°ì •)
            'size': 0.15,          # ì´ ì–¼êµ´ í¬ê¸° (0.35 â†’ 0.15, í¬ê¸° ë¹„ì¤‘ ëŒ€í­ ê°ì†Œ)
            'center': 0.20,        # í™”ë©´ ì¤‘ì•™ ê°€ì¤‘ì¹˜ (0.25 â†’ 0.20)
            'time_distribution': 0.15,  # ì‹œê°„ì  ë¶„í¬ (ìœ ì§€)
            'motion': 0.15,        # ì›€ì§ì„ ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€, í™œë°œí•œ í™”ì ê°ì§€)
            'consistency': 0.05,   # í¬ê¸° ì¼ê´€ì„± (0.10 â†’ 0.05)
            'confidence': 0.05     # í‰ê·  ì‹ ë¢°ë„ (ìœ ì§€)
        }
    
    def _initialize_face_detector(self):
        """ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”"""
        cascade_path = "/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"
        
        if Path(cascade_path).exists():
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if not self.face_cascade.empty():
                if self.debug_mode:
                    logger.info("Haar Cascade ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
            else:
                raise RuntimeError("âŒ Haar Cascade ë¡œë“œ ì‹¤íŒ¨")
        else:
            raise RuntimeError(f"âŒ Haar Cascade íŒŒì¼ ì—†ìŒ: {cascade_path}")
    
    def _initialize_facenet(self):
        """FaceNet ëª¨ë¸ ì´ˆê¸°í™”"""
        if not MODEL_MANAGER_AVAILABLE:
            if self.debug_mode:
                logger.warning("ModelManager ì—†ìŒ. ì„ë² ë”© ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
            
        try:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model_manager = ModelManager(device)
            self.resnet = self.model_manager.get_resnet()
            
            # ì–¼êµ´ ì „ì²˜ë¦¬ ë³€í™˜ (FaceNetìš©)
            self.face_transform = transforms.Compose([
                transforms.Resize((160, 160)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            if self.debug_mode:
                logger.info(f"FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
                
        except (ImportError, ModuleNotFoundError, RuntimeError, AttributeError) as e:
            if self.debug_mode:
                logger.warning(f"FaceNet ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model_manager = None
            self.resnet = None
            self.face_transform = None
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ FaceNet ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.model_manager = None
            self.resnet = None
            self.face_transform = None
    
    def extract_face_embedding(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> Optional[torch.Tensor]:
        """ì–¼êµ´ í¬ë¡­ì—ì„œ ì„ë² ë”© ì¶”ì¶œ"""
        if self.resnet is None or self.face_transform is None:
            return None
            
        try:
            x1, y1, x2, y2 = bbox
            face_crop = frame[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                return None
            
            # OpenCV BGR â†’ PIL RGB ë³€í™˜
            rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_crop)
            
            # ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            face_tensor = self.face_transform(pil_image).unsqueeze(0)
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                face_tensor = face_tensor.cuda()
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                embedding = self.resnet(face_tensor)
                
            # L2 ì •ê·œí™”
            embedding = F.normalize(embedding, p=2, dim=1)
            
            return embedding.squeeze(0).cpu()  # CPUë¡œ ì´ë™ í›„ ë°°ì¹˜ ì°¨ì› ì œê±°
            
        except (RuntimeError, ValueError, AttributeError, TypeError) as e:
            if self.debug_mode:
                logger.warning(f"ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì„ë² ë”© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def scan_video(self, video_path: str) -> List[Dict[str, Any]]:
        """ì „ì²´ ì˜ìƒ ìŠ¤ìº”í•˜ì—¬ ëª¨ë“  ì–¼êµ´ ê²€ì¶œ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"ìë™ í™”ì ë¶„ì„ ì‹œì‘: {video_path}")
        logger.debug(f"ìƒ˜í”Œë§ ë¹„ìœ¨: {self.sample_rate:.1%}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        sample_interval = int(1 / self.sample_rate)
        
        print(f"   ğŸ“¹ ì´ {total_frames}í”„ë ˆì„, {total_frames/fps:.1f}ì´ˆ")
        logger.debug(f"ë¶„ì„í•  í”„ë ˆì„: {total_frames//sample_interval}ê°œ")
        
        all_detections = []
        frame_idx = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ìƒ˜í”Œë§: sample_intervalë§ˆë‹¤ë§Œ ì²˜ë¦¬
                if frame_idx % sample_interval == 0:
                    timestamp = frame_idx / fps
                    faces = self._detect_faces_in_frame(frame)
                    
                    for face_bbox, confidence in faces:
                        x1, y1, x2, y2 = face_bbox
                        center_x = (x1 + x2) / 2
                        center_y = (y1 + y2) / 2
                        size = (x2 - x1) * (y2 - y1)
                        
                        # ì„ë² ë”© ì¶”ì¶œ
                        embedding = self.extract_face_embedding(frame, face_bbox)
                        
                        detection_data = {
                            'frame_idx': frame_idx,
                            'timestamp': timestamp,
                            'bbox': face_bbox,
                            'center': (center_x, center_y),
                            'size': size,
                            'confidence': confidence,
                            'embedding': embedding
                        }
                        
                        all_detections.append(detection_data)
                
                frame_idx += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_idx % (sample_interval * 50) == 0:  # 50ê°œ ìƒ˜í”Œë§ˆë‹¤
                    progress = (frame_idx / total_frames) * 100
                    elapsed = time.time() - start_time
                    logger.debug(f"ì§„í–‰ë¥ : {progress:.1f}% ({len(all_detections)}ê°œ ì–¼êµ´, {elapsed:.1f}ì´ˆ)")
        
        finally:
            cap.release()
        
        elapsed_time = time.time() - start_time
        logger.info(f"ìŠ¤ìº” ì™„ë£Œ: {len(all_detections)}ê°œ ì–¼êµ´ ë°œê²¬ ({elapsed_time:.1f}ì´ˆ)")
        
        return all_detections
    
    def scan_video_left_right(self, video_path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """ì¢Œìš° ì˜ì—­ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì „ì²´ ì˜ìƒ ìŠ¤ìº”"""
        logger.info(f"ì¢Œìš° ë¶„ë¦¬ í™”ì ë¶„ì„ ì‹œì‘: {video_path}")
        logger.debug(f"ìƒ˜í”Œë§ ë¹„ìœ¨: {self.sample_rate:.1%}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        sample_interval = int(1 / self.sample_rate)
        
        print(f"   ğŸ“¹ ì´ {total_frames}í”„ë ˆì„, {total_frames/fps:.1f}ì´ˆ")
        logger.debug(f"ë¶„ì„í•  í”„ë ˆì„: {total_frames//sample_interval}ê°œ")
        print(f"   âš–ï¸ ì¢Œìš° ë¶„ë¦¬ ê¸°ì¤€: x=960px")
        
        left_detections = []   # x < 960 (ì™¼ìª½ ì˜ì—­)
        right_detections = []  # x >= 960 (ì˜¤ë¥¸ìª½ ì˜ì—­)
        frame_idx = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ìƒ˜í”Œë§: sample_intervalë§ˆë‹¤ë§Œ ì²˜ë¦¬
                if frame_idx % sample_interval == 0:
                    timestamp = frame_idx / fps
                    faces = self._detect_faces_in_frame(frame)
                    
                    for face_bbox, confidence in faces:
                        x1, y1, x2, y2 = face_bbox
                        center_x = (x1 + x2) / 2
                        center_y = (y1 + y2) / 2
                        size = (x2 - x1) * (y2 - y1)
                        
                        # ì„ë² ë”© ì¶”ì¶œ
                        embedding = self.extract_face_embedding(frame, face_bbox)
                        
                        detection_data = {
                            'frame_idx': frame_idx,
                            'timestamp': timestamp,
                            'bbox': face_bbox,
                            'center': (center_x, center_y),
                            'size': size,
                            'confidence': confidence,
                            'embedding': embedding
                        }
                        
                        # ì¢Œìš° ë¶„ë¦¬ (x=960 ê¸°ì¤€)
                        if center_x < 960:  # ì™¼ìª½ ì˜ì—­
                            left_detections.append(detection_data)
                        else:  # ì˜¤ë¥¸ìª½ ì˜ì—­
                            right_detections.append(detection_data)
                
                frame_idx += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_idx % (sample_interval * 50) == 0:  # 50ê°œ ìƒ˜í”Œë§ˆë‹¤
                    progress = (frame_idx / total_frames) * 100
                    elapsed = time.time() - start_time
                    logger.debug(f"ì§„í–‰ë¥ : {progress:.1f}% (ì¢Œ:{len(left_detections)}ê°œ, ìš°:{len(right_detections)}ê°œ, {elapsed:.1f}ì´ˆ)")
        
        finally:
            cap.release()
        
        elapsed = time.time() - start_time
        logger.info(f"ì¢Œìš° ë¶„ë¦¬ ìŠ¤ìº” ì™„ë£Œ: ì¢Œì¸¡ {len(left_detections)}ê°œ, ìš°ì¸¡ {len(right_detections)}ê°œ ({elapsed:.1f}ì´ˆ)")
        
        return left_detections, right_detections
    
    def _detect_faces_in_frame(self, frame: np.ndarray) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ - MTCNN ìš°ì„ , Haar Cascade í´ë°±"""
        faces = []
        
        # 1. MTCNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ ì‹œë„ (ModelManager ì‚¬ìš© ê°€ëŠ¥ì‹œ)
        if self.model_manager and self.model_manager.mtcnn:
            try:
                mtcnn = self.model_manager.mtcnn
                # PIL Imageë¡œ ë³€í™˜ (MTCNN ìš”êµ¬ì‚¬í•­)
                from PIL import Image
                if isinstance(frame, np.ndarray):
                    pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                else:
                    pil_frame = frame
                
                boxes, probs = mtcnn.detect(pil_frame)
                
                if boxes is not None and len(boxes) > 0:
                    for box, prob in zip(boxes, probs):
                        if prob > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                            x1, y1, x2, y2 = box.astype(int)
                            # ë°”ìš´ë”© ë°•ìŠ¤ ê²€ì¦
                            if x2 > x1 and y2 > y1 and (x2-x1) >= self.min_face_size and (y2-y1) >= self.min_face_size:
                                bbox = (x1, y1, x2, y2)
                                faces.append((bbox, float(prob)))
                    
                    if faces:  # MTCNNì—ì„œ ì–¼êµ´ì„ ì°¾ìœ¼ë©´ ë°˜í™˜
                        return faces
            except Exception as e:
                if self.debug_mode:
                    logger.warning(f"MTCNN ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Haar Cascade í´ë°±: {e}")
        
        # 2. Haar Cascade í´ë°±
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            detected_faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1, 
                minNeighbors=5, 
                minSize=(self.min_face_size, self.min_face_size)
            )
            
            for (x, y, w, h) in detected_faces:
                # ì–¼êµ´ í¬ê¸° ë° ì¢…íš¡ë¹„ ê²€ì¦
                if w > 20 and h > 20 and w < 200 and h < 200:
                    aspect_ratio = w / h
                    if 0.7 < aspect_ratio < 1.3:
                        bbox = (x, y, x + w, y + h)
                        confidence = 0.9  # HaarëŠ” ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë¯€ë¡œ ê³ ì •ê°’
                        faces.append((bbox, confidence))
        except Exception as e:
            if self.debug_mode:
                logger.warning(f"Haar Cascade ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
        
        return faces
    
    def cluster_faces(self, all_detections: List[Dict[str, Any]]) -> List[FaceCluster]:
        """ì–¼êµ´ ì„ë² ë”©ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        print(f"ğŸ”„ ì–¼êµ´ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
        
        # ì„ë² ë”©ì´ ìˆëŠ” ê²€ì¶œë§Œ í•„í„°ë§
        valid_detections = [det for det in all_detections if det['embedding'] is not None]
        
        if len(valid_detections) < self.min_cluster_size:
            logger.warning(f"ì¶©ë¶„í•œ ì„ë² ë”© ë°ì´í„° ì—†ìŒ ({len(valid_detections)}ê°œ < {self.min_cluster_size}ê°œ)")
            return self._fallback_clustering(all_detections)
        
        # ì„ë² ë”© ìŠ¤íƒ
        embeddings = [det['embedding'] for det in valid_detections]
        embedding_matrix = torch.stack(embeddings).numpy()
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity_matrix = cosine_similarity(embedding_matrix)
        # ê±°ë¦¬ í–‰ë ¬ ìƒì„± (ìŒìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ í´ë¦¬í•‘)
        distance_matrix = 1 - similarity_matrix
        distance_matrix = np.clip(distance_matrix, 0, 2)  # 0~2 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        if CLUSTERING_AVAILABLE:
            clustering = DBSCAN(
                eps=self.clustering_threshold,
                min_samples=self.min_cluster_size,
                metric='precomputed'
            )
            cluster_labels = clustering.fit_predict(distance_matrix)
        else:
            # ê¸°ë³¸ í´ëŸ¬ìŠ¤í„°ë§ (ì²« ë‘ ê°œë§Œ)
            cluster_labels = np.zeros(len(valid_detections))
            if len(valid_detections) > len(valid_detections) // 2:
                cluster_labels[len(valid_detections) // 2:] = 1
        
        # í´ëŸ¬ìŠ¤í„° ìƒì„±
        clusters = {}
        for idx, label in enumerate(cluster_labels):
            if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                continue
            if label not in clusters:
                clusters[label] = FaceCluster(label)
            clusters[label].add_detection(valid_detections[idx])
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„ ê³„ì‚°
        cluster_list = list(clusters.values())
        for cluster in cluster_list:
            cluster.calculate_statistics()
        
        # í¬ê¸° í•„í„°ë§ (ë„ˆë¬´ ì‘ì€ í´ëŸ¬ìŠ¤í„° ì œê±°)
        filtered_clusters = [c for c in cluster_list if len(c.detections) >= self.min_cluster_size]
        
        # ë™ì¼ ì¸ë¬¼ í´ëŸ¬ìŠ¤í„° ë³‘í•© (ì¤‘ë³µ ë°©ì§€)
        merged_clusters = self.merge_similar_clusters(filtered_clusters)
        
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(merged_clusters)}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„± (ë³‘í•© í›„)")
        
        return merged_clusters
    
    def _split_single_cluster(self, cluster: FaceCluster, video_duration: float) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°ë¥¼ í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ 2ê°œë¡œ ë¶„í• """
        print(f"ğŸ”„ ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° ë¶„í•  ì‹œë„ ({len(cluster.detections)}ê°œ ê²€ì¶œ)")
        
        if len(cluster.detections) < 10:
            logger.error(f"ê²€ì¶œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ ({len(cluster.detections)}ê°œ)")
            return None, None
        
        # í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í° ì–¼êµ´ vs ì‘ì€ ì–¼êµ´)
        detections_with_size = [(det, det['size']) for det in cluster.detections]
        detections_with_size.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 60%ì™€ í•˜ìœ„ 40%ë¡œ ë¶„í•  (í° ì–¼êµ´ì´ ë” ì¤‘ìš”)
        split_point = int(len(detections_with_size) * 0.6)
        
        # ë¶„í• ëœ í´ëŸ¬ìŠ¤í„° ìƒì„±
        cluster1 = FaceCluster(0)  # í° ì–¼êµ´ë“¤
        cluster2 = FaceCluster(1)  # ì‘ì€ ì–¼êµ´ë“¤
        
        for i, (det, size) in enumerate(detections_with_size):
            if i < split_point:
                cluster1.add_detection(det)
            else:
                cluster2.add_detection(det)
        
        cluster1.calculate_statistics()
        cluster2.calculate_statistics()
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        cluster1.importance_score = self.calculate_importance_score(cluster1, video_duration)
        cluster2.importance_score = self.calculate_importance_score(cluster2, video_duration)
        
        logger.info(f"í´ëŸ¬ìŠ¤í„° ë¶„í•  ì™„ë£Œ: {len(cluster1.detections)}ê°œ + {len(cluster2.detections)}ê°œ")
        
        # í™”ì ì •ë³´ ìƒì„±
        speaker1_info = {
            'cluster': cluster1,
            'representative_embedding': cluster1.representative_embedding,
            'average_position': cluster1.average_position,
            'importance_score': cluster1.importance_score,
            'appearance_count': len(cluster1.detections),
            'stats': cluster1.get_stats()
        }
        
        speaker2_info = {
            'cluster': cluster2,
            'representative_embedding': cluster2.representative_embedding,
            'average_position': cluster2.average_position,
            'importance_score': cluster2.importance_score,
            'appearance_count': len(cluster2.detections),
            'stats': cluster2.get_stats()
        }
        
        return speaker1_info, speaker2_info
    
    def calculate_motion_score(self, cluster: FaceCluster) -> float:
        """ì›€ì§ì„ ì ìˆ˜ ê³„ì‚°: ìœ„ì¹˜ ë³€í™”ê°€ ë§ì€ ì–¼êµ´ = í™œë°œí•œ í™”ì"""
        if len(cluster.positions) < 2:
            return 0.0
        
        total_movement = 0
        movement_count = 0
        
        for i in range(1, len(cluster.positions)):
            pos1 = cluster.positions[i-1]
            pos2 = cluster.positions[i]
            
            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
            movement = ((pos2[0] - pos1[0]) ** 2 + (pos2[1] - pos1[1]) ** 2) ** 0.5
            total_movement += movement
            movement_count += 1
        
        if movement_count == 0:
            return 0.0
        
        # í‰ê·  ì›€ì§ì„ ê³„ì‚°
        avg_movement = total_movement / movement_count
        
        # ì •ê·œí™” (0~1): í‰ê·  í”½ì…€ ì´ë™ì´ 50pxì¼ ë•Œ 1.0
        motion_score = min(avg_movement / 50.0, 1.0)
        
        return motion_score
    
    def merge_similar_clusters(self, clusters: List[FaceCluster]) -> List[FaceCluster]:
        """ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ë³‘í•© (ë™ì¼ ì¸ë¬¼ ì¤‘ë³µ ë°©ì§€)"""
        if len(clusters) <= 1:
            return clusters
            
        print(f"ğŸ”„ í´ëŸ¬ìŠ¤í„° ë³‘í•© ì‹œì‘: {len(clusters)}ê°œ í´ëŸ¬ìŠ¤í„°")
        
        merged = []
        used = set()
        merge_threshold = 0.65  # ì„ë² ë”© ìœ ì‚¬ë„ ì„ê³„ê°’ (0.8â†’0.65ë¡œ ë” ì—„ê²©í•œ ë³‘í•©)
        
        import torch.nn.functional as F
        
        for i, cluster1 in enumerate(clusters):
            if i in used:
                continue
            
            # cluster1ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•  í´ëŸ¬ìŠ¤í„°ë“¤ ì°¾ê¸°
            merge_candidates = [cluster1]
            
            # ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ë“¤ê³¼ ë¹„êµ
            for j, cluster2 in enumerate(clusters[i+1:], i+1):
                if j in used:
                    continue
                
                # ëŒ€í‘œ ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš°ë§Œ ë¹„êµ
                if (cluster1.representative_embedding is not None and 
                    cluster2.representative_embedding is not None):
                    
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = F.cosine_similarity(
                        cluster1.representative_embedding.unsqueeze(0),
                        cluster2.representative_embedding.unsqueeze(0)
                    ).item()
                    
                    if similarity > merge_threshold:
                        merge_candidates.append(cluster2)
                        used.add(j)
                        
                        if self.debug_mode:
                            print(f"   ğŸ”— í´ëŸ¬ìŠ¤í„° {i}ì™€ {j} ë³‘í•© (ìœ ì‚¬ë„: {similarity:.3f})")
            
            # ë³‘í•© ì‹¤í–‰ (ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„° ìœ ì§€ ì¡°ê±´ ì¶”ê°€)
            if len(merge_candidates) > 1:
                # ì•ˆì „ì¥ì¹˜: ë³‘í•© í›„ì—ë„ ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„°ê°€ ë‚¨ëŠ”ì§€ í™•ì¸
                remaining_clusters = len(clusters) - len([c for c in clusters if clusters.index(c) not in used]) - len(merge_candidates) + 1
                
                if remaining_clusters >= 2 or len(merged) == 0:  # 2ê°œ ì´ìƒ ë‚¨ê±°ë‚˜ ì²« ë²ˆì§¸ ë³‘í•©ì´ë©´ ì§„í–‰
                    merged_cluster = self._merge_clusters(merge_candidates)
                    merged.append(merged_cluster)
                    
                    if self.debug_mode:
                        logger.info(f"{len(merge_candidates)}ê°œ í´ëŸ¬ìŠ¤í„° ë³‘í•© ì™„ë£Œ (ë‚¨ì€ í´ëŸ¬ìŠ¤í„°: {remaining_clusters}ê°œ)")
                else:
                    # ë³‘í•©í•˜ë©´ 1ê°œë§Œ ë‚¨ìœ¼ë©´ ë³‘í•© ì·¨ì†Œ
                    merged.append(cluster1)
                    if self.debug_mode:
                        logger.warning("ë³‘í•© ì·¨ì†Œ (ìµœì†Œ 2ê°œ í´ëŸ¬ìŠ¤í„° ìœ ì§€ë¥¼ ìœ„í•´)")
            else:
                merged.append(cluster1)
        
        logger.info(f"í´ëŸ¬ìŠ¤í„° ë³‘í•© ì™„ë£Œ: {len(clusters)}ê°œ â†’ {len(merged)}ê°œ")
        return merged
    
    def _merge_clusters(self, clusters: List[FaceCluster]) -> FaceCluster:
        """ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©"""
        if len(clusters) == 1:
            return clusters[0]
        
        # ì²« ë²ˆì§¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ë¨¸ì§€ ë³‘í•©
        base_cluster = clusters[0]
        
        for cluster in clusters[1:]:
            # ëª¨ë“  ê²€ì¶œ ë°ì´í„° ë³‘í•©
            base_cluster.detections.extend(cluster.detections)
            base_cluster.embeddings.extend(cluster.embeddings)
            base_cluster.positions.extend(cluster.positions)
            base_cluster.sizes.extend(cluster.sizes)
            base_cluster.timestamps.extend(cluster.timestamps)
            base_cluster.confidences.extend(cluster.confidences)
        
        # í†µê³„ ì¬ê³„ì‚°
        base_cluster.calculate_statistics()
        
        return base_cluster
    
    def _fallback_clustering(self, all_detections: List[Dict[str, Any]]) -> List[FaceCluster]:
        """ì„ë² ë”© ì—†ì„ ë•Œ ìœ„ì¹˜ ê¸°ë°˜ í´ë°± í´ëŸ¬ìŠ¤í„°ë§"""
        logger.warning("ìœ„ì¹˜ ê¸°ë°˜ í´ë°± í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš©")
        
        if len(all_detections) < 10:
            return []
        
        # X ì¢Œí‘œë¡œ ì •ë ¬í•˜ì—¬ ì¢Œìš° êµ¬ë¶„
        sorted_detections = sorted(all_detections, key=lambda x: x['center'][0])
        mid_idx = len(sorted_detections) // 2
        
        # ì¢Œìš° í´ëŸ¬ìŠ¤í„° ìƒì„±
        left_cluster = FaceCluster(0)
        right_cluster = FaceCluster(1)
        
        for i, detection in enumerate(sorted_detections):
            if i < mid_idx:
                left_cluster.add_detection(detection)
            else:
                right_cluster.add_detection(detection)
        
        left_cluster.calculate_statistics()
        right_cluster.calculate_statistics()
        
        return [left_cluster, right_cluster]
    
    def calculate_importance_score(self, cluster: FaceCluster, video_duration: float) -> float:
        """í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        if not cluster.detections:
            return 0.0
        
        # 1. ë“±ì¥ ë¹ˆë„ ì ìˆ˜ (ì •ê·œí™”)
        frequency_score = min(len(cluster.detections) / 100, 1.0)
        
        # 2. ì´ ì–¼êµ´ í¬ê¸° ì ìˆ˜ (ì •ê·œí™”)
        total_size = sum(cluster.sizes)
        size_score = min(total_size / 1000000, 1.0)
        
        # 3. í™”ë©´ ì¤‘ì•™ ê°€ì¤‘ì¹˜
        center_weights = []
        for pos in cluster.positions:
            x, y = pos
            # 1920x1080 ê¸°ì¤€ ì¤‘ì•™(960, 540)ì—ì„œì˜ ê±°ë¦¬
            distance_from_center = ((x - 960)**2 + (y - 540)**2)**0.5
            center_weight = max(0, 1 - distance_from_center / 800)
            center_weights.append(center_weight)
        center_score = sum(center_weights) / len(center_weights) if center_weights else 0
        
        # 4. ì‹œê°„ì  ë¶„í¬ ì ìˆ˜
        if len(cluster.timestamps) > 1:
            time_span = max(cluster.timestamps) - min(cluster.timestamps)
            time_distribution_score = min(time_span / video_duration, 1.0)
        else:
            time_distribution_score = 0.0
        
        # 5. í¬ê¸° ì¼ê´€ì„± ì ìˆ˜ (í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        if len(cluster.sizes) > 1:
            size_std = np.std(cluster.sizes)
            size_mean = np.mean(cluster.sizes)
            consistency_score = max(0, 1 - size_std / (size_mean + 1e-6))
        else:
            consistency_score = 1.0
        
        # 6. í‰ê·  ì‹ ë¢°ë„
        confidence_score = sum(cluster.confidences) / len(cluster.confidences) if cluster.confidences else 0
        
        # 7. ì›€ì§ì„ ì ìˆ˜ (í™œë°œí•œ í™”ì ê°ì§€)
        motion_score = self.calculate_motion_score(cluster)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        importance_score = (
            frequency_score * self.weights['frequency'] +
            size_score * self.weights['size'] +
            center_score * self.weights['center'] +
            time_distribution_score * self.weights['time_distribution'] +
            motion_score * self.weights['motion'] +
            consistency_score * self.weights['consistency'] +
            confidence_score * self.weights['confidence']
        )
        
        return importance_score
    
    def select_main_speakers(self, clusters: List[FaceCluster], video_duration: float) -> Tuple[Optional[FaceCluster], Optional[FaceCluster]]:
        """ì£¼ìš” í™”ì 2ëª… ì„ ì •"""
        logger.debug("ì£¼ìš” í™”ì ì„ ì • ì¤‘...")
        
        if len(clusters) < 2:
            logger.warning(f"ì¶©ë¶„í•œ í´ëŸ¬ìŠ¤í„° ì—†ìŒ ({len(clusters)}ê°œ)")
            return None, None
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        for cluster in clusters:
            cluster.importance_score = self.calculate_importance_score(cluster, video_duration)
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_clusters = sorted(clusters, key=lambda c: c.importance_score, reverse=True)
        
        speaker1 = sorted_clusters[0]
        speaker2 = sorted_clusters[1]
        
        # ê²°ê³¼ ì¶œë ¥
        if self.debug_mode:
            logger.info("ì£¼ìš” í™”ì ìë™ ì„ ì • ì™„ë£Œ:")
            for i, speaker in enumerate([speaker1, speaker2], 1):
                stats = speaker.get_stats()
                print(f"   í™”ì{i}: {stats['appearance_count']}íšŒ ë“±ì¥, "
                      f"ì ìˆ˜ {speaker.importance_score:.3f}, "
                      f"í‰ê· ìœ„ì¹˜ {stats['average_position']}")
        
        return speaker1, speaker2
    
    def analyze_video(self, video_path: str) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """ì¢Œìš° ê¸°ë°˜ ì „ì²´ ì˜ìƒ ë¶„ì„í•˜ì—¬ ì£¼ìš” í™”ì 2ëª… ë°˜í™˜"""
        start_time = time.time()
        
        print("\n" + "=" * 60)
        logger.debug("ì¢Œìš° ê¸°ë°˜ í™”ì ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        # 1ë‹¨ê³„: ì¢Œìš° ë¶„ë¦¬ ìŠ¤ìº”
        left_detections, right_detections = self.scan_video_left_right(video_path)
        
        # ë¹„ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        video_duration = total_frames / fps
        cap.release()
        
        logger.debug("ì¢Œìš° ë¶„ë¦¬ ê²°ê³¼:")
        print(f"   ì™¼ìª½ ì˜ì—­: {len(left_detections)}ê°œ ì–¼êµ´")
        print(f"   ì˜¤ë¥¸ìª½ ì˜ì—­: {len(right_detections)}ê°œ ì–¼êµ´")
        
        # 2ë‹¨ê³„: ê° ì˜ì—­ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§
        speaker1_info = None
        speaker2_info = None
        
        # ì™¼ìª½ ì˜ì—­ì—ì„œ ì£¼ìš” í™”ì ì„ ì • (Person1)
        if len(left_detections) >= self.min_cluster_size:
            print(f"\nğŸ”„ ì™¼ìª½ ì˜ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            left_clusters = self.cluster_faces(left_detections)
            
            if left_clusters:
                # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í´ëŸ¬ìŠ¤í„° ì„ íƒ
                left_clusters.sort(key=lambda c: len(c.detections), reverse=True)
                main_left_cluster = left_clusters[0]
                main_left_cluster.importance_score = self.calculate_importance_score(main_left_cluster, video_duration)
                
                speaker1_info = self._create_speaker_info(main_left_cluster, "Person1 (Left)")
                logger.info(f"Person1 ì„ ì •: {len(main_left_cluster.detections)}ê°œ ê²€ì¶œ, ì ìˆ˜ {main_left_cluster.importance_score:.3f}")
            else:
                logger.error("ì™¼ìª½ ì˜ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
        else:
            logger.warning(f"ì™¼ìª½ ì˜ì—­ ë°ì´í„° ë¶€ì¡±: {len(left_detections)}ê°œ < {self.min_cluster_size}ê°œ")
        
        # ì˜¤ë¥¸ìª½ ì˜ì—­ì—ì„œ ì£¼ìš” í™”ì ì„ ì • (Person2)
        if len(right_detections) >= self.min_cluster_size:
            print(f"\nğŸ”„ ì˜¤ë¥¸ìª½ ì˜ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘...")
            right_clusters = self.cluster_faces(right_detections)
            
            if right_clusters:
                # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í´ëŸ¬ìŠ¤í„° ì„ íƒ
                right_clusters.sort(key=lambda c: len(c.detections), reverse=True)
                main_right_cluster = right_clusters[0]
                main_right_cluster.importance_score = self.calculate_importance_score(main_right_cluster, video_duration)
                
                speaker2_info = self._create_speaker_info(main_right_cluster, "Person2 (Right)")
                logger.info(f"Person2 ì„ ì •: {len(main_right_cluster.detections)}ê°œ ê²€ì¶œ, ì ìˆ˜ {main_right_cluster.importance_score:.3f}")
            else:
                logger.error("ì˜¤ë¥¸ìª½ ì˜ì—­ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")
        else:
            logger.warning(f"ì˜¤ë¥¸ìª½ ì˜ì—­ ë°ì´í„° ë¶€ì¡±: {len(right_detections)}ê°œ < {self.min_cluster_size}ê°œ")
        
        # 3ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° ë°˜í™˜
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ ì¢Œìš° ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)")
        
        if speaker1_info and speaker2_info:
            logger.info("ì–‘ìª½ í™”ì ëª¨ë‘ ì„ ì • ì„±ê³µ")
            return speaker1_info, speaker2_info
        elif speaker1_info or speaker2_info:
            logger.warning("í•œìª½ í™”ìë§Œ ì„ ì •ë¨")
            return speaker1_info, speaker2_info
        else:
            logger.error("í™”ì ì„ ì • ì‹¤íŒ¨")
            return None, None
    
    def _create_speaker_info(self, cluster: FaceCluster, label: str = "") -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„°ë¡œë¶€í„° í™”ì ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
        # í†µê³„ ì •ë³´ ìƒì„±
        stats = cluster.get_stats()
        
        # í™”ì ì •ë³´ êµ¬ì„±
        speaker_info = {
            'cluster': cluster,
            'representative_embedding': cluster.representative_embedding,
            'average_position': cluster.average_position,
            'importance_score': getattr(cluster, 'importance_score', 0.0),
            'appearance_count': len(cluster.detections),
            'stats': stats,
            'label': label
        }
        
        if self.debug_mode:
            print(f"ğŸ“‹ {label} ì •ë³´:")
            print(f"   ê²€ì¶œ íšŸìˆ˜: {speaker_info['appearance_count']}íšŒ")
            print(f"   ì¤‘ìš”ë„ ì ìˆ˜: {speaker_info['importance_score']:.3f}")
            print(f"   í‰ê·  ìœ„ì¹˜: {speaker_info['average_position']}")
            print(f"   í‰ê·  í¬ê¸°: {stats.get('average_size', 0):.1f}px")
        
        return speaker_info


class OneMinuteAnalyzer:
    """ì²˜ìŒ 1ë¶„(60ì´ˆ)ì„ 100% ë¶„ì„í•˜ì—¬ í™•ì‹¤í•œ í™”ì í”„ë¡œíŒŒì¼ ìƒì„± + IdentityBank í†µí•©"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        
        # Phase 2: IdentityBank í†µí•©
        from dual_face_tracker.core.identity_bank import IdentityBank
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.identity_bank = IdentityBank(max_samples=128, device=device)  # 1ë¶„ ë¶„ì„ìš© ë” í° ë±…í¬
        
        # FaceNet ëª¨ë¸ ì´ˆê¸°í™” (AutoSpeakerDetectorì™€ ë™ì¼)
        self.resnet = None
        self.face_transform = None
        self._initialize_facenet()
        
        # ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™”
        self.face_cascade = None
        self.model_manager = None
        self._initialize_face_detector()
        
        # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
        self.clustering_eps = 0.3
        self.min_cluster_size = 20  # 1ë¶„ ë¶„ì„ì´ë¯€ë¡œ ë” í° í´ëŸ¬ìŠ¤í„° ìš”êµ¬
        
    def _initialize_face_detector(self):
        """ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” (AutoSpeakerDetectorì™€ ë™ì¼)"""
        cascade_path = "/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"
        
        if Path(cascade_path).exists():
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if not self.face_cascade.empty():
                if self.debug_mode:
                    logger.info("Haar Cascade ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
            else:
                raise RuntimeError("âŒ Haar Cascade ë¡œë“œ ì‹¤íŒ¨")
        else:
            raise RuntimeError(f"âŒ Haar Cascade íŒŒì¼ ì—†ìŒ: {cascade_path}")
            
        # ModelManager ìë™ ê°ì§€ ì‹œë„
        try:
            from .model_manager import ModelManager
            self.model_manager = ModelManager()
            if self.debug_mode:
                logger.info("ModelManager ë¡œë“œ ì™„ë£Œ (MTCNN ì‚¬ìš© ê°€ëŠ¥)")
        except ImportError:
            if self.debug_mode:
                logger.warning("ModelManager ì—†ìŒ (Haar Cascadeë§Œ ì‚¬ìš©)")
    
    def _initialize_facenet(self):
        """FaceNet ëª¨ë¸ ì´ˆê¸°í™” (AutoSpeakerDetectorì™€ ë™ì¼)"""
        try:
            from torchvision.models import inception_v3
            from facenet_pytorch import InceptionResnetV1
            
            # InceptionResnetV1 ëª¨ë¸ ë¡œë“œ
            self.resnet = InceptionResnetV1(pretrained='vggface2').eval()
            if torch.cuda.is_available():
                self.resnet = self.resnet.cuda()
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            self.face_transform = transforms.Compose([
                transforms.Resize((160, 160)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
            if self.debug_mode:
                logger.info("FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
        except ImportError as e:
            if self.debug_mode:
                logger.warning(f"FaceNet ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.resnet = None
            self.face_transform = None

    def analyze_first_minute(self, video_path: str) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """ì²˜ìŒ 1ë¶„ì„ 100% ë¶„ì„í•˜ì—¬ í™•ì‹¤í•œ í™”ì í”„ë¡œíŒŒì¼ ìƒì„±"""
        print("\n" + "=" * 70)
        logger.debug("1ë¶„ ì§‘ì¤‘ ë¶„ì„ ì‹œì‘")
        print("=" * 70)
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        analyze_frames = min(int(fps * 60), total_frames)  # 60ì´ˆ ë˜ëŠ” ì „ì²´ í”„ë ˆì„ ì¤‘ ì ì€ ê²ƒ
        
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {total_frames/fps:.1f}ì´ˆ, {fps:.1f}fps")
        logger.debug(f"ë¶„ì„ ë²”ìœ„: ì²˜ìŒ {analyze_frames}í”„ë ˆì„ (60ì´ˆ)")
        print(f"âš–ï¸ ì¢Œìš° ë¶„ë¦¬ ê¸°ì¤€: x=960px")
        
        left_face_data = []   # ì™¼ìª½ ì˜ì—­ ëª¨ë“  ì–¼êµ´
        right_face_data = []  # ì˜¤ë¥¸ìª½ ì˜ì—­ ëª¨ë“  ì–¼êµ´
        frame_idx = 0
        start_time = time.time()
        
        try:
            while frame_idx < analyze_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ëª¨ë“  í”„ë ˆì„ ë¶„ì„ (100% ìƒ˜í”Œë§)
                faces = self._detect_faces_in_frame(frame)
                
                for face_bbox, confidence in faces:
                    x1, y1, x2, y2 = face_bbox
                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    size = (x2 - x1) * (y2 - y1)
                    
                    # ì„ë² ë”© ì¶”ì¶œ
                    embedding = self.extract_face_embedding(frame, face_bbox)
                    
                    face_info = {
                        'frame_idx': frame_idx,
                        'bbox': face_bbox,
                        'center': (center_x, center_y),
                        'size': size,
                        'confidence': confidence,
                        'embedding': embedding
                    }
                    
                    # ì¢Œìš° ë¶„ë¦¬ (x=960 ê¸°ì¤€)
                    if center_x < 960:  # ì™¼ìª½ ì˜ì—­
                        left_face_data.append(face_info)
                    else:  # ì˜¤ë¥¸ìª½ ì˜ì—­
                        right_face_data.append(face_info)
                
                frame_idx += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_idx % 300 == 0:  # 10ì´ˆë§ˆë‹¤
                    progress = (frame_idx / analyze_frames) * 100
                    elapsed = time.time() - start_time
                    logger.debug(f"ì§„í–‰ë¥ : {progress:.1f}% (ì¢Œ:{len(left_face_data)}, ìš°:{len(right_face_data)}, {elapsed:.1f}ì´ˆ)")
        
        finally:
            cap.release()
        
        elapsed = time.time() - start_time
        logger.info(f"1ë¶„ ìŠ¤ìº” ì™„ë£Œ: ì¢Œì¸¡ {len(left_face_data)}ê°œ, ìš°ì¸¡ {len(right_face_data)}ê°œ ({elapsed:.1f}ì´ˆ)")
        
        # ê° ì˜ì—­ì—ì„œ í™”ì í”„ë¡œíŒŒì¼ ìƒì„±
        person1_profile = self._create_speaker_profile(left_face_data, "Person1 (Left)")
        person2_profile = self._create_speaker_profile(right_face_data, "Person2 (Right)")
        
        return person1_profile, person2_profile

    def extract_face_embedding(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> Optional[torch.Tensor]:
        """ì–¼êµ´ í¬ë¡­ì—ì„œ ì„ë² ë”© ì¶”ì¶œ (AutoSpeakerDetectorì™€ ë™ì¼)"""
        if self.resnet is None or self.face_transform is None:
            return None
            
        try:
            x1, y1, x2, y2 = bbox
            face_crop = frame[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                return None
            
            # OpenCV BGR â†’ PIL RGB ë³€í™˜
            rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_crop)
            
            # ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            face_tensor = self.face_transform(pil_image).unsqueeze(0)
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                face_tensor = face_tensor.cuda()
            
            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                embedding = self.resnet(face_tensor)
                
            # L2 ì •ê·œí™”
            embedding = F.normalize(embedding, p=2, dim=1)
            
            return embedding.squeeze(0).cpu()  # CPUë¡œ ì´ë™ í›„ ë°°ì¹˜ ì°¨ì› ì œê±°
            
        except (RuntimeError, ValueError, AttributeError, TypeError) as e:
            if self.debug_mode:
                logger.warning(f"ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì„ë² ë”© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def _detect_faces_in_frame(self, frame: np.ndarray) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ (AutoSpeakerDetectorì™€ ë™ì¼)"""
        faces = []
        
        # 1. MTCNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ ì‹œë„ (ModelManager ì‚¬ìš© ê°€ëŠ¥ì‹œ)
        if self.model_manager and self.model_manager.mtcnn:
            try:
                mtcnn = self.model_manager.mtcnn
                # PIL Imageë¡œ ë³€í™˜ (MTCNN ìš”êµ¬ì‚¬í•­)
                from PIL import Image
                if isinstance(frame, np.ndarray):
                    pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                else:
                    pil_frame = frame
                
                boxes, probs = mtcnn.detect(pil_frame)
                
                if boxes is not None and len(boxes) > 0:
                    for box, prob in zip(boxes, probs):
                        if prob > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                            x1, y1, x2, y2 = box.astype(int)
                            # ë°”ìš´ë”© ë°•ìŠ¤ ê²€ì¦
                            if x2 > x1 and y2 > y1 and (x2-x1) >= 30 and (y2-y1) >= 30:
                                bbox = (x1, y1, x2, y2)
                                faces.append((bbox, float(prob)))
                    
                    if faces:  # MTCNNì—ì„œ ì–¼êµ´ì„ ì°¾ìœ¼ë©´ ë°˜í™˜
                        return faces
            except Exception as e:
                if self.debug_mode:
                    logger.warning(f"MTCNN ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨, Haar Cascade í´ë°±: {e}")
        
        # 2. Haar Cascade í´ë°±
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            detected_faces = self.face_cascade.detectMultiScale(
                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
            )
            
            for (x, y, w, h) in detected_faces:
                bbox = (x, y, x + w, y + h)
                confidence = 0.8  # Haar CascadeëŠ” í™•ë¥ ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê³ ì •ê°’
                faces.append((bbox, confidence))
                
        except Exception as e:
            if self.debug_mode:
                logger.warning(f"Haar Cascade ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
        
        return faces

    def _create_speaker_profile(self, face_data: List[Dict[str, Any]], label: str) -> Optional[Dict[str, Any]]:
        """1ë¶„ ë°ì´í„°ë¡œ í™•ì‹¤í•œ í™”ì í”„ë¡œíŒŒì¼ ìƒì„±"""
        print(f"\nğŸ”„ {label} í”„ë¡œíŒŒì¼ ìƒì„± ì¤‘...")
        
        if len(face_data) < self.min_cluster_size:
            logger.error(f"{label} ë°ì´í„° ë¶€ì¡±: {len(face_data)}ê°œ < {self.min_cluster_size}ê°œ")
            return None
        
        # ì„ë² ë”©ì´ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
        valid_data = [f for f in face_data if f['embedding'] is not None]
        
        if len(valid_data) < self.min_cluster_size:
            logger.error(f"{label} ìœ íš¨ ì„ë² ë”© ë¶€ì¡±: {len(valid_data)}ê°œ < {self.min_cluster_size}ê°œ")
            return None
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê°™ì€ ì‚¬ëŒë¼ë¦¬ ê·¸ë£¹í™”
        try:
            embeddings = [f['embedding'] for f in valid_data]
            embedding_matrix = torch.stack(embeddings).numpy()
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            similarity_matrix = cosine_similarity(embedding_matrix)
            distance_matrix = 1 - similarity_matrix
            distance_matrix = np.clip(distance_matrix, 0, 2)
            
            if CLUSTERING_AVAILABLE:
                clustering = DBSCAN(
                    eps=self.clustering_eps,
                    min_samples=self.min_cluster_size,
                    metric='precomputed'
                )
                cluster_labels = clustering.fit_predict(distance_matrix)
            else:
                # í´ë°±: ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ
                cluster_labels = np.zeros(len(valid_data))
            
            # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° = ì£¼ìš” í™”ì
            from collections import Counter
            label_counts = Counter(cluster_labels)
            # -1 (ë…¸ì´ì¦ˆ) ì œì™¸í•˜ê³  ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°
            valid_labels = [(label, count) for label, count in label_counts.items() if label != -1]
            
            if not valid_labels:
                logger.error(f"{label} ìœ íš¨ í´ëŸ¬ìŠ¤í„° ì—†ìŒ")
                return None
            
            main_cluster_label = max(valid_labels, key=lambda x: x[1])[0]
            main_faces = [valid_data[i] for i, l in enumerate(cluster_labels) if l == main_cluster_label]
            
            logger.info(f"{label} í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(main_faces)}ê°œ ê²€ì¶œ (ì „ì²´ {len(cluster_labels)}ê°œ ì¤‘)")
            
        except Exception as e:
            logger.warning(f"{label} í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ì „ì²´ ë°ì´í„° ì‚¬ìš©: {e}")
            main_faces = valid_data
        
        # Phase 2: IdentityBankë¥¼ ì‚¬ìš©í•œ ê°•ë ¥í•œ í”„ë¡œíŒŒì¼ ìƒì„±
        embeddings = [f['embedding'] for f in main_faces]
        centers = [f['center'] for f in main_faces]
        sizes = [f['size'] for f in main_faces]
        
        # IdentityBankì— ì„ë² ë”© ë“±ë¡ (A/B ìŠ¬ë¡¯ ê²°ì •ì€ labelì— ë”°ë¼)
        slot = 'A' if 'Person1' in label or 'Left' in label else 'B'
        
        # ëª¨ë“  ì„ë² ë”©ì„ IdentityBankì— ì—…ë°ì´íŠ¸
        for emb in embeddings:
            self.identity_bank.update(slot, emb)
        
        # IdentityBankì—ì„œ ì¤‘ì•™ê°’ í”„ë¡œí† íƒ€ì… ìƒì„± (ë…¸ì´ì¦ˆ ê°•ê±´)
        prototype_embedding = self.identity_bank.proto(slot)
        
        profile = {
            'label': label,
            'slot': slot,  # A/B ìŠ¬ë¡¯ ì •ë³´ ì¶”ê°€
            'appearance_count': len(main_faces),
            
            # Phase 2: IdentityBank í”„ë¡œí† íƒ€ì… (ì¤‘ì•™ê°’ ê¸°ë°˜)
            'reference_embedding': prototype_embedding,
            'identity_bank_size': len(self.identity_bank.bank[slot]),
            
            # í‰ê·  ìœ„ì¹˜ (ì¢Œìš° ê¸°ì¤€ì )
            'average_position': np.mean(centers, axis=0),
            
            # í‰ê·  í¬ê¸° (ì•ë’¤ ê±°ë¦¬ ì¶”ì •)
            'average_size': np.mean(sizes),
            
            # í¬ê¸° ë²”ìœ„ (ìµœì†Œ/ìµœëŒ€)
            'size_range': (min(sizes), max(sizes)),
            
            # ìœ„ì¹˜ ë²”ìœ„ (ì›€ì§ì„ ë²”ìœ„)
            'position_range': {
                'x_min': min([c[0] for c in centers]),
                'x_max': max([c[0] for c in centers]),
                'y_min': min([c[1] for c in centers]),
                'y_max': max([c[1] for c in centers])
            }
        }
        
        if self.debug_mode:
            print(f"ğŸ“‹ {label} í”„ë¡œíŒŒì¼ (IdentityBank ìŠ¬ë¡¯: {slot}):")
            print(f"   - ê²€ì¶œ íšŸìˆ˜: {profile['appearance_count']}íšŒ")
            print(f"   - IdentityBank í¬ê¸°: {profile['identity_bank_size']}ê°œ ì„ë² ë”©")
            print(f"   - í‰ê·  í¬ê¸°: {profile['average_size']:.0f}px")
            print(f"   - í‰ê·  ìœ„ì¹˜: ({profile['average_position'][0]:.0f}, {profile['average_position'][1]:.0f})")
            print(f"   - X ë²”ìœ„: {profile['position_range']['x_min']:.0f} ~ {profile['position_range']['x_max']:.0f}")
            print(f"   - Y ë²”ìœ„: {profile['position_range']['y_min']:.0f} ~ {profile['position_range']['y_max']:.0f}")
            logger.info(f"{label} í”„ë¡œí† íƒ€ì…: {'âœ… ìƒì„±ë¨' if prototype_embedding is not None else 'âŒ ì—†ìŒ'}")
        
        return profile


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    detector = AutoSpeakerDetector(debug_mode=True)
    
    # ìƒ˜í”Œ ë¹„ë””ì˜¤ë¡œ í…ŒìŠ¤íŠ¸
    video_path = "tests/videos/2people_sample1.mp4"
    if Path(video_path).exists():
        speaker1, speaker2 = detector.analyze_video(video_path)
        
        if speaker1 and speaker2:
            print("\n" + "="*50)
            logger.info("ìë™ í™”ì ì„ ì • ê²°ê³¼")
            print("="*50)
            print(f"í™”ì1: {speaker1['appearance_count']}íšŒ ë“±ì¥, ì ìˆ˜ {speaker1['importance_score']:.3f}")
            print(f"í™”ì2: {speaker2['appearance_count']}íšŒ ë“±ì¥, ì ìˆ˜ {speaker2['importance_score']:.3f}")
        else:
            print("âŒ í™”ì ì„ ì • ì‹¤íŒ¨")
    else:
        logger.warning(f"í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì—†ìŒ: {video_path}")