"""
GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ

VRAM ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import gc
import time
import threading
from typing import Dict, Any, Optional, List, Callable, Tuple
from collections import deque
from dataclasses import dataclass

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

from ..utils.logger import logger
from ..utils.exceptions import GPUMemoryError, MemoryManagementError


@dataclass
class MemorySnapshot:
    """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·"""
    timestamp: float
    gpu_allocated_mb: float
    gpu_reserved_mb: float
    gpu_free_mb: float
    system_memory_mb: float
    system_memory_percent: float
    
    @property
    def gpu_utilization_percent(self) -> float:
        total = self.gpu_allocated_mb + self.gpu_free_mb
        if total > 0:
            return (self.gpu_allocated_mb / total) * 100
        return 0


class MemoryPool:
    """GPU ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬"""
    
    def __init__(self, initial_size_mb: int = 1024):
        self.initial_size_mb = initial_size_mb
        self.allocated_tensors: List[torch.Tensor] = []
        self.free_tensors: List[torch.Tensor] = []
        self._lock = threading.Lock()
        
    def preallocate(self, sizes: List[Tuple[int, ...]], dtype=torch.float32):
        """ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹"""
        if not TORCH_AVAILABLE:
            return
            
        with self._lock:
            for size in sizes:
                try:
                    tensor = torch.empty(size, dtype=dtype, device='cuda')
                    self.free_tensors.append(tensor)
                    logger.debug(f"ğŸ’¾ ë©”ëª¨ë¦¬ í’€ í• ë‹¹: {size} ({tensor.numel() * tensor.element_size() / 1024**2:.1f}MB)")
                except Exception as e:
                    logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ í’€ í• ë‹¹ ì‹¤íŒ¨: {e}")
    
    def get_tensor(self, size: Tuple[int, ...], dtype=torch.float32) -> Optional[torch.Tensor]:
        """í’€ì—ì„œ í…ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if not TORCH_AVAILABLE:
            return None
            
        with self._lock:
            # ì í•©í•œ í¬ê¸°ì˜ í…ì„œ ì°¾ê¸°
            for i, tensor in enumerate(self.free_tensors):
                if tensor.shape == size and tensor.dtype == dtype:
                    self.free_tensors.pop(i)
                    self.allocated_tensors.append(tensor)
                    return tensor
            
            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            try:
                tensor = torch.empty(size, dtype=dtype, device='cuda')
                self.allocated_tensors.append(tensor)
                return tensor
            except Exception as e:
                logger.warning(f"âš ï¸ í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                return None
    
    def return_tensor(self, tensor: torch.Tensor):
        """í’€ì— í…ì„œ ë°˜í™˜"""
        if not TORCH_AVAILABLE:
            return
            
        with self._lock:
            if tensor in self.allocated_tensors:
                self.allocated_tensors.remove(tensor)
                self.free_tensors.append(tensor)
    
    def clear_pool(self):
        """í’€ ì™„ì „ ì •ë¦¬"""
        with self._lock:
            self.allocated_tensors.clear()
            self.free_tensors.clear()


class MemoryManager:
    """
    GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ì
    
    ê¸°ëŠ¥:
    - VRAM ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    - ì„ê³„ê°’ ê¸°ë°˜ ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬
    - ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬
    - ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
    - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€
    - ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ê´€ë¦¬
    """
    
    def __init__(self, 
                 threshold_percent: float = 80.0,
                 critical_threshold_percent: float = 95.0,
                 monitoring_interval: float = 10.0):
        """
        Args:
            threshold_percent: ì •ë¦¬ ì‹œì‘ ì„ê³„ê°’ (%)
            critical_threshold_percent: ê¸´ê¸‰ ì •ë¦¬ ì„ê³„ê°’ (%)
            monitoring_interval: ëª¨ë‹ˆí„°ë§ ê°„ê²© (ì´ˆ)
        """
        self.threshold_percent = threshold_percent
        self.critical_threshold_percent = critical_threshold_percent
        self.monitoring_interval = monitoring_interval
        
        # GPU ì •ë³´
        self.gpu_available = TORCH_AVAILABLE and torch.cuda.is_available()
        self.total_gpu_memory_mb = 0
        
        if self.gpu_available:
            # GPU ë©”ëª¨ë¦¬ ì´ëŸ‰ í™•ì¸
            self.total_gpu_memory_mb = torch.cuda.get_device_properties(0).total_memory / 1024**2
            logger.info(f"ğŸ–¥ï¸ ì´ GPU ë©”ëª¨ë¦¬: {self.total_gpu_memory_mb/1024:.1f}GB")
        
        # ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 100ê°œ)
        self.memory_history = deque(maxlen=100)
        
        # ë©”ëª¨ë¦¬ í’€
        self.memory_pool = MemoryPool()
        
        # ì •ë¦¬ ì½œë°± í•¨ìˆ˜ë“¤
        self.cleanup_callbacks: List[Callable[[], None]] = []
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        
        # í†µê³„
        self.stats = {
            'total_cleanups': 0,
            'emergency_cleanups': 0,
            'bytes_freed': 0,
            'peak_memory_mb': 0,
            'avg_memory_mb': 0
        }
        
        logger.info(f"ğŸ’¾ MemoryManager ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì„ê³„ê°’: {threshold_percent}% / ê¸´ê¸‰: {critical_threshold_percent}%")
    
    def get_memory_info(self) -> MemorySnapshot:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘"""
        snapshot = MemorySnapshot(
            timestamp=time.time(),
            gpu_allocated_mb=0,
            gpu_reserved_mb=0,
            gpu_free_mb=0,
            system_memory_mb=0,
            system_memory_percent=0
        )
        
        if self.gpu_available:
            try:
                allocated = torch.cuda.memory_allocated() / 1024**2
                reserved = torch.cuda.memory_reserved() / 1024**2
                free = self.total_gpu_memory_mb - reserved
                
                snapshot.gpu_allocated_mb = allocated
                snapshot.gpu_reserved_mb = reserved
                snapshot.gpu_free_mb = free
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['peak_memory_mb'] = max(self.stats['peak_memory_mb'], allocated)
                
            except Exception as e:
                logger.warning(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        if PSUTIL_AVAILABLE:
            try:
                memory = psutil.virtual_memory()
                snapshot.system_memory_mb = memory.used / 1024**2
                snapshot.system_memory_percent = memory.percent
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return snapshot
    
    def check_and_cleanup(self, force: bool = False) -> bool:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬ ë° í•„ìš”ì‹œ ì •ë¦¬"""
        snapshot = self.get_memory_info()
        self.memory_history.append(snapshot)
        
        if not self.gpu_available:
            return True
        
        current_percent = snapshot.gpu_utilization_percent
        
        # ì •ë¦¬ í•„ìš”ì„± íŒë‹¨
        needs_cleanup = (
            force or 
            current_percent > self.threshold_percent or
            current_percent > self.critical_threshold_percent
        )
        
        if needs_cleanup:
            is_emergency = current_percent > self.critical_threshold_percent
            return self._perform_cleanup(snapshot, is_emergency)
        
        return True
    
    def _perform_cleanup(self, snapshot: MemorySnapshot, is_emergency: bool = False) -> bool:
        """ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰"""
        cleanup_type = "ê¸´ê¸‰" if is_emergency else "ì¼ë°˜"
        logger.info(f"ğŸ§¹ {cleanup_type} ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘ "
                   f"({snapshot.gpu_utilization_percent:.1f}% ì‚¬ìš©)")
        
        initial_allocated = snapshot.gpu_allocated_mb
        cleanup_start = time.time()
        
        try:
            # 1. PyTorch CUDA ìºì‹œ ì •ë¦¬
            if self.gpu_available:
                torch.cuda.empty_cache()
                logger.debug("ğŸ—‘ï¸ CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
            # 2. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()
            logger.debug("ğŸ—‘ï¸ Python GC ì™„ë£Œ")
            
            # 3. ë©”ëª¨ë¦¬ í’€ ì •ë¦¬ (ê¸´ê¸‰ì‹œì—ë§Œ)
            if is_emergency:
                self.memory_pool.clear_pool()
                logger.debug("ğŸ—‘ï¸ ë©”ëª¨ë¦¬ í’€ ì •ë¦¬ ì™„ë£Œ")
            
            # 4. ë“±ë¡ëœ ì •ë¦¬ ì½œë°± ì‹¤í–‰
            for callback in self.cleanup_callbacks:
                try:
                    callback()
                except Exception as e:
                    logger.warning(f"âš ï¸ ì •ë¦¬ ì½œë°± ì‹¤íŒ¨: {e}")
            
            # ì •ë¦¬ í›„ ìƒíƒœ í™•ì¸
            if self.gpu_available:
                final_allocated = torch.cuda.memory_allocated() / 1024**2
                freed_mb = initial_allocated - final_allocated
                
                logger.info(f"âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {freed_mb:.1f}MB í•´ì œ "
                           f"({initial_allocated:.1f}MB â†’ {final_allocated:.1f}MB)")
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['total_cleanups'] += 1
                if is_emergency:
                    self.stats['emergency_cleanups'] += 1
                self.stats['bytes_freed'] += freed_mb * 1024**2
                
                cleanup_time = time.time() - cleanup_start
                logger.debug(f"â±ï¸ ì •ë¦¬ ì†Œìš” ì‹œê°„: {cleanup_time:.2f}ì´ˆ")
                
                return True
            
        except Exception as e:
            logger.error(f"âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
        
        return True
    
    def register_cleanup_callback(self, callback: Callable[[], None]):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ì‹œ ì‹¤í–‰í•  ì½œë°± ë“±ë¡"""
        self.cleanup_callbacks.append(callback)
        logger.debug(f"ğŸ“ ì •ë¦¬ ì½œë°± ë“±ë¡ (ì´ {len(self.cleanup_callbacks)}ê°œ)")
    
    def unregister_cleanup_callback(self, callback: Callable[[], None]):
        """ì½œë°± ë“±ë¡ í•´ì œ"""
        if callback in self.cleanup_callbacks:
            self.cleanup_callbacks.remove(callback)
    
    def get_recommended_batch_size(self, 
                                 base_batch_size: int,
                                 memory_per_item_mb: float) -> int:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ê¸°ë°˜ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°"""
        if not self.gpu_available:
            return base_batch_size
        
        snapshot = self.get_memory_info()
        available_mb = snapshot.gpu_free_mb * 0.8  # ì•ˆì „ ë§ˆì§„
        
        if memory_per_item_mb > 0:
            max_items = int(available_mb / memory_per_item_mb)
            recommended = min(base_batch_size, max_items)
            
            if recommended < base_batch_size:
                logger.info(f"ğŸ“‰ ë°°ì¹˜ í¬ê¸° ì¡°ì • ê¶Œì¥: {base_batch_size} â†’ {recommended} "
                           f"(ê°€ìš© ë©”ëª¨ë¦¬: {available_mb:.1f}MB)")
            
            return max(1, recommended)
        
        return base_batch_size
    
    def start_monitoring(self):
        """ë°±ê·¸ë¼ìš´ë“œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            logger.warning("âš ï¸ ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        self.is_monitoring = True
        
        def monitor_loop():
            logger.info("ğŸ” ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
            
            while self.is_monitoring:
                try:
                    self.check_and_cleanup()
                    time.sleep(self.monitoring_interval)
                except Exception as e:
                    logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì—ëŸ¬: {e}")
                    time.sleep(self.monitoring_interval)
            
            logger.info("ğŸ”š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
        
        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        
        if self.monitor_thread:
            self.monitor_thread.join(timeout=self.monitoring_interval + 1)
    
    def detect_memory_leak(self, window_minutes: int = 30) -> bool:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€"""
        if len(self.memory_history) < 10:
            return False
        
        cutoff_time = time.time() - (window_minutes * 60)
        recent_snapshots = [s for s in self.memory_history if s.timestamp > cutoff_time]
        
        if len(recent_snapshots) < 5:
            return False
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ ë¶„ì„
        memory_values = [s.gpu_allocated_mb for s in recent_snapshots]
        
        # ë‹¨ìˆœ ì„ í˜• ì¦ê°€ ê°ì§€
        increase_count = 0
        for i in range(1, len(memory_values)):
            if memory_values[i] > memory_values[i-1]:
                increase_count += 1
        
        # 70% ì´ìƒì´ ì¦ê°€ íŠ¸ë Œë“œë©´ ëˆ„ìˆ˜ ê°€ëŠ¥ì„±
        leak_threshold = 0.7
        is_leak = (increase_count / (len(memory_values) - 1)) > leak_threshold
        
        if is_leak:
            start_memory = memory_values[0]
            end_memory = memory_values[-1]
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ê°ì§€: "
                          f"{start_memory:.1f}MB â†’ {end_memory:.1f}MB "
                          f"({window_minutes}ë¶„ê°„)")
        
        return is_leak
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ í†µê³„ ë°˜í™˜"""
        if not self.memory_history:
            current_snapshot = self.get_memory_info()
        else:
            current_snapshot = self.memory_history[-1]
        
        # í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        if self.memory_history:
            avg_memory = sum(s.gpu_allocated_mb for s in self.memory_history) / len(self.memory_history)
            self.stats['avg_memory_mb'] = avg_memory
        
        return {
            **self.stats,
            'current_allocated_mb': current_snapshot.gpu_allocated_mb,
            'current_utilization_percent': current_snapshot.gpu_utilization_percent,
            'total_gpu_memory_mb': self.total_gpu_memory_mb,
            'memory_pool_size': len(self.memory_pool.allocated_tensors) + len(self.memory_pool.free_tensors),
            'cleanup_callbacks': len(self.cleanup_callbacks),
            'monitoring_active': self.is_monitoring
        }
    
    def print_memory_summary(self):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ìš”ì•½ ì¶œë ¥"""
        stats = self.get_memory_stats()
        
        print(f"""
ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬ ìš”ì•½:
   â€¢ í˜„ì¬ ì‚¬ìš©ëŸ‰: {stats['current_allocated_mb']:.1f}MB ({stats['current_utilization_percent']:.1f}%)
   â€¢ ì´ GPU ë©”ëª¨ë¦¬: {stats['total_gpu_memory_mb']/1024:.1f}GB
   â€¢ í‰ê·  ì‚¬ìš©ëŸ‰: {stats['avg_memory_mb']:.1f}MB
   â€¢ ìµœëŒ€ ì‚¬ìš©ëŸ‰: {stats['peak_memory_mb']:.1f}MB
   â€¢ ì´ ì •ë¦¬ íšŸìˆ˜: {stats['total_cleanups']}íšŒ
   â€¢ ê¸´ê¸‰ ì •ë¦¬: {stats['emergency_cleanups']}íšŒ
   â€¢ í•´ì œëœ ë©”ëª¨ë¦¬: {stats['bytes_freed']/1024**3:.2f}GB
   â€¢ ëª¨ë‹ˆí„°ë§: {'í™œì„±' if stats['monitoring_active'] else 'ë¹„í™œì„±'}
        """)
    
    def __enter__(self):
        """Context manager ì§„ì…"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ"""
        self.stop_monitoring()
        # ìµœì¢… ì •ë¦¬
        self.check_and_cleanup(force=True)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª MemoryManager í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    with MemoryManager(threshold_percent=50.0, monitoring_interval=2.0) as manager:
        print("ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬ì ì‹¤í–‰ ì¤‘...")
        
        # ê°€ì§œ ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
        if TORCH_AVAILABLE and torch.cuda.is_available():
            test_tensors = []
            for i in range(5):
                tensor = torch.randn(1000, 1000, device='cuda')
                test_tensors.append(tensor)
                print(f"ğŸ”„ í…ì„œ {i+1} ìƒì„±")
                time.sleep(1)
            
            # ì •ë¦¬ í…ŒìŠ¤íŠ¸
            manager.check_and_cleanup(force=True)
            
            # í…ì„œ í•´ì œ
            del test_tensors
        
        time.sleep(5)
        manager.print_memory_summary()
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")