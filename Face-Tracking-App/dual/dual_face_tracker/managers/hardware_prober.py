"""
HardwareProber - í•˜ë“œì›¨ì–´ ìžë™ í”„ë¡œë¹™ ì‹œìŠ¤í…œ

GPU ëŠ¥ë ¥, NVDEC/NVENC ì„¸ì…˜ í•œë„, ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ìžë™ìœ¼ë¡œ ì¸¡ì •í•˜ì—¬
ìµœì í™”ëœ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import logging
from datetime import datetime
from typing import Dict, Any, Optional
import torch

from ..utils.exceptions import DualFaceTrackerError, GPUMemoryError
from ..utils.cuda_utils import check_cuda_available, get_gpu_memory_info


class HardwareProber:
    """
    í•˜ë“œì›¨ì–´ ìžë™ í”„ë¡œë¹™ í´ëž˜ìŠ¤
    
    GPU ëŠ¥ë ¥ ì¸¡ì • ë° ìµœì  ì„¤ì • ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.probe_results: Optional[Dict[str, Any]] = None
        self.logger = logging.getLogger(__name__)
        
    def generate_optimal_config(self) -> Dict[str, Any]:
        """
        ìµœì  ì„¤ì • ìƒì„±
        
        Returns:
            Dict[str, Any]: í•˜ë“œì›¨ì–´ ê¸°ë°˜ ìµœì  ì„¤ì •
            
        Raises:
            DualFaceTrackerError: í”„ë¡œë¹™ ì‹¤íŒ¨ì‹œ
        """
        self.logger.info("ðŸ” GPU í•˜ë“œì›¨ì–´ ëŠ¥ë ¥ ì¸¡ì • ì‹œìž‘...")
        
        # GPU ëŠ¥ë ¥ ì¸¡ì •
        gpu_info = self._probe_gpu_capabilities()
        
        # ìµœì í™”ëœ ì„¤ì • ìƒì„±
        optimal_config = {
            'hardware': gpu_info,
            'performance': {
                'max_concurrent_streams': self._calculate_optimal_streams(gpu_info),
                'batch_size_analyze': self._calculate_optimal_batch_size(gpu_info),
                'vram_safety_margin': 0.15,  # 15% ì•ˆì „ ë§ˆì§„
                'target_gpu_utilization': 0.85  # 85% ëª©í‘œ
            },
            'nvdec_settings': {
                'max_sessions': gpu_info['nvdec_max_sessions'],
                'preferred_format': 'nv12'
            },
            'nvenc_settings': {
                'max_sessions': gpu_info['nvenc_max_sessions'],
                'preset': 'medium',
                'rc_mode': 'cbr'
            },
            'generated_timestamp': datetime.now().isoformat(),
            'gpu_driver_version': gpu_info.get('driver_version'),
            'cuda_version': gpu_info.get('cuda_version')
        }
        
        self.logger.info("âœ… í•˜ë“œì›¨ì–´ í”„ë¡œë¹™ ì™„ë£Œ")
        return optimal_config
        
    def _probe_gpu_capabilities(self) -> Dict[str, Any]:
        """
        GPU í•˜ë“œì›¨ì–´ ëŠ¥ë ¥ ì¸¡ì •
        
        Returns:
            Dict[str, Any]: GPU ì •ë³´
            
        Raises:
            DualFaceTrackerError: GPU ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ì‹œ
        """
        if not check_cuda_available():
            raise DualFaceTrackerError("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            
        try:
            # PyTorchë¥¼ í†µí•œ GPU ì •ë³´ ìˆ˜ì§‘
            device = torch.cuda.current_device()
            gpu_name = torch.cuda.get_device_name(device)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = get_gpu_memory_info()
            vram_total_gb = memory_info['total'] // (1024**3)
            
            # NVDEC/NVENC ì„¸ì…˜ í•œë„ í…ŒìŠ¤íŠ¸
            nvdec_sessions = self._test_concurrent_decoders()
            nvenc_sessions = self._test_concurrent_encoders()
            
            # CUDA ë²„ì „
            cuda_version = torch.version.cuda
            
            gpu_info = {
                'gpu_name': gpu_name,
                'nvdec_max_sessions': nvdec_sessions,
                'nvenc_max_sessions': nvenc_sessions,
                'vram_gb': vram_total_gb,
                'cuda_version': cuda_version,
                'compute_capability': self._get_compute_capability(),
                'memory_info': memory_info
            }
            
            self.logger.info(f"ðŸŽ¯ GPU ê°ì§€: {gpu_name} ({vram_total_gb}GB VRAM)")
            self.logger.info(f"ðŸ”§ NVDEC ì„¸ì…˜: {nvdec_sessions}, NVENC ì„¸ì…˜: {nvenc_sessions}")
            
            return gpu_info
            
        except Exception as e:
            raise DualFaceTrackerError(f"GPU ëŠ¥ë ¥ ì¸¡ì • ì‹¤íŒ¨: {e}")
            
    def _test_concurrent_decoders(self) -> int:
        """
        ë™ì‹œ NVDEC ë””ì½”ë” ì„¸ì…˜ ìˆ˜ í…ŒìŠ¤íŠ¸
        
        Returns:
            int: ìµœëŒ€ ë™ì‹œ NVDEC ì„¸ì…˜ ìˆ˜
        """
        # RTX 5090ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ 4ê°œ NVDEC ì—”ì§„
        # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì—†ì´ GPU ì´ë¦„ ê¸°ë°˜ ì¶”ì •
        try:
            gpu_name = torch.cuda.get_device_name(0).lower()
            
            if 'rtx 5090' in gpu_name or 'rtx 4090' in gpu_name:
                return 4
            elif 'rtx' in gpu_name and ('3090' in gpu_name or '4080' in gpu_name):
                return 3
            elif 'rtx' in gpu_name:
                return 2
            else:
                return 2  # ì•ˆì „í•œ ê¸°ë³¸ê°’
                
        except Exception:
            self.logger.warning("âš ï¸ NVDEC ì„¸ì…˜ ìˆ˜ ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return 2
            
    def _test_concurrent_encoders(self) -> int:
        """
        ë™ì‹œ NVENC ì¸ì½”ë” ì„¸ì…˜ ìˆ˜ í…ŒìŠ¤íŠ¸
        
        Returns:
            int: ìµœëŒ€ ë™ì‹œ NVENC ì„¸ì…˜ ìˆ˜
        """
        # RTX 5090ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ 3ê°œ NVENC ì„¸ì…˜
        # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì—†ì´ GPU ì´ë¦„ ê¸°ë°˜ ì¶”ì •
        try:
            gpu_name = torch.cuda.get_device_name(0).lower()
            
            if 'rtx 5090' in gpu_name:
                return 3
            elif 'rtx 4090' in gpu_name:
                return 3
            elif 'rtx' in gpu_name and ('3090' in gpu_name or '4080' in gpu_name):
                return 2
            elif 'rtx' in gpu_name:
                return 2
            else:
                return 2  # ì•ˆì „í•œ ê¸°ë³¸ê°’
                
        except Exception:
            self.logger.warning("âš ï¸ NVENC ì„¸ì…˜ ìˆ˜ ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return 2
            
    def _get_compute_capability(self) -> str:
        """
        GPU Compute Capability í™•ì¸
        
        Returns:
            str: Compute capability (ì˜ˆ: "8.9")
        """
        try:
            device = torch.cuda.current_device()
            major, minor = torch.cuda.get_device_capability(device)
            return f"{major}.{minor}"
        except Exception:
            return "Unknown"
            
    def _calculate_optimal_streams(self, gpu_info: Dict[str, Any]) -> int:
        """
        ìµœì  ë™ì‹œ ìŠ¤íŠ¸ë¦¼ ìˆ˜ ê³„ì‚° (ë³´ìˆ˜ì  ì ‘ê·¼)
        
        Args:
            gpu_info: GPU ì •ë³´
            
        Returns:
            int: ìµœì  ìŠ¤íŠ¸ë¦¼ ìˆ˜
        """
        # NVDEC ì„¸ì…˜ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        nvdec_sessions = gpu_info.get('nvdec_max_sessions', 2)
        vram_gb = gpu_info.get('vram_gb', 8)
        
        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì œí•œ
        if vram_gb >= 24:  # RTX 5090 ë“±
            max_by_memory = 4
        elif vram_gb >= 16:
            max_by_memory = 3
        elif vram_gb >= 12:
            max_by_memory = 2
        else:
            max_by_memory = 1
            
        # ë³´ìˆ˜ì ìœ¼ë¡œ ì ì€ ê°’ ì„ íƒ
        optimal_streams = min(nvdec_sessions, max_by_memory)
        
        self.logger.info(f"ðŸŽ¯ ìµœì  ìŠ¤íŠ¸ë¦¼ ìˆ˜: {optimal_streams}")
        return optimal_streams
        
    def _calculate_optimal_batch_size(self, gpu_info: Dict[str, Any]) -> int:
        """
        ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        
        Args:
            gpu_info: GPU ì •ë³´
            
        Returns:
            int: ìµœì  ë°°ì¹˜ í¬ê¸°
        """
        vram_gb = gpu_info.get('vram_gb', 8)
        
        # VRAM ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if vram_gb >= 32:  # RTX 5090
            return 256
        elif vram_gb >= 24:  # RTX 4090
            return 128
        elif vram_gb >= 16:
            return 64
        elif vram_gb >= 12:
            return 32
        else:
            return 16
            
    def get_system_info(self) -> Dict[str, Any]:
        """
        ì‹œìŠ¤í…œ ì „ì²´ ì •ë³´ ìˆ˜ì§‘
        
        Returns:
            Dict[str, Any]: ì‹œìŠ¤í…œ ì •ë³´
        """
        import psutil
        import platform
        
        return {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_gb': psutil.virtual_memory().total // (1024**3),
            'disk_space_gb': psutil.disk_usage('/').total // (1024**3),
            'cuda_available': check_cuda_available(),
            'torch_version': torch.__version__
        }