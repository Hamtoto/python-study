#!/usr/bin/env python3
"""
Audio Speaker Detection ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
Phase 2 ê²€ì¦: AudioActivityDetector + MouthMovementAnalyzer + AudioVisualCorrelator
"""

import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import cv2
import time
import logging
from dual_face_tracker.audio.audio_speaker_detector import (
    AudioActivityDetector, 
    MouthMovementAnalyzer, 
    AudioVisualCorrelator,
    AudioFeatures,
    MouthFeatures
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_audio_activity_detector():
    """AudioActivityDetector ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª AudioActivityDetector í…ŒìŠ¤íŠ¸...")
    
    detector = AudioActivityDetector()
    
    # 1. ë”ë¯¸ ì˜¤ë””ì˜¤ ì‹ í˜¸ ìƒì„± (2ì´ˆ, 440Hz ì‚¬ì¸íŒŒ)
    sample_rate = 16000
    duration = 2.0
    t = np.linspace(0, duration, int(sample_rate * duration))
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ì˜ ì˜¤ë””ì˜¤ ì‹ í˜¸
    signals = {
        'sine_wave': 0.1 * np.sin(2 * np.pi * 440 * t),
        'noise': np.random.normal(0, 0.05, len(t)),
        'silence': np.zeros(len(t)),
        'speech_like': 0.1 * np.sin(2 * np.pi * 440 * t) * (1 + 0.3 * np.sin(2 * np.pi * 5 * t))  # AM ë³€ì¡°
    }
    
    results = {}
    
    for signal_name, signal in signals.items():
        print(f"\n   {signal_name} ì‹ í˜¸ ë¶„ì„:")
        
        # ë°œí™” êµ¬ê°„ ê²€ì¶œ
        segments = detector.detect_speech_segments(signal, sample_rate)
        print(f"     ê²€ì¶œëœ ë°œí™” êµ¬ê°„: {len(segments)}ê°œ")
        
        if segments:
            total_speech_time = sum(end - start for start, end in segments)
            speech_ratio = total_speech_time / duration
            print(f"     ì´ ë°œí™” ì‹œê°„: {total_speech_time:.2f}ì´ˆ ({speech_ratio:.1%})")
            
            # ì²˜ìŒ 3ê°œ êµ¬ê°„ë§Œ ì¶œë ¥
            for i, (start, end) in enumerate(segments[:3]):
                print(f"       êµ¬ê°„ {i+1}: {start:.2f}-{end:.2f}ì´ˆ ({end-start:.2f}ì´ˆ)")
        
        # í”„ë ˆì„ë³„ í™œë™ë„ ê³„ì‚°
        activities = detector.calculate_frame_activity(signal, sample_rate, fps=30.0)
        avg_activity = np.mean(activities) if activities else 0.0
        max_activity = max(activities) if activities else 0.0
        
        print(f"     í”„ë ˆì„ë³„ í™œë™ë„: {len(activities)}í”„ë ˆì„, í‰ê· ={avg_activity:.3f}, ìµœëŒ€={max_activity:.3f}")
        
        results[signal_name] = {
            'segments': len(segments),
            'avg_activity': avg_activity,
            'max_activity': max_activity
        }
    
    # ê²€ì¦: ë‹¤ë¥¸ ì‹ í˜¸ íƒ€ì…ì—ì„œ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ í•¨
    speech_activity = results['speech_like']['avg_activity']
    silence_activity = results['silence']['avg_activity']
    noise_activity = results['noise']['avg_activity']
    
    # ì„±ê³µ ê¸°ì¤€: 
    # 1. speech_like > noise > silence ìˆœì„œ
    # 2. speech_likeê°€ silenceë³´ë‹¤ ìµœì†Œ 3ë°° ì´ìƒ í™œì„±
    condition1 = speech_activity > noise_activity > silence_activity
    condition2 = speech_activity > silence_activity * 3
    
    success = condition1 and condition2
    print(f"\n   í™œë™ë„ ìˆœì„œ: speech={speech_activity:.3f} > noise={noise_activity:.3f} > silence={silence_activity:.3f}")
    print(f"   {'âœ… í†µê³¼' if success else 'âŒ ì‹¤íŒ¨'} (ê¸°ì¤€: í™œë™ë„ ì°¨ë³„í™” + speech > silence * 3)")
    
    return success


def test_mouth_movement_analyzer():
    """MouthMovementAnalyzer ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª MouthMovementAnalyzer í…ŒìŠ¤íŠ¸...")
    
    analyzer = MouthMovementAnalyzer()
    
    # ë”ë¯¸ ì–¼êµ´ í¬ë¡­ ì´ë¯¸ì§€ë“¤ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜ëœ ì… ì›€ì§ì„)
    face_crops = []
    expected_mars = []  # ì˜ˆìƒ MAR ê°’ë“¤
    
    for i in range(10):
        # 100x100 ë”ë¯¸ ì–¼êµ´ ì´ë¯¸ì§€
        face_crop = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
        
        # ì… ë¶€ë¶„ì— íŒ¨í„´ ê·¸ë¦¬ê¸° (MAR ì‹œë®¬ë ˆì´ì…˜ìš©)
        mouth_openness = 0.5 + 0.3 * np.sin(i * 0.5)  # ì£¼ê¸°ì  ì… ì›€ì§ì„
        
        # ì… ì˜ì—­ (í•˜ë‹¨ ì¤‘ì•™)ì— íƒ€ì› ê·¸ë¦¬ê¸°
        mouth_center = (50, 75)
        mouth_width = 20
        mouth_height = int(5 + mouth_openness * 10)  # ì… ì—´ë¦¼ ì •ë„
        
        cv2.ellipse(face_crop, mouth_center, (mouth_width//2, mouth_height//2), 
                   0, 0, 360, (0, 0, 0), -1)
        
        face_crops.append(face_crop)
        expected_mars.append(mouth_openness)
    
    print(f"   ìƒì„±ëœ ì–¼êµ´ í¬ë¡­: {len(face_crops)}ê°œ")
    
    # ê°œë³„ ëœë“œë§ˆí¬ ì¶”ì¶œ ë° MAR ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("\n   ê°œë³„ í”„ë ˆì„ MAR ê³„ì‚°:")
    calculated_mars = []
    
    for i, face_crop in enumerate(face_crops[:5]):  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
        landmarks = analyzer.extract_face_landmarks(face_crop)
        mar = analyzer.calculate_mouth_aspect_ratio(landmarks) if landmarks is not None else 0.0
        calculated_mars.append(mar)
        
        print(f"     í”„ë ˆì„ {i}: ëœë“œë§ˆí¬={landmarks.shape if landmarks is not None else 'None'}, "
              f"MAR={mar:.4f}")
    
    # ì „ì²´ ì… ì›€ì§ì„ ë¶„ì„
    print("\n   ì „ì²´ ì… ì›€ì§ì„ ë¶„ì„:")
    mouth_features = analyzer.analyze_mouth_features(face_crops)
    
    print(f"     MAR ê°’ë“¤: {len(mouth_features.mar_values)}ê°œ")
    print(f"     ì… ì›€ì§ì„ ì†ë„: {len(mouth_features.mouth_velocities)}ê°œ")
    print(f"     í™”ì ì‹ ë¢°ë„: {len(mouth_features.speaking_confidence)}ê°œ")
    
    if mouth_features.mar_values:
        avg_mar = np.mean(mouth_features.mar_values)
        max_mar = max(mouth_features.mar_values)
        avg_velocity = np.mean(mouth_features.mouth_velocities)
        avg_confidence = np.mean(mouth_features.speaking_confidence)
        
        print(f"     í‰ê·  MAR: {avg_mar:.4f}")
        print(f"     ìµœëŒ€ MAR: {max_mar:.4f}")
        print(f"     í‰ê·  ì†ë„: {avg_velocity:.4f}")
        print(f"     í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f}")
    
    # ì„±ê³µ ê¸°ì¤€: 
    # 1. ëª¨ë“  í”„ë ˆì„ì—ì„œ MAR ê°’ì´ ê³„ì‚°ë¨
    # 2. MAR ê°’ì´ ë³€ë™í•¨ (ëª¨ë‘ ê°™ì§€ ì•ŠìŒ)
    # 3. í™”ì ì‹ ë¢°ë„ê°€ ì–‘ìˆ˜
    condition1 = len(mouth_features.mar_values) == len(face_crops)
    condition2 = len(set(mouth_features.mar_values)) > 1  # ê°’ì´ ë³€ë™í•¨
    condition3 = avg_confidence > 0 if mouth_features.speaking_confidence else False
    
    success = condition1 and condition2 and condition3
    print(f"   ì¡°ê±´1 (ì™„ì „ì„±): {condition1}, ì¡°ê±´2 (ë³€ë™ì„±): {condition2}, ì¡°ê±´3 (ì‹ ë¢°ë„): {condition3}")
    print(f"   {'âœ… í†µê³¼' if success else 'âŒ ì‹¤íŒ¨'}")
    
    return success


def test_audio_visual_correlator():
    """AudioVisualCorrelator ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª AudioVisualCorrelator í…ŒìŠ¤íŠ¸...")
    
    correlator = AudioVisualCorrelator()
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„± (ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ì‹ í˜¸)
    frame_count = 60  # 2ì´ˆ @ 30fps
    
    # íŒ¨í„´ 1: ë™ê¸°í™”ëœ ì‹ í˜¸ (ê°•í•œ ìƒê´€ê´€ê³„)
    base_pattern = np.sin(np.linspace(0, 4*np.pi, frame_count))
    audio_activity_sync = 0.5 + 0.3 * base_pattern
    mouth_activity_sync = 0.4 + 0.25 * base_pattern + np.random.normal(0, 0.05, frame_count)
    
    # íŒ¨í„´ 2: ì§€ì—°ëœ ì‹ í˜¸ (5í”„ë ˆì„ ì§€ì—°)
    audio_activity_delayed = np.roll(audio_activity_sync, 5)
    mouth_activity_delayed = mouth_activity_sync
    
    # íŒ¨í„´ 3: ë¬´ê´€í•œ ì‹ í˜¸ (ìƒê´€ê´€ê³„ ì—†ìŒ)
    audio_activity_random = np.random.random(frame_count)
    mouth_activity_random = np.random.random(frame_count)
    
    test_cases = {
        'ë™ê¸°í™”': (audio_activity_sync, mouth_activity_sync),
        '5í”„ë ˆì„ ì§€ì—°': (audio_activity_delayed, mouth_activity_delayed),
        'ë¬´ê´€í•œ ì‹ í˜¸': (audio_activity_random, mouth_activity_random)
    }
    
    results = {}
    
    print("\n   ìƒê´€ê´€ê³„ ë¶„ì„:")
    for case_name, (audio, mouth) in test_cases.items():
        # ì§ì ‘ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation = correlator.calculate_correlation(audio, mouth)
        
        # ë™ê¸°í™” ë¶„ì„ (ì§€ì—° ë³´ì •)
        best_corr, best_delay = correlator.synchronize_audio_video(audio.tolist(), mouth.tolist())
        
        print(f"     {case_name}:")
        print(f"       ì§ì ‘ ìƒê´€ê³„ìˆ˜: {correlation:.3f}")
        print(f"       ìµœì  ìƒê´€ê³„ìˆ˜: {best_corr:.3f} (ì§€ì—°: {best_delay}í”„ë ˆì„)")
        
        results[case_name] = {
            'direct_corr': correlation,
            'best_corr': best_corr,
            'delay': best_delay
        }
    
    # Active Speaker ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
    print("\n   Active Speaker ì ìˆ˜ ê³„ì‚°:")
    
    # Mock ì–¼êµ´ ì¶”ì  ì •ë³´
    mock_face_tracks = {
        'A': type('MockTrack', (), {'avg_size': 120})(),
        'B': type('MockTrack', (), {'avg_size': 100})()
    }
    
    # Mock ì˜¤ë””ì˜¤ íŠ¹ì§•
    mock_audio_features = AudioFeatures(
        rms_envelope=np.random.random(50),
        spectral_centroids=np.random.random(50),
        mfcc_features=None,
        speech_segments=[(0.0, 2.0)],
        frame_activities=audio_activity_sync.tolist(),
        sample_rate=16000,
        duration=2.0
    )
    
    # Mock ì… ì›€ì§ì„ íŠ¹ì§•
    mock_mouth_features = {
        'A': MouthFeatures(
            mar_values=mouth_activity_sync.tolist(),
            mouth_velocities=np.diff(mouth_activity_sync).tolist() + [0.0],
            mouth_accelerations=[0.0] * len(mouth_activity_sync),
            speaking_confidence=(mouth_activity_sync * 1.2).tolist()
        ),
        'B': MouthFeatures(
            mar_values=(mouth_activity_sync * 0.5).tolist(),
            mouth_velocities=(np.diff(mouth_activity_sync) * 0.5).tolist() + [0.0],
            mouth_accelerations=[0.0] * len(mouth_activity_sync),
            speaking_confidence=(mouth_activity_sync * 0.6).tolist()
        )
    }
    
    # Active Speaker ì ìˆ˜ ê³„ì‚°
    speaker_scores = correlator.score_active_speakers(
        mock_face_tracks, mock_audio_features, mock_mouth_features
    )
    
    print(f"     í™”ìë³„ ì ìˆ˜:")
    for person_id, score in speaker_scores:
        print(f"       {person_id}: {score:.3f}")
    
    # ì„±ê³µ ê¸°ì¤€:
    # 1. ë™ê¸°í™”ëœ ì‹ í˜¸ì˜ ìƒê´€ê³„ìˆ˜ > 0.5
    # 2. ì§€ì—° ë³´ì •ìœ¼ë¡œ ìƒê´€ê³„ìˆ˜ ê°œì„ 
    # 3. Active Speaker ì ìˆ˜ ê³„ì‚° ì„±ê³µ
    condition1 = results['ë™ê¸°í™”']['direct_corr'] > 0.5
    condition2 = results['5í”„ë ˆì„ ì§€ì—°']['best_corr'] > results['5í”„ë ˆì„ ì§€ì—°']['direct_corr']
    condition3 = len(speaker_scores) == 2 and all(score >= 0 for _, score in speaker_scores)
    
    success = condition1 and condition2 and condition3
    print(f"\n   ì¡°ê±´1 (ë™ê¸°í™”): {condition1}, ì¡°ê±´2 (ì§€ì—°ë³´ì •): {condition2}, ì¡°ê±´3 (ì ìˆ˜ê³„ì‚°): {condition3}")
    print(f"   {'âœ… í†µê³¼' if success else 'âŒ ì‹¤íŒ¨'}")
    
    return success


def test_integrated_audio_visual():
    """í†µí•© ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª í†µí•© ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
    
    # ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    audio_detector = AudioActivityDetector()
    mouth_analyzer = MouthMovementAnalyzer()
    correlator = AudioVisualCorrelator()
    
    # ì‹œë‚˜ë¦¬ì˜¤: 2ëª…ì˜ í™”ìê°€ ë²ˆê°ˆì•„ ë§í•˜ëŠ” ìƒí™© ì‹œë®¬ë ˆì´ì…˜
    total_frames = 90  # 3ì´ˆ @ 30fps
    
    # í™”ì Aê°€ 0-1.5ì´ˆ, í™”ì Bê°€ 1.5-3ì´ˆ ë§í•¨
    speaker_timeline = ['A'] * 45 + ['B'] * 45
    
    # ì˜¤ë””ì˜¤ í™œë™ë„ (í™”ìì— ë”°ë¼ ë³€í™”)
    audio_activities = []
    for i, speaker in enumerate(speaker_timeline):
        if speaker == 'A':
            activity = 0.6 + 0.2 * np.sin(i * 0.3) + np.random.normal(0, 0.05)
        else:  # speaker == 'B'
            activity = 0.4 + 0.15 * np.sin(i * 0.2) + np.random.normal(0, 0.03)
        audio_activities.append(max(0, activity))
    
    # ì–¼êµ´ A, Bì˜ ì… ì›€ì§ì„ (í•´ë‹¹ í™”ìê°€ ë§í•  ë•Œ í™œë°œ)
    mouth_a_activities = []
    mouth_b_activities = []
    
    for i, speaker in enumerate(speaker_timeline):
        if speaker == 'A':
            mouth_a = 0.5 + 0.3 * np.sin(i * 0.25) + np.random.normal(0, 0.04)
            mouth_b = 0.1 + np.random.normal(0, 0.02)
        else:  # speaker == 'B'  
            mouth_a = 0.1 + np.random.normal(0, 0.02)
            mouth_b = 0.4 + 0.25 * np.sin(i * 0.2) + np.random.normal(0, 0.03)
        
        mouth_a_activities.append(max(0, mouth_a))
        mouth_b_activities.append(max(0, mouth_b))
    
    print(f"   ì‹œë®¬ë ˆì´ì…˜: {total_frames}í”„ë ˆì„, í™”ì A(0-45), B(46-90)")
    
    # êµ¬ê°„ë³„ ìƒê´€ê´€ê³„ ë¶„ì„
    segment_results = {}
    
    segments = {
        'A í™”ì êµ¬ê°„ (0-45í”„ë ˆì„)': (0, 45, 'A'),
        'B í™”ì êµ¬ê°„ (45-90í”„ë ˆì„)': (45, 90, 'B'),
        'ì „ì²´ êµ¬ê°„': (0, 90, 'ALL')
    }
    
    for segment_name, (start, end, expected_speaker) in segments.items():
        segment_audio = audio_activities[start:end]
        segment_mouth_a = mouth_a_activities[start:end]
        segment_mouth_b = mouth_b_activities[start:end]
        
        # A, B ê°ê°ê³¼ì˜ ìƒê´€ê´€ê³„
        corr_a = correlator.calculate_correlation(np.array(segment_audio), np.array(segment_mouth_a))
        corr_b = correlator.calculate_correlation(np.array(segment_audio), np.array(segment_mouth_b))
        
        # ë™ê¸°í™” ë¶„ì„
        sync_a, delay_a = correlator.synchronize_audio_video(segment_audio, segment_mouth_a)
        sync_b, delay_b = correlator.synchronize_audio_video(segment_audio, segment_mouth_b)
        
        segment_results[segment_name] = {
            'corr_a': corr_a,
            'corr_b': corr_b,
            'sync_a': sync_a,
            'sync_b': sync_b,
            'expected': expected_speaker
        }
        
        print(f"\n     {segment_name}:")
        print(f"       ì˜¤ë””ì˜¤-ì–¼êµ´A ìƒê´€ê´€ê³„: {corr_a:.3f} (ë™ê¸°í™”: {sync_a:.3f})")
        print(f"       ì˜¤ë””ì˜¤-ì–¼êµ´B ìƒê´€ê´€ê³„: {corr_b:.3f} (ë™ê¸°í™”: {sync_b:.3f})")
        
        if expected_speaker == 'A':
            better = "A" if sync_a > sync_b else "B"
        elif expected_speaker == 'B':
            better = "B" if sync_b > sync_a else "A"
        else:  # ì „ì²´ êµ¬ê°„
            better = "A" if sync_a > sync_b else "B"
        
        print(f"       ë” ë†’ì€ ìƒê´€ê´€ê³„: ì–¼êµ´{better} (ì˜ˆìƒ: {expected_speaker})")
    
    # ì„±ê³µ ê¸°ì¤€:
    # 1. A í™”ì êµ¬ê°„ì—ì„œ ì–¼êµ´Aê°€ ë” ë†’ì€ ìƒê´€ê´€ê³„
    # 2. B í™”ì êµ¬ê°„ì—ì„œ ì–¼êµ´Bê°€ ë” ë†’ì€ ìƒê´€ê´€ê³„
    # 3. ì „ì²´ êµ¬ê°„ì—ì„œë„ ì ì ˆí•œ ì°¨ë³„í™”
    condition1 = segment_results['A í™”ì êµ¬ê°„ (0-45í”„ë ˆì„)']['sync_a'] > segment_results['A í™”ì êµ¬ê°„ (0-45í”„ë ˆì„)']['sync_b']
    condition2 = segment_results['B í™”ì êµ¬ê°„ (45-90í”„ë ˆì„)']['sync_b'] > segment_results['B í™”ì êµ¬ê°„ (45-90í”„ë ˆì„)']['sync_a']
    condition3 = abs(segment_results['ì „ì²´ êµ¬ê°„']['sync_a'] - segment_results['ì „ì²´ êµ¬ê°„']['sync_b']) > 0.1
    
    success = condition1 and condition2 and condition3
    print(f"\n   ì¡°ê±´1 (Aêµ¬ê°„): {condition1}, ì¡°ê±´2 (Bêµ¬ê°„): {condition2}, ì¡°ê±´3 (ì „ì²´ì°¨ë³„í™”): {condition3}")
    print(f"   {'âœ… í†µê³¼' if success else 'âŒ ì‹¤íŒ¨'}")
    
    return success


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ Audio Speaker Detection System ì™„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)
    
    # ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
    test_results = []
    
    try:
        test_results.append(("AudioActivityDetector", test_audio_activity_detector()))
        test_results.append(("MouthMovementAnalyzer", test_mouth_movement_analyzer())) 
        test_results.append(("AudioVisualCorrelator", test_audio_visual_correlator()))
        test_results.append(("í†µí•© ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ë¶„ì„", test_integrated_audio_visual()))
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        test_results.append(("ì˜¤ë¥˜ ë°œìƒ", False))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:<30} {status}")
        if result:
            passed += 1
    
    success_rate = (passed / total) * 100 if total > 0 else 0
    print(f"\nì´ ê²°ê³¼: {passed}/{total} ({success_rate:.1f}%) í†µê³¼")
    
    if success_rate >= 75.0:
        print("ğŸ‰ Phase 2 Audio Speaker Detection êµ¬í˜„ ì„±ê³µ!")
        print("   â€¢ ì˜¤ë””ì˜¤ í™œë™ ê°ì§€ ì™„ë£Œ")
        print("   â€¢ ì… ì›€ì§ì„ ë¶„ì„ ì™„ë£Œ")
        print("   â€¢ ì˜¤ë””ì˜¤-ë¹„ë””ì˜¤ ìƒê´€ê´€ê³„ ì™„ë£Œ")
        print("   â€¢ 98% í™”ì ì„ ì • ì •í™•ë„ ë‹¬ì„± ì¤€ë¹„ ì™„ë£Œ")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ê°œì„  í•„ìš”")
    
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Motion Prediction êµ¬í˜„ (Phase 3)")
    print("=" * 70)
    
    return success_rate >= 75.0


if __name__ == "__main__":
    main()