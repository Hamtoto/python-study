#!/usr/bin/env python3
"""
CUDA/TensorRT í™˜ê²½ ì§„ë‹¨ ë„êµ¬
D7.1 ì´ìŠˆ í•´ê²°ì„ ìœ„í•œ ì¢…í•© ì§„ë‹¨
"""

import os
import sys
import subprocess
import platform
from pathlib import Path

def run_command(cmd, description="", capture_output=True):
    """ëª…ë ¹ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    print(f"\nğŸ” {description}")
    print(f"$ {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=capture_output, 
                              text=True, timeout=30)
        
        if capture_output:
            if result.returncode == 0:
                print(f"âœ… Success")
                if result.stdout:
                    print(result.stdout.strip())
                return True, result.stdout
            else:
                print(f"âŒ Failed (return code: {result.returncode})")
                if result.stderr:
                    print(f"Error: {result.stderr.strip()}")
                return False, result.stderr
        else:
            return result.returncode == 0, ""
            
    except subprocess.TimeoutExpired:
        print("â° Timeout after 30 seconds")
        return False, "Timeout"
    except Exception as e:
        print(f"âŒ Exception: {e}")
        return False, str(e)

def check_system_info():
    """ì‹œìŠ¤í…œ ê¸°ë³¸ ì •ë³´ í™•ì¸"""
    print("=" * 60)
    print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
    print("=" * 60)
    
    print(f"OS: {platform.system()} {platform.release()}")
    print(f"Architecture: {platform.machine()}")
    print(f"Python: {sys.version}")
    
    # GPU í™•ì¸
    success, output = run_command("lspci | grep -i nvidia", "NVIDIA GPU í™•ì¸")
    if success and output:
        print(f"GPU ë°œê²¬: {len(output.splitlines())} ê°œ")
    
    return True

def check_nvidia_driver():
    """NVIDIA ë“œë¼ì´ë²„ í™•ì¸"""
    print("\n=" * 60)
    print("ğŸš— NVIDIA ë“œë¼ì´ë²„")
    print("=" * 60)
    
    # nvidia-smi ì‹¤í–‰
    success, output = run_command("nvidia-smi", "nvidia-smi ì‹¤í–‰")
    if not success:
        print("âŒ nvidia-smi ì‹¤í–‰ ì‹¤íŒ¨ - NVIDIA ë“œë¼ì´ë²„ ë¬¸ì œ")
        return False
    
    # ë“œë¼ì´ë²„ ë²„ì „ ì¶”ì¶œ
    if "Driver Version:" in output:
        lines = output.split('\n')
        for line in lines:
            if "Driver Version:" in line:
                print(f"ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸ë¨: {line.strip()}")
                break
    
    # CUDA ë²„ì „ í™•ì¸
    if "CUDA Version:" in output:
        lines = output.split('\n')
        for line in lines:
            if "CUDA Version:" in line:
                print(f"CUDA ë²„ì „ í™•ì¸ë¨: {line.strip()}")
                break
    
    return True

def check_cuda_installation():
    """CUDA ì„¤ì¹˜ í™•ì¸"""
    print("\n=" * 60)
    print("âš¡ CUDA ì„¤ì¹˜")
    print("=" * 60)
    
    # nvcc í™•ì¸
    success, output = run_command("nvcc --version", "CUDA Compiler (nvcc) í™•ì¸")
    if success:
        print("CUDA ì»´íŒŒì¼ëŸ¬ ì •ìƒ")
    else:
        print("âš ï¸ CUDA ì»´íŒŒì¼ëŸ¬ ì—†ìŒ - ëŸ°íƒ€ì„ë§Œ ì„¤ì¹˜ë¨")
    
    # CUDA ê²½ë¡œ í™•ì¸
    cuda_paths = [
        "/usr/local/cuda",
        "/opt/cuda",
        "/usr/cuda"
    ]
    
    print("\nCUDA ì„¤ì¹˜ ê²½ë¡œ í™•ì¸:")
    for path in cuda_paths:
        if Path(path).exists():
            print(f"âœ… {path} ì¡´ì¬")
            lib_path = Path(path) / "lib64"
            if lib_path.exists():
                print(f"  â””â”€ {lib_path} ì¡´ì¬")
        else:
            print(f"âŒ {path} ì—†ìŒ")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    print(f"\nCUDA í™˜ê²½ ë³€ìˆ˜:")
    cuda_home = os.environ.get('CUDA_HOME', 'Not set')
    cuda_path = os.environ.get('CUDA_PATH', 'Not set')
    ld_path = os.environ.get('LD_LIBRARY_PATH', 'Not set')
    
    print(f"CUDA_HOME: {cuda_home}")
    print(f"CUDA_PATH: {cuda_path}")
    print(f"LD_LIBRARY_PATH: {ld_path}")
    
    return True

def check_python_packages():
    """Python íŒ¨í‚¤ì§€ í™•ì¸"""
    print("\n=" * 60)
    print("ğŸ Python íŒ¨í‚¤ì§€")
    print("=" * 60)
    
    packages_to_check = [
        "torch",
        "torchvision", 
        "tensorrt",
        "onnx",
        "onnxruntime-gpu",
        "ultralytics",
        "numpy"
    ]
    
    for package in packages_to_check:
        try:
            if package == "torch":
                import torch
                print(f"âœ… torch {torch.__version__}")
                print(f"   CUDA available: {torch.cuda.is_available()}")
                if torch.cuda.is_available():
                    print(f"   CUDA version: {torch.version.cuda}")
                    print(f"   GPU count: {torch.cuda.device_count()}")
                    for i in range(torch.cuda.device_count()):
                        print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                        
            elif package == "tensorrt":
                import tensorrt as trt
                print(f"âœ… tensorrt {trt.__version__}")
                
            elif package == "onnxruntime-gpu":
                import onnxruntime as ort
                print(f"âœ… onnxruntime {ort.__version__}")
                print(f"   Available providers: {ort.get_available_providers()}")
                
            else:
                module = __import__(package)
                if hasattr(module, '__version__'):
                    print(f"âœ… {package} {module.__version__}")
                else:
                    print(f"âœ… {package} (no version info)")
                    
        except ImportError:
            print(f"âŒ {package} not installed")
        except Exception as e:
            print(f"âš ï¸ {package} error: {e}")
    
    return True

def check_cuda_runtime():
    """CUDA ëŸ°íƒ€ì„ í…ŒìŠ¤íŠ¸"""
    print("\n=" * 60)
    print("ğŸ§ª CUDA ëŸ°íƒ€ì„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        import torch
        
        # ê¸°ë³¸ CUDA í…ŒìŠ¤íŠ¸
        if torch.cuda.is_available():
            print("âœ… PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥")
            
            # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°
            device = torch.device('cuda:0')
            print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
            
            # ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.matmul(x, y)
            print(f"âœ… GPU í…ì„œ ì—°ì‚° ì„±ê³µ - ê²°ê³¼ shape: {z.shape}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            memory_allocated = torch.cuda.memory_allocated(device) / 1024**3
            memory_reserved = torch.cuda.memory_reserved(device) / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {memory_allocated:.2f}GB, ì˜ˆì•½: {memory_reserved:.2f}GB")
            
            return True
        else:
            print("âŒ PyTorch CUDA ì‚¬ìš© ë¶ˆê°€")
            return False
            
    except Exception as e:
        print(f"âŒ CUDA ëŸ°íƒ€ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def check_tensorrt():
    """TensorRT ìƒì„¸ í™•ì¸"""
    print("\n=" * 60)
    print("ğŸš€ TensorRT ì§„ë‹¨")
    print("=" * 60)
    
    try:
        import tensorrt as trt
        print(f"âœ… TensorRT ë²„ì „: {trt.__version__}")
        
        # TensorRT Logger ìƒì„± í…ŒìŠ¤íŠ¸
        logger = trt.Logger(trt.Logger.INFO)
        print("âœ… TensorRT Logger ìƒì„± ì„±ê³µ")
        
        # Builder ìƒì„± í…ŒìŠ¤íŠ¸
        try:
            builder = trt.Builder(logger)
            print("âœ… TensorRT Builder ìƒì„± ì„±ê³µ")
            
            # Runtime ìƒì„± í…ŒìŠ¤íŠ¸
            runtime = trt.Runtime(logger)
            print("âœ… TensorRT Runtime ìƒì„± ì„±ê³µ")
            
            return True
            
        except Exception as e:
            print(f"âŒ TensorRT Builder/Runtime ìƒì„± ì‹¤íŒ¨: {e}")
            print("   ì´ê²ƒì€ CUDA Error 35ì˜ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            return False
            
    except ImportError:
        print("âŒ TensorRT íŒ¨í‚¤ì§€ ì—†ìŒ")
        return False
    except Exception as e:
        print(f"âŒ TensorRT í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def suggest_solutions():
    """í•´ê²° ë°©ì•ˆ ì œì‹œ"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ í•´ê²° ë°©ì•ˆ")
    print("=" * 60)
    
    print("""
ğŸ”§ CUDA Error 35 í•´ê²° ë°©ì•ˆ:

1. **NVIDIA ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜**:
   sudo apt purge nvidia-*
   sudo apt install nvidia-driver-525  # ë˜ëŠ” ìµœì‹  ë²„ì „
   sudo reboot

2. **CUDA ëŸ°íƒ€ì„ ì¬ì„¤ì¹˜**:
   # Docker í™˜ê²½ì—ì„œëŠ” í˜¸ìŠ¤íŠ¸ì˜ CUDAì™€ ë²„ì „ ë§ì¶”ê¸°
   pip uninstall tensorrt
   pip install tensorrt==10.5.0  # CUDA 12.8 í˜¸í™˜

3. **ëŒ€ì•ˆ: ONNX Runtime ì‚¬ìš©**:
   # TensorRT ëŒ€ì‹  ONNX Runtimeìœ¼ë¡œ Phase 2 ì§„í–‰
   # ì„±ëŠ¥ì€ ì•½ê°„ ë‚®ì§€ë§Œ ì•ˆì •ì 

4. **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**:
   export CUDA_VISIBLE_DEVICES=0
   export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

5. **DevContainer ì¬ì‹œì‘**:
   # GPU ê¶Œí•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ
   docker system prune
   ./run_dev.sh
""")

def main():
    """ë©”ì¸ ì§„ë‹¨ í•¨ìˆ˜"""
    print("ğŸ”¬ CUDA/TensorRT í™˜ê²½ ì¢…í•© ì§„ë‹¨")
    print("D7.1 ì´ìŠˆ í•´ê²°ì„ ìœ„í•œ ì‹œìŠ¤í…œ ë¶„ì„")
    
    results = {}
    
    # 1. ì‹œìŠ¤í…œ ì •ë³´
    results['system'] = check_system_info()
    
    # 2. NVIDIA ë“œë¼ì´ë²„
    results['driver'] = check_nvidia_driver()
    
    # 3. CUDA ì„¤ì¹˜
    results['cuda'] = check_cuda_installation()
    
    # 4. Python íŒ¨í‚¤ì§€
    results['packages'] = check_python_packages()
    
    # 5. CUDA ëŸ°íƒ€ì„
    results['runtime'] = check_cuda_runtime()
    
    # 6. TensorRT
    results['tensorrt'] = check_tensorrt()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ì§„ë‹¨ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"{test_name.ljust(10)}: {status}")
        if result:
            passed += 1
    
    print(f"\nì „ì²´: {passed}/{total} í†µê³¼")
    
    if not results.get('tensorrt', False):
        print("\nâš ï¸ TensorRT ì‹¤íŒ¨ - ONNX Runtimeìœ¼ë¡œ ëŒ€ì•ˆ ì§„í–‰ ê¶Œì¥")
    
    # í•´ê²° ë°©ì•ˆ ì œì‹œ
    suggest_solutions()
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)