#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ TensorRT ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ (ì˜ì¡´ì„± ìµœì†Œí™”)
"""

import sys
import argparse
from pathlib import Path
import tensorrt as trt
import numpy as np

def convert_onnx_to_tensorrt(onnx_path, engine_path, precision="fp16", max_batch_size=8):
    """ONNXë¥¼ TensorRTë¡œ ë³€í™˜"""
    print(f"ğŸ”§ Converting {onnx_path} to TensorRT...")
    
    # TensorRT Logger
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
    
    # Builder ìƒì„±
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # ONNX íŒŒì¼ íŒŒì‹±
    print("  Parsing ONNX model...")
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            print("âŒ Failed to parse ONNX model")
            for error in range(parser.num_errors):
                print(f"  Error: {parser.get_error(error)}")
            return False
    
    # Config ì„¤ì •
    config = builder.create_builder_config()
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 4GB
    
    # ì •ë°€ë„ ì„¤ì •
    if precision == "fp16":
        if builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            print("  FP16 precision enabled")
        else:
            print("  Warning: FP16 not supported, using FP32")
    
    # ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •
    profile = builder.create_optimization_profile()
    
    # ì…ë ¥ í…ì„œ ì •ë³´
    for i in range(network.num_inputs):
        input_tensor = network.get_input(i)
        input_name = input_tensor.name
        input_shape = input_tensor.shape
        
        print(f"  Input: {input_name} - shape: {input_shape}")
        
        # ë™ì  shape ì„¤ì • (ë°°ì¹˜ë§Œ ë™ì )
        min_shape = [1] + list(input_shape[1:])
        opt_shape = [max_batch_size // 2] + list(input_shape[1:])
        max_shape = [max_batch_size] + list(input_shape[1:])
        
        profile.set_shape(input_name, min_shape, opt_shape, max_shape)
        print(f"    Dynamic shapes: {min_shape} / {opt_shape} / {max_shape}")
    
    config.add_optimization_profile(profile)
    
    # ì—”ì§„ ë¹Œë“œ
    print("  Building TensorRT engine... (this may take a while)")
    engine_bytes = builder.build_serialized_network(network, config)
    
    if engine_bytes is None:
        print("âŒ Failed to build engine")
        return False
    
    # ì—”ì§„ ì €ì¥
    with open(engine_path, 'wb') as f:
        f.write(engine_bytes)
    
    # íŒŒì¼ í¬ê¸° ì¶œë ¥
    size_mb = Path(engine_path).stat().st_size / (1024 * 1024)
    print(f"âœ… Engine saved: {engine_path} ({size_mb:.1f} MB)")
    
    return True

def main():
    parser = argparse.ArgumentParser(description="Convert ONNX to TensorRT")
    parser.add_argument("--onnx", required=True, help="ONNX model path")
    parser.add_argument("--engine", help="Output engine path")
    parser.add_argument("--precision", choices=["fp32", "fp16"], default="fp16")
    parser.add_argument("--max-batch", type=int, default=8)
    
    args = parser.parse_args()
    
    onnx_path = Path(args.onnx)
    if not onnx_path.exists():
        print(f"âŒ ONNX file not found: {onnx_path}")
        return 1
    
    engine_path = Path(args.engine) if args.engine else onnx_path.with_suffix('.engine')
    
    # ì—”ì§„ ë””ë ‰í† ë¦¬ ìƒì„±
    engine_path.parent.mkdir(exist_ok=True)
    
    if convert_onnx_to_tensorrt(onnx_path, engine_path, args.precision, args.max_batch):
        print("âœ… Conversion successful!")
        return 0
    else:
        print("âŒ Conversion failed!")
        return 1

if __name__ == "__main__":
    sys.exit(main())