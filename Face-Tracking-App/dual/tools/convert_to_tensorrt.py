#!/usr/bin/env python3
"""
ONNX ëª¨ë¸ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
FP16/FP32 ì •ë°€ë„ ë° ë™ì  ë°°ì¹˜ í¬ê¸° ì§€ì›
"""

import os
import sys
import argparse
from pathlib import Path
import tensorrt as trt
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dual_face_tracker.utils.logger import UnifiedLogger
from dual_face_tracker.utils.cuda_utils import get_gpu_memory_info

logger = UnifiedLogger()

# TensorRT ë¡œê±° ì„¤ì •
TRT_LOGGER = trt.Logger(trt.Logger.INFO)

class ONNXToTensorRT:
    """ONNXë¥¼ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        onnx_path: str,
        engine_path: str,
        precision: str = "fp32",
        max_batch_size: int = 8,
        min_batch_size: int = 1,
        opt_batch_size: int = 4,
        max_workspace_gb: float = 4.0
    ):
        """
        Args:
            onnx_path: ì…ë ¥ ONNX íŒŒì¼ ê²½ë¡œ
            engine_path: ì¶œë ¥ TensorRT ì—”ì§„ ê²½ë¡œ
            precision: ì •ë°€ë„ ("fp32", "fp16", "int8")
            max_batch_size: ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
            min_batch_size: ìµœì†Œ ë°°ì¹˜ í¬ê¸°
            opt_batch_size: ìµœì  ë°°ì¹˜ í¬ê¸°
            max_workspace_gb: ìµœëŒ€ ì‘ì—… ê³µê°„ (GB)
        """
        self.onnx_path = Path(onnx_path)
        self.engine_path = Path(engine_path)
        self.precision = precision.lower()
        self.max_batch_size = max_batch_size
        self.min_batch_size = min_batch_size
        self.opt_batch_size = opt_batch_size
        self.max_workspace_gb = max_workspace_gb
        
        if not self.onnx_path.exists():
            raise FileNotFoundError(f"ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.onnx_path}")
    
    def build_engine(self) -> bool:
        """TensorRT ì—”ì§„ì„ ë¹Œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.stage(f"ğŸ”§ TensorRT ì—”ì§„ ë¹Œë“œ ì‹œì‘: {self.onnx_path.name}")
            logger.info(f"  ì •ë°€ë„: {self.precision.upper()}")
            logger.info(f"  ë°°ì¹˜ í¬ê¸°: {self.min_batch_size}-{self.opt_batch_size}-{self.max_batch_size}")
            
            # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            gpu_info = get_gpu_memory_info()
            logger.info(f"  GPU ë©”ëª¨ë¦¬: {gpu_info['used_gb']:.1f}/{gpu_info['total_gb']:.1f} GB ì‚¬ìš© ì¤‘")
            
            # Builder ìƒì„±
            builder = trt.Builder(TRT_LOGGER)
            network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
            network = builder.create_network(network_flags)
            parser = trt.OnnxParser(network, TRT_LOGGER)
            
            # ONNX íŒŒì¼ íŒŒì‹±
            logger.info("ONNX íŒŒì¼ íŒŒì‹± ì¤‘...")
            with open(self.onnx_path, 'rb') as f:
                if not parser.parse(f.read()):
                    for error in range(parser.num_errors):
                        logger.error(f"ONNX íŒŒì‹± ì˜¤ë¥˜: {parser.get_error(error)}")
                    return False
            
            # Builder ì„¤ì •
            config = builder.create_builder_config()
            
            # ì‘ì—… ê³µê°„ ë©”ëª¨ë¦¬ ì„¤ì •
            config.set_memory_pool_limit(
                trt.MemoryPoolType.WORKSPACE, 
                int(self.max_workspace_gb * (1 << 30))  # GB to bytes
            )
            
            # ì •ë°€ë„ ì„¤ì •
            if self.precision == "fp16":
                if builder.platform_has_fast_fp16:
                    config.set_flag(trt.BuilderFlag.FP16)
                    logger.info("FP16 ì •ë°€ë„ í™œì„±í™”")
                else:
                    logger.warning("âš ï¸ FP16ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ, FP32 ì‚¬ìš©")
            elif self.precision == "int8":
                if builder.platform_has_fast_int8:
                    config.set_flag(trt.BuilderFlag.INT8)
                    logger.info("INT8 ì •ë°€ë„ í™œì„±í™” (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)")
                    # INT8 ìº˜ë¦¬ë¸Œë ˆì´ì…˜ êµ¬í˜„ í•„ìš”
                else:
                    logger.warning("âš ï¸ INT8ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ, FP32 ì‚¬ìš©")
            
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •
            profile = builder.create_optimization_profile()
            
            # ë„¤íŠ¸ì›Œí¬ ì…ë ¥ ì„¤ì •
            for i in range(network.num_inputs):
                input_tensor = network.get_input(i)
                input_name = input_tensor.name
                input_shape = input_tensor.shape
                
                logger.info(f"ì…ë ¥ í…ì„œ: {input_name} - {input_shape}")
                
                # ë™ì  shape ì„¤ì • (ë°°ì¹˜ ì°¨ì›ë§Œ ë™ì )
                min_shape = [self.min_batch_size] + list(input_shape[1:])
                opt_shape = [self.opt_batch_size] + list(input_shape[1:])
                max_shape = [self.max_batch_size] + list(input_shape[1:])
                
                profile.set_shape(input_name, min_shape, opt_shape, max_shape)
                logger.debug(f"  ë™ì  shape: {min_shape} / {opt_shape} / {max_shape}")
            
            config.add_optimization_profile(profile)
            
            # ì¶”ê°€ ìµœì í™” í”Œë˜ê·¸
            config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
            
            # ì—”ì§„ ë¹Œë“œ
            logger.info("TensorRT ì—”ì§„ ë¹Œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            engine_bytes = builder.build_serialized_network(network, config)
            
            if engine_bytes is None:
                logger.error("ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨")
                return False
            
            # ì—”ì§„ ì €ì¥
            with open(self.engine_path, 'wb') as f:
                f.write(engine_bytes)
            
            # ì—”ì§„ ì •ë³´
            size_mb = self.engine_path.stat().st_size / (1024 * 1024)
            logger.success(f"âœ… TensorRT ì—”ì§„ ìƒì„± ì™„ë£Œ: {self.engine_path}")
            logger.info(f"  ì—”ì§„ í¬ê¸°: {size_mb:.1f} MB")
            
            # ì—”ì§„ ê²€ì¦
            return self.verify_engine()
            
        except Exception as e:
            logger.error(f"ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            if self.engine_path.exists():
                self.engine_path.unlink()
            return False
    
    def verify_engine(self) -> bool:
        """ìƒì„±ëœ TensorRT ì—”ì§„ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        try:
            logger.info("ì—”ì§„ ê²€ì¦ ì¤‘...")
            
            # ëŸ°íƒ€ì„ ìƒì„±
            runtime = trt.Runtime(TRT_LOGGER)
            
            # ì—”ì§„ ë¡œë“œ
            with open(self.engine_path, 'rb') as f:
                engine_data = f.read()
            
            engine = runtime.deserialize_cuda_engine(engine_data)
            if engine is None:
                logger.error("ì—”ì§„ ì—­ì§ë ¬í™” ì‹¤íŒ¨")
                return False
            
            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = engine.create_execution_context()
            if context is None:
                logger.error("ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # ì…ì¶œë ¥ ì •ë³´ ì¶œë ¥
            logger.info("ì—”ì§„ ì…ì¶œë ¥ ì •ë³´:")
            for i in range(engine.num_io_tensors):
                name = engine.get_tensor_name(i)
                dtype = engine.get_tensor_dtype(name)
                shape = engine.get_tensor_shape(name)
                mode = engine.get_tensor_mode(name)
                
                io_type = "ì…ë ¥" if mode == trt.TensorIOMode.INPUT else "ì¶œë ¥"
                logger.info(f"  {io_type}: {name} - {shape} ({dtype})")
            
            # ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­
            logger.info(f"ë””ë°”ì´ìŠ¤ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰: {engine.device_memory_size / (1024**2):.1f} MB")
            
            logger.success("âœ… ì—”ì§„ ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì—”ì§„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def benchmark_engine(self, num_iterations: int = 100) -> dict:
        """ì—”ì§„ ì„±ëŠ¥ì„ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤."""
        try:
            import torch
            import time
            
            logger.stage("â±ï¸ ì—”ì§„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
            
            # ëŸ°íƒ€ì„ ë° ì—”ì§„ ë¡œë“œ
            runtime = trt.Runtime(TRT_LOGGER)
            with open(self.engine_path, 'rb') as f:
                engine = runtime.deserialize_cuda_engine(f.read())
            
            context = engine.create_execution_context()
            
            # ì…ë ¥ shape ì„¤ì •
            input_name = engine.get_tensor_name(0)
            input_shape = [self.opt_batch_size, 3, 640, 640]  # ê¸°ë³¸ shape
            context.set_input_shape(input_name, input_shape)
            
            # GPU ë©”ëª¨ë¦¬ í• ë‹¹
            bindings = []
            for i in range(engine.num_io_tensors):
                name = engine.get_tensor_name(i)
                shape = context.get_tensor_shape(name)
                dtype = trt.nptype(engine.get_tensor_dtype(name))
                size = np.prod(shape)
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                device_mem = torch.empty(size, dtype=torch.float32, device='cuda')
                bindings.append(device_mem.data_ptr())
            
            # ì›Œë°ì—…
            logger.info("ì›Œë°ì—… ì¤‘...")
            for _ in range(10):
                context.execute_v2(bindings)
            
            # ë²¤ì¹˜ë§ˆí¬
            torch.cuda.synchronize()
            start_time = time.time()
            
            for _ in range(num_iterations):
                context.execute_v2(bindings)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            # ê²°ê³¼ ê³„ì‚°
            total_time = end_time - start_time
            avg_time = total_time / num_iterations * 1000  # ms
            fps = num_iterations * self.opt_batch_size / total_time
            
            results = {
                "total_time": total_time,
                "avg_latency_ms": avg_time,
                "throughput_fps": fps,
                "batch_size": self.opt_batch_size,
                "iterations": num_iterations
            }
            
            logger.success("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
            logger.info(f"  í‰ê·  ì§€ì—°ì‹œê°„: {avg_time:.2f} ms")
            logger.info(f"  ì²˜ë¦¬ëŸ‰: {fps:.1f} FPS")
            logger.info(f"  ë°°ì¹˜ í¬ê¸°: {self.opt_batch_size}")
            
            return results
            
        except Exception as e:
            logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {}

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ONNXë¥¼ TensorRTë¡œ ë³€í™˜")
    parser.add_argument("--onnx", type=str, help="ì…ë ¥ ONNX íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--engine", type=str, help="ì¶œë ¥ TensorRT ì—”ì§„ ê²½ë¡œ")
    parser.add_argument("--precision", choices=["fp32", "fp16", "int8"], default="fp16", help="ì •ë°€ë„")
    parser.add_argument("--max-batch", type=int, default=8, help="ìµœëŒ€ ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--min-batch", type=int, default=1, help="ìµœì†Œ ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--opt-batch", type=int, default=4, help="ìµœì  ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--workspace", type=float, default=4.0, help="ì‘ì—… ê³µê°„ í¬ê¸° (GB)")
    parser.add_argument("--benchmark", action="store_true", help="ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    parser.add_argument("--all", action="store_true", help="ëª¨ë“  ONNX ëª¨ë¸ ë³€í™˜")
    
    args = parser.parse_args()
    
    logger.stage("ğŸš€ TensorRT ë³€í™˜ ì‹œì‘")
    
    models_dir = Path("models")
    engines_dir = Path("engines")
    engines_dir.mkdir(exist_ok=True)
    
    if args.all:
        # ëª¨ë“  ONNX ëª¨ë¸ ë³€í™˜
        success_count = 0
        total_count = 0
        
        for onnx_file in models_dir.glob("*.onnx"):
            total_count += 1
            engine_path = engines_dir / f"{onnx_file.stem}_{args.precision}.engine"
            
            converter = ONNXToTensorRT(
                onnx_file,
                engine_path,
                args.precision,
                args.max_batch,
                args.min_batch,
                args.opt_batch,
                args.workspace
            )
            
            if converter.build_engine():
                success_count += 1
                
                if args.benchmark:
                    converter.benchmark_engine()
        
        logger.stage(f"ğŸ“Š ë³€í™˜ ê²°ê³¼: {success_count}/{total_count} ì„±ê³µ")
        
        # ìƒì„±ëœ ì—”ì§„ íŒŒì¼ ëª©ë¡
        logger.info("\nğŸ“ TensorRT ì—”ì§„ íŒŒì¼:")
        for engine_file in engines_dir.glob("*.engine"):
            size_mb = engine_file.stat().st_size / (1024 * 1024)
            logger.info(f"  - {engine_file.name} ({size_mb:.1f} MB)")
            
    else:
        # ë‹¨ì¼ ëª¨ë¸ ë³€í™˜
        if not args.onnx:
            logger.error("--onnx ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")
            return 1
        
        onnx_path = Path(args.onnx)
        if not onnx_path.exists():
            logger.error(f"ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {onnx_path}")
            return 1
        
        engine_path = Path(args.engine) if args.engine else onnx_path.with_suffix('.engine')
        
        converter = ONNXToTensorRT(
            onnx_path,
            engine_path,
            args.precision,
            args.max_batch,
            args.min_batch,
            args.opt_batch,
            args.workspace
        )
        
        if not converter.build_engine():
            return 1
        
        if args.benchmark:
            converter.benchmark_engine()
    
    return 0

if __name__ == "__main__":
    sys.exit(main())