# Dual-Face GPU íŒŒì´í”„ë¼ì¸ ê¸°ìˆ  ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### **í•µì‹¬ ì„¤ê³„ ì² í•™**

**ì œë¡œì¹´í”¼ Full GPU íŒŒì´í”„ë¼ì¸**: CPUâ†”GPU ë©”ëª¨ë¦¬ ë³µì‚¬ ì™„ì „ ì œê±°
**ëª¨ë“ˆëŸ¬ ì„¤ê³„**: ê° ì»´í¬ë„ŒíŠ¸ ë…ë¦½ì  ê°œë°œ/í…ŒìŠ¤íŠ¸/êµì²´ ê°€ëŠ¥
**ì¥ì•  íƒ„ë ¥ì„±**: ë‹¨ì¼ ì‹¤íŒ¨ì  ì œê±° ë° ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ

### **ì „ì²´ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨**

```mermaid
graph TB
    A[Video Input Files] --> B[HybridConfigManager]
    B --> C[NvDecoder Ã— 4]
    
    C --> D[CUDA Stream 1]
    C --> E[CUDA Stream 2] 
    C --> F[CUDA Stream 3]
    C --> G[CUDA Stream 4]
    
    D --> H[TensorRT Inference Engine]
    E --> H
    F --> H
    G --> H
    
    H --> I[ConditionalReID System]
    I --> J[GPU Tile Composer]
    J --> K[NVENC Encoder Ã— 2]
    
    K --> L[Output Video Files]
    
    M[HardwareMonitor] --> N[StreamRecoveryManager]
    N --> O[TileCompositionErrorPolicy]
    O --> J
```

---

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì•„í‚¤í…ì²˜

### 1. **HybridConfigManager** (ì„¤ì • ê´€ë¦¬ ê³„ì¸µ)

```python
class HybridConfigManager:
    """
    3ë‹¨ê³„ ìš°ì„ ìˆœìœ„ ì„¤ì • ì‹œìŠ¤í…œ:
    1ìˆœìœ„: manual_config.yaml (ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì •)
    2ìˆœìœ„: auto_detected.yaml (í•˜ë“œì›¨ì–´ í”„ë¡œë¹™ ê²°ê³¼)  
    3ìˆœìœ„: fallback_config.yaml (ì•ˆì „í•œ ê¸°ë³¸ê°’)
    """
    
    def __init__(self):
        self.config_hierarchy = {
            'manual': ManualConfigLoader(),
            'probing': HardwareProber(), 
            'fallback': FallbackConfigLoader()
        }
        self.current_config = None
        self.validation_rules = ConfigValidationRules()
        
    def load_optimal_config(self) -> Dict[str, Any]:
        """ìµœì  ì„¤ì • ë¡œë“œ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        for config_type in ['manual', 'probing', 'fallback']:
            try:
                config = self.config_hierarchy[config_type].load()
                if self.validation_rules.validate(config):
                    self.current_config = config
                    logger.info(f"âœ… {config_type} ì„¤ì • ì ìš© ì„±ê³µ")
                    return config
            except ConfigurationError as e:
                logger.warning(f"âš ï¸ {config_type} ì„¤ì • ì‹¤íŒ¨: {e}")
                continue
        
        raise CriticalConfigurationError("ëª¨ë“  ì„¤ì • ì˜µì…˜ ì‹¤íŒ¨")
        
    def get_stream_allocation(self) -> StreamAllocationConfig:
        """ìŠ¤íŠ¸ë¦¼ë³„ ë¦¬ì†ŒìŠ¤ í• ë‹¹"""
        return StreamAllocationConfig(
            nvdec_sessions=self.current_config['nvdec']['max_sessions'],
            nvenc_sessions=self.current_config['nvenc']['max_sessions'],
            cuda_streams=self.current_config['cuda']['stream_count'],
            vram_per_stream=self.current_config['memory']['vram_per_stream_mb']
        )
```

**ì„¤ì • íŒŒì¼ êµ¬ì¡°**:

```yaml
# manual_config.yaml (ì‚¬ìš©ì ìš°ì„  ì„¤ì •)
hardware:
  gpu_id: 0
  nvdec_engines: 4      # RTX 5090 í™•ì •ê°’
  nvenc_engines: 2      # RTX 5090 í™•ì •ê°’
  
processing:
  batch_size_decode: 8
  batch_size_inference: 16
  cuda_streams: 4
  
memory:
  vram_limit_gb: 24     # 32GB ì¤‘ 75% ì‚¬ìš©
  vram_per_stream_mb: 6144  # 6GB per stream
  
models:
  face_detection: "yolo_face_fp16.trt"
  reid_model: "reid_lightweight_128d.trt"
```

### 2. **NvDecoder** (ë””ì½”ë”© ê³„ì¸µ)

```python
class NvDecoder:
    """
    PyNvCodec ê¸°ë°˜ í•˜ë“œì›¨ì–´ ë””ì½”ë”
    ì™„ì „ GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬ (ì œë¡œì¹´í”¼)
    """
    
    def __init__(self, video_path: str, gpu_id: int = 0):
        self.video_path = video_path
        self.gpu_id = gpu_id
        
        # PyNvCodec ì´ˆê¸°í™”
        self.decoder = nvc.PyDecodeHW(
            video_path, 
            nvc.PixelFormat.NV12, 
            gpu_id
        )
        
        # ìƒ‰ê³µê°„ ë³€í™˜ê¸° (NV12 â†’ RGB)
        self.converter = nvc.PySurfaceConverter(
            self.decoder.Width(),
            self.decoder.Height(),
            nvc.PixelFormat.NV12,
            nvc.PixelFormat.RGB,
            gpu_id
        )
        
        # GPU ë©”ëª¨ë¦¬ í’€
        self.surface_pool = SurfacePool(
            width=self.decoder.Width(),
            height=self.decoder.Height(),
            pool_size=16  # ì¶©ë¶„í•œ ë²„í¼ë§
        )
        
    def decode_batch(self, batch_size: int) -> List[torch.Tensor]:
        """ë°°ì¹˜ ë‹¨ìœ„ ë””ì½”ë”© (GPU ë©”ëª¨ë¦¬ ì§ì ‘ ì²˜ë¦¬)"""
        batch_frames = []
        
        for _ in range(batch_size):
            # NVDEC ë””ì½”ë”© (GPU ë©”ëª¨ë¦¬)
            nv12_surface = self.decoder.DecodeSurface()
            if not nv12_surface:
                break
                
            # ìƒ‰ê³µê°„ ë³€í™˜ (GPU ë©”ëª¨ë¦¬ ë‚´)
            rgb_surface = self.converter.Execute(nv12_surface)
            
            # DLPackì„ í†µí•œ ì œë¡œì¹´í”¼ í…ì„œ ë³€í™˜
            tensor = dlpack.fromDLPack(rgb_surface.GetDLPackTensor())
            batch_frames.append(tensor)
            
        return batch_frames
        
    def get_video_info(self) -> VideoInfo:
        """ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°"""
        return VideoInfo(
            width=self.decoder.Width(),
            height=self.decoder.Height(), 
            fps=self.decoder.Framerate(),
            total_frames=self.decoder.Numframes(),
            duration_seconds=self.decoder.Numframes() / self.decoder.Framerate()
        )
```

### 3. **TensorRTInferenceEngine** (ì¶”ë¡  ê³„ì¸µ)

```python
class TensorRTInferenceEngine:
    """
    TensorRT ê¸°ë°˜ ê³ ì„±ëŠ¥ ì¶”ë¡  ì—”ì§„
    FP16 ì •ë°€ë„, ë™ì  ë°°ì¹˜ ì²˜ë¦¬
    """
    
    def __init__(self, model_config: ModelConfig):
        self.model_config = model_config
        self.trt_logger = trt.Logger(trt.Logger.INFO)
        
        # TensorRT ì—”ì§„ ë¡œë“œ
        self.engines = {
            'face_detection': self._load_engine('yolo_face_fp16.trt'),
            'reid': self._load_engine('reid_lightweight_128d.trt')
        }
        
        # CUDA ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ (ìŠ¤íŠ¸ë¦¼ë³„)
        self.contexts = {
            name: engine.create_execution_context() 
            for name, engine in self.engines.items()
        }
        
        # GPU ë©”ëª¨ë¦¬ í• ë‹¹ (ì‚¬ì „ í• ë‹¹)
        self.gpu_buffers = self._allocate_gpu_buffers()
        
    def infer_face_detection(self, 
                           batch_frames: List[torch.Tensor],
                           cuda_stream: torch.cuda.Stream) -> List[Detection]:
        """ì–¼êµ´ ê²€ì¶œ ì¶”ë¡  (ë°°ì¹˜ ì²˜ë¦¬)"""
        
        with torch.cuda.stream(cuda_stream):
            # ë°°ì¹˜ í…ì„œ êµ¬ì„±
            batch_tensor = torch.stack(batch_frames)
            batch_tensor = batch_tensor.to(dtype=torch.float16)  # FP16
            
            # TensorRT ì¶”ë¡  ì‹¤í–‰
            context = self.contexts['face_detection']
            
            # ì…ë ¥ ë°”ì¸ë”©
            context.set_binding_shape(0, batch_tensor.shape)
            
            # GPU ë©”ëª¨ë¦¬ ë³µì‚¬ (ì œë¡œì¹´í”¼)
            input_buffer = self.gpu_buffers['face_detection']['input']
            input_buffer.copy_(batch_tensor.flatten())
            
            # ì¶”ë¡  ì‹¤í–‰
            context.execute_async_v2(
                bindings=self.gpu_buffers['face_detection']['bindings'],
                stream_handle=cuda_stream.cuda_stream
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            output_buffer = self.gpu_buffers['face_detection']['output']
            detections = self._parse_detection_output(output_buffer, batch_size=len(batch_frames))
            
        return detections
        
    def infer_reid(self, 
                   face_crops: List[torch.Tensor],
                   cuda_stream: torch.cuda.Stream) -> torch.Tensor:
        """ReID ì„ë² ë”© ì¶”ë¡ """
        
        with torch.cuda.stream(cuda_stream):
            batch_crops = torch.stack(face_crops)
            batch_crops = batch_crops.to(dtype=torch.float16)
            
            context = self.contexts['reid']
            context.set_binding_shape(0, batch_crops.shape)
            
            # ReID ì¶”ë¡ 
            input_buffer = self.gpu_buffers['reid']['input']
            input_buffer.copy_(batch_crops.flatten())
            
            context.execute_async_v2(
                bindings=self.gpu_buffers['reid']['bindings'],
                stream_handle=cuda_stream.cuda_stream
            )
            
            # 128-D ì„ë² ë”© ë²¡í„° ë°˜í™˜
            output_buffer = self.gpu_buffers['reid']['output']
            embeddings = output_buffer.view(len(face_crops), 128)
            
            # L2 ì •ê·œí™”
            embeddings = F.normalize(embeddings, p=2, dim=1)
            
        return embeddings
        
    def _load_engine(self, engine_path: str) -> trt.ICudaEngine:
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
            
        runtime = trt.Runtime(self.trt_logger)
        engine = runtime.deserialize_cuda_engine(engine_data)
        
        if not engine:
            raise TensorRTError(f"ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {engine_path}")
            
        return engine
        
    def _allocate_gpu_buffers(self) -> Dict[str, Dict[str, torch.Tensor]]:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í• ë‹¹ (ì„±ëŠ¥ ìµœì í™”)"""
        buffers = {}
        
        for model_name, engine in self.engines.items():
            model_buffers = {
                'bindings': [],
                'input': None,
                'output': None
            }
            
            for binding in engine:
                binding_idx = engine.get_binding_index(binding)
                shape = engine.get_binding_shape(binding)
                dtype = trt.nptype(engine.get_binding_dtype(binding))
                
                # GPU ë©”ëª¨ë¦¬ í• ë‹¹
                size = trt.volume(shape) * 8  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸° ê³ ë ¤
                buffer = torch.empty(size, dtype=torch.float16, device='cuda')
                
                if engine.binding_is_input(binding):
                    model_buffers['input'] = buffer
                else:
                    model_buffers['output'] = buffer
                    
                model_buffers['bindings'].append(buffer.data_ptr())
                
            buffers[model_name] = model_buffers
            
        return buffers
```

### 4. **ConditionalReID** (ì¡°ê±´ë¶€ ì¬ì‹ë³„ ì‹œìŠ¤í…œ)

```python
class ConditionalReID:
    """
    ByteTrack + ê²½ëŸ‰ ReID í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ
    ID ìŠ¤ì™‘ ê°ì§€ ì‹œì—ë§Œ ReID í™œì„±í™”
    """
    
    def __init__(self, reid_threshold: float = 0.6):
        self.reid_threshold = reid_threshold
        self.bytetrack = ByteTracker()
        self.reid_engine = None  # ì¡°ê±´ë¶€ ë¡œë“œ
        
        # ID ìŠ¤ì™‘ ê°ì§€ ë©”íŠ¸ë¦­
        self.tracking_history = defaultdict(list)
        self.id_swap_detector = IDSwapDetector()
        self.reid_activation_stats = {
            'total_frames': 0,
            'reid_active_frames': 0,
            'activation_ratio': 0.0
        }
        
    def track_faces(self, 
                   detections: List[Detection],
                   frame_idx: int,
                   cuda_stream: torch.cuda.Stream) -> List[TrackedFace]:
        """ì–¼êµ´ ì¶”ì  (ì¡°ê±´ë¶€ ReID í¬í•¨)"""
        
        # 1ë‹¨ê³„: ByteTrack ê¸°ë³¸ ì¶”ì 
        tracks = self.bytetrack.update(detections)
        
        # 2ë‹¨ê³„: ID ìŠ¤ì™‘ ê°ì§€
        swap_risk = self.id_swap_detector.assess_risk(
            tracks, 
            self.tracking_history,
            frame_idx
        )
        
        # 3ë‹¨ê³„: ì¡°ê±´ë¶€ ReID í™œì„±í™”
        if swap_risk.requires_reid:
            logger.debug(f"ğŸ”§ í”„ë ˆì„ {frame_idx}: ReID í™œì„±í™” (ìœ„í—˜ë„: {swap_risk.risk_score:.2f})")
            tracks = self._apply_reid_correction(tracks, detections, cuda_stream)
            self.reid_activation_stats['reid_active_frames'] += 1
            
        # 4ë‹¨ê³„: ì¶”ì  ì´ë ¥ ì—…ë°ì´íŠ¸
        self._update_tracking_history(tracks, frame_idx)
        self.reid_activation_stats['total_frames'] += 1
        
        # 5ë‹¨ê³„: ì¢Œìš° ë¶„ê¸° í• ë‹¹
        tracked_faces = self._assign_left_right(tracks)
        
        return tracked_faces
        
    def _apply_reid_correction(self, 
                             tracks: List[Track],
                             detections: List[Detection],
                             cuda_stream: torch.cuda.Stream) -> List[Track]:
        """ReID ê¸°ë°˜ ID ë³´ì •"""
        
        # ReID ì—”ì§„ ì§€ì—° ë¡œë”©
        if self.reid_engine is None:
            self.reid_engine = self._load_reid_engine()
            
        # ì–¼êµ´ í¬ë¡­ ì¶”ì¶œ
        face_crops = []
        for detection in detections:
            crop = self._extract_face_crop(detection)
            face_crops.append(crop)
            
        # ReID ì„ë² ë”© ê³„ì‚°
        embeddings = self.reid_engine.infer_reid(face_crops, cuda_stream)
        
        # ê¸°ì¡´ íŠ¸ë™ê³¼ ë§¤ì¹­
        corrected_tracks = []
        for i, track in enumerate(tracks):
            # ì´ì „ ì„ë² ë”©ê³¼ ë¹„êµ
            historical_embedding = self._get_historical_embedding(track.id)
            if historical_embedding is not None:
                similarity = F.cosine_similarity(
                    embeddings[i].unsqueeze(0),
                    historical_embedding.unsqueeze(0)
                ).item()
                
                if similarity < self.reid_threshold:
                    # ID ìŠ¤ì™‘ ê°ì§€ â†’ ë³´ì •
                    correct_id = self._find_matching_id(embeddings[i])
                    track.id = correct_id
                    logger.warning(f"ğŸ”„ ID ìŠ¤ì™‘ ë³´ì •: {track.id} (ìœ ì‚¬ë„: {similarity:.3f})")
                    
            corrected_tracks.append(track)
            
        return corrected_tracks
        
    def _assign_left_right(self, tracks: List[Track]) -> List[TrackedFace]:
        """ì¢Œìš° ë¶„ê¸° í• ë‹¹ (ê°œì„ ëœ ë¡œì§)"""
        tracked_faces = []
        
        # ìœ„ì¹˜ ê¸°ë°˜ ì´ˆê¸° í• ë‹¹
        for track in tracks:
            center_x = (track.bbox.x1 + track.bbox.x2) / 2
            frame_center = track.frame_width / 2
            
            # EMA ê¸°ë°˜ ìœ„ì¹˜ ì•ˆì •í™”
            if track.id not in self.position_ema:
                self.position_ema[track.id] = center_x
            else:
                alpha = 0.3  # EMA ê°€ì¤‘ì¹˜
                self.position_ema[track.id] = (
                    alpha * center_x + 
                    (1 - alpha) * self.position_ema[track.id]
                )
                
            # ì¢Œìš° í• ë‹¹
            stable_position = self.position_ema[track.id]
            if stable_position < frame_center:
                side = 'left'
            else:
                side = 'right'
                
            tracked_face = TrackedFace(
                track_id=track.id,
                bbox=track.bbox,
                confidence=track.confidence,
                side=side,
                position_stability=self._calculate_position_stability(track.id)
            )
            
            tracked_faces.append(tracked_face)
            
        return tracked_faces
        
    def get_activation_stats(self) -> Dict[str, float]:
        """ReID í™œì„±í™” í†µê³„"""
        if self.reid_activation_stats['total_frames'] > 0:
            activation_ratio = (
                self.reid_activation_stats['reid_active_frames'] / 
                self.reid_activation_stats['total_frames']
            )
        else:
            activation_ratio = 0.0
            
        return {
            'activation_ratio': activation_ratio,
            'total_frames': self.reid_activation_stats['total_frames'],
            'reid_frames': self.reid_activation_stats['reid_active_frames']
        }
```

### 5. **GPUTileComposer** (GPU í•©ì„± ê³„ì¸µ)

```python
class GPUTileComposer:
    """
    ì™„ì „ GPU ê¸°ë°˜ íƒ€ì¼ í•©ì„± ì‹œìŠ¤í…œ
    CUDA ì»¤ë„ ì§ì ‘ êµ¬í˜„ìœ¼ë¡œ ìµœëŒ€ ì„±ëŠ¥
    """
    
    def __init__(self, output_width: int = 1920, output_height: int = 1080):
        self.output_width = output_width
        self.output_height = output_height
        self.tile_width = output_width // 2  # 960px
        self.tile_height = output_height     # 1080px
        
        # CUDA ì»¤ë„ ì»´íŒŒì¼ (JIT)
        self.cuda_kernels = self._compile_cuda_kernels()
        
        # GPU ë©”ëª¨ë¦¬ í’€ (ì‚¬ì „ í• ë‹¹)
        self.composition_buffers = self._allocate_composition_buffers()
        
        # ì—ëŸ¬ ì²˜ë¦¬ ì •ì±…
        self.error_policy = TileCompositionErrorPolicy()
        
    def compose_frames(self, 
                      left_frame: torch.Tensor,
                      right_frame: torch.Tensor,
                      cuda_stream: torch.cuda.Stream) -> torch.Tensor:
        """í”„ë ˆì„ íƒ€ì¼ í•©ì„± (ì™„ì „ GPU ì²˜ë¦¬)"""
        
        try:
            with torch.cuda.stream(cuda_stream):
                # 1ë‹¨ê³„: í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ (GPU)
                left_resized = self._gpu_resize(left_frame, 
                                              (self.tile_width, self.tile_height))
                right_resized = self._gpu_resize(right_frame,
                                               (self.tile_width, self.tile_height))
                
                # 2ë‹¨ê³„: íƒ€ì¼ í•©ì„± (CUDA ì»¤ë„)
                composed_frame = self._cuda_tile_compose(
                    left_resized, right_resized, cuda_stream
                )
                
                # 3ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
                if not self._validate_composition(composed_frame):
                    raise CompositionError("í•©ì„± ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨")
                    
                return composed_frame
                
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬ ì •ì±… ì ìš©
            return self.error_policy.handle_composition_error(
                left_frame, right_frame, str(e)
            )
            
    def _gpu_resize(self, 
                   frame: torch.Tensor, 
                   target_size: Tuple[int, int]) -> torch.Tensor:
        """GPU ê¸°ë°˜ í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ"""
        
        # ë°©ë²• 1: OpenCV CUDA (ê¶Œì¥)
        if hasattr(cv2, 'cuda'):
            gpu_mat = cv2.cuda_GpuMat()
            gpu_mat.upload(frame.cpu().numpy())
            resized = cv2.cuda.resize(gpu_mat, target_size)
            result = torch.from_numpy(resized.download()).cuda()
            return result
            
        # ë°©ë²• 2: PyTorch ë‚´ì¥ (ë°±ì—…)
        else:
            return F.interpolate(
                frame.unsqueeze(0), 
                size=target_size, 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
            
    def _cuda_tile_compose(self, 
                          left_frame: torch.Tensor,
                          right_frame: torch.Tensor,
                          cuda_stream: torch.cuda.Stream) -> torch.Tensor:
        """CUDA ì»¤ë„ ê¸°ë°˜ íƒ€ì¼ í•©ì„±"""
        
        # ì¶œë ¥ ë²„í¼ í• ë‹¹
        composed = self.composition_buffers['output']
        composed.zero_()
        
        # CUDA ì»¤ë„ ì‹¤í–‰
        threads_per_block = (16, 16)
        blocks_per_grid_x = (self.tile_width + threads_per_block[0] - 1) // threads_per_block[0]
        blocks_per_grid_y = (self.tile_height + threads_per_block[1] - 1) // threads_per_block[1]
        blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)
        
        self.cuda_kernels.tile_compose_kernel[blocks_per_grid, threads_per_block, cuda_stream.cuda_stream](
            left_frame.data_ptr(),
            right_frame.data_ptr(),
            composed.data_ptr(),
            self.tile_width,
            self.tile_height,
            self.output_width
        )
        
        return composed.clone()  # ê²°ê³¼ ë³µì‚¬ë³¸ ë°˜í™˜
        
    def _compile_cuda_kernels(self) -> Any:
        """CUDA ì»¤ë„ JIT ì»´íŒŒì¼"""
        
        kernel_source = """
        __global__ void tile_compose_kernel(
            float* left_data,
            float* right_data, 
            float* output_data,
            int tile_width,
            int tile_height,
            int output_width
        ) {
            int x = blockIdx.x * blockDim.x + threadIdx.x;
            int y = blockIdx.y * blockDim.y + threadIdx.y;
            
            if (x < tile_width && y < tile_height) {
                // ì¢Œì¸¡ íƒ€ì¼ ë³µì‚¬
                int left_idx = y * tile_width + x;
                int output_left_idx = y * output_width + x;
                output_data[output_left_idx] = left_data[left_idx];
                
                // ìš°ì¸¡ íƒ€ì¼ ë³µì‚¬
                int right_idx = y * tile_width + x; 
                int output_right_idx = y * output_width + (x + tile_width);
                output_data[output_right_idx] = right_data[right_idx];
            }
        }
        """
        
        # PyCUDA ë˜ëŠ” CuPyë¥¼ í†µí•œ ì»¤ë„ ì»´íŒŒì¼
        try:
            import cupy as cp
            return cp.RawKernel(kernel_source, 'tile_compose_kernel')
        except ImportError:
            # ë°±ì—…: PyTorch ê¸°ë°˜ êµ¬í˜„
            return self._pytorch_fallback_compose
```

### 6. **NvEncoder** (ì¸ì½”ë”© ê³„ì¸µ)

```python
class NvEncoder:
    """
    NVENC í•˜ë“œì›¨ì–´ ì¸ì½”ë”
    H.264 ìµœì  ì„¤ì •, ì„¸ì…˜ ì œí•œ ê´€ë¦¬
    """
    
    def __init__(self, 
                 output_path: str,
                 width: int = 1920, 
                 height: int = 1080,
                 fps: float = 30.0,
                 encoder_id: int = 0):
        
        self.output_path = output_path
        self.width = width
        self.height = height
        self.fps = fps
        self.encoder_id = encoder_id
        
        # PyNvCodec ì¸ì½”ë” ì„¤ì •
        self.encoder_config = self._create_encoder_config()
        self.encoder = nvc.PyEncoderHW(
            self.encoder_config,
            encoder_id
        )
        
        # ì¶œë ¥ íŒŒì¼ ì‘ì„±ê¸°
        self.file_writer = nvc.PyFileWriter(output_path)
        
        # ì¸ì½”ë”© í†µê³„
        self.encoding_stats = EncodingStats()
        
    def encode_frame(self, 
                    frame_tensor: torch.Tensor,
                    cuda_stream: torch.cuda.Stream) -> bool:
        """í”„ë ˆì„ ì¸ì½”ë”© (GPU ë©”ëª¨ë¦¬ ì§ì ‘ ì²˜ë¦¬)"""
        
        try:
            with torch.cuda.stream(cuda_stream):
                # DLPackì„ í†µí•œ ì œë¡œì¹´í”¼ Surface ë³€í™˜
                surface = self._tensor_to_surface(frame_tensor)
                
                # NVENC ì¸ì½”ë”©
                encoded_packet = self.encoder.EncodeSurface(surface)
                
                if encoded_packet:
                    # íŒŒì¼ ì¶œë ¥
                    self.file_writer.WritePacket(encoded_packet)
                    self.encoding_stats.increment_frames()
                    return True
                else:
                    self.encoding_stats.increment_failures()
                    return False
                    
        except Exception as e:
            logger.error(f"ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            self.encoding_stats.increment_failures()
            return False
            
    def finalize(self):
        """ì¸ì½”ë”© ì™„ë£Œ ì²˜ë¦¬"""
        
        # ë‚¨ì€ í”„ë ˆì„ flush
        while True:
            packet = self.encoder.Flush()
            if not packet:
                break
            self.file_writer.WritePacket(packet)
            
        # ë¦¬ì†ŒìŠ¤ í•´ì œ
        self.file_writer.Close()
        
        # í†µê³„ ì¶œë ¥
        stats = self.encoding_stats.get_summary()
        logger.info(f"ì¸ì½”ë”© ì™„ë£Œ - ì„±ê³µ: {stats['success_frames']}, ì‹¤íŒ¨: {stats['failed_frames']}")
        
    def _create_encoder_config(self) -> Dict[str, Any]:
        """NVENC ì¸ì½”ë” ì„¤ì •"""
        return {
            'width': self.width,
            'height': self.height,
            'framerate': self.fps,
            'codec': nvc.PixelFormat.H264,
            
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            'preset': 'fast',           # ë¹ ë¥¸ ì¸ì½”ë”©
            'profile': 'high',          # ê³ í’ˆì§ˆ í”„ë¡œíŒŒì¼
            'level': '4.1',             # í˜¸í™˜ì„±
            
            # ë¹„íŠ¸ë ˆì´íŠ¸ ì œì–´
            'bitrate': '5M',            # 5Mbps ê¸°ë³¸ê°’
            'rate_control': 'cbr',      # ì¼ì • ë¹„íŠ¸ë ˆì´íŠ¸
            
            # ì €ì§€ì—° ì„¤ì • (ì‹¤ì‹œê°„ ì²˜ë¦¬)
            'low_latency': True,
            'b_frames': 0,              # B-í”„ë ˆì„ ë¹„í™œì„±í™”
            'gop_size': 30,             # GOP í¬ê¸°
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            'surfaces': 16,             # ì¶©ë¶„í•œ ì„œí”¼ìŠ¤ ë²„í¼
        }
        
    def _tensor_to_surface(self, tensor: torch.Tensor) -> Any:
        """í…ì„œ â†’ PyNvCodec Surface ë³€í™˜"""
        
        # RGB í…ì„œë¥¼ NV12ë¡œ ë³€í™˜ (GPU ë‚´ì—ì„œ)
        rgb_surface = nvc.PySurface.from_dlpack(tensor.__dlpack__())
        
        # ìƒ‰ê³µê°„ ë³€í™˜ê¸°
        converter = nvc.PySurfaceConverter(
            self.width, self.height,
            nvc.PixelFormat.RGB,
            nvc.PixelFormat.NV12,
            self.encoder_id
        )
        
        nv12_surface = converter.Execute(rgb_surface)
        return nv12_surface
```

---

## ğŸ”„ ë°ì´í„° í”Œë¡œìš° ë° ë©”ëª¨ë¦¬ ê´€ë¦¬

### **ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜**

```python
class GPUMemoryManager:
    """
    í†µí•© GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
    VRAM íš¨ìœ¨ì„± ê·¹ëŒ€í™”, OOM ë°©ì§€
    """
    
    def __init__(self, total_vram_gb: int = 32):
        self.total_vram = total_vram_gb * 1024**3  # bytes
        self.safe_limit = int(self.total_vram * 0.75)  # 75% ì‚¬ìš© ì œí•œ
        
        # ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬
        self.memory_pools = {
            'decode_surfaces': SurfacePool(pool_size=64),
            'inference_tensors': TensorPool(pool_size=128),
            'composition_buffers': CompositionPool(pool_size=32),
            'encode_surfaces': SurfacePool(pool_size=64)
        }
        
        # ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        self.usage_tracker = MemoryUsageTracker()
        
    def allocate_stream_memory(self, stream_id: int) -> StreamMemoryContext:
        """ìŠ¤íŠ¸ë¦¼ë³„ ë©”ëª¨ë¦¬ í• ë‹¹"""
        
        estimated_usage = self._estimate_stream_memory_usage()
        
        if self.get_available_memory() < estimated_usage:
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì •ë¦¬
            self._cleanup_unused_memory()
            
            if self.get_available_memory() < estimated_usage:
                raise GPUMemoryError(f"ìŠ¤íŠ¸ë¦¼ {stream_id} ë©”ëª¨ë¦¬ ë¶€ì¡±")
                
        # ìŠ¤íŠ¸ë¦¼ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = StreamMemoryContext(
            stream_id=stream_id,
            decode_pool=self.memory_pools['decode_surfaces'].get_slice(),
            inference_pool=self.memory_pools['inference_tensors'].get_slice(),
            composition_pool=self.memory_pools['composition_buffers'].get_slice(),
            encode_pool=self.memory_pools['encode_surfaces'].get_slice()
        )
        
        return context
        
    def get_memory_status(self) -> MemoryStatus:
        """ë©”ëª¨ë¦¬ ì‚¬ìš© í˜„í™©"""
        allocated = torch.cuda.memory_allocated()
        reserved = torch.cuda.memory_reserved()
        
        return MemoryStatus(
            allocated_gb=allocated / 1024**3,
            reserved_gb=reserved / 1024**3,
            utilization_ratio=allocated / self.total_vram,
            safe_limit_reached=allocated > self.safe_limit
        )
```

### **CUDA Stream ê´€ë¦¬**

```python
class CUDAStreamManager:
    """
    4ê°œ CUDA Stream ë³‘ë ¬ ì²˜ë¦¬ ê´€ë¦¬
    ìŠ¤íŠ¸ë¦¼ ë™ê¸°í™” ë° ë¦¬ì†ŒìŠ¤ ê²©ë¦¬
    """
    
    def __init__(self, num_streams: int = 4):
        self.num_streams = num_streams
        self.streams = [torch.cuda.Stream() for _ in range(num_streams)]
        
        # ìŠ¤íŠ¸ë¦¼ë³„ ì›Œì»¤
        self.stream_workers = {
            i: StreamWorker(stream=self.streams[i], stream_id=i)
            for i in range(num_streams)
        }
        
        # ë™ê¸°í™” ê´€ë¦¬
        self.sync_manager = StreamSyncManager(self.streams)
        
    def process_videos_parallel(self, video_paths: List[str]) -> List[str]:
        """ë¹„ë””ì˜¤ ë³‘ë ¬ ì²˜ë¦¬"""
        
        if len(video_paths) > self.num_streams:
            raise ValueError(f"ë¹„ë””ì˜¤ ìˆ˜({len(video_paths)})ê°€ ìŠ¤íŠ¸ë¦¼ ìˆ˜({self.num_streams}) ì´ˆê³¼")
            
        # ê° ìŠ¤íŠ¸ë¦¼ì— ë¹„ë””ì˜¤ í• ë‹¹
        futures = []
        for i, video_path in enumerate(video_paths):
            worker = self.stream_workers[i]
            future = worker.process_video_async(video_path)
            futures.append(future)
            
        # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ë™ê¸°í™” ëŒ€ê¸°
        self.sync_manager.wait_all_complete()
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for future in futures:
            result = future.get()  # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            results.append(result)
            
        return results
        
class StreamWorker:
    """ê°œë³„ CUDA Stream ì›Œì»¤"""
    
    def __init__(self, stream: torch.cuda.Stream, stream_id: int):
        self.stream = stream
        self.stream_id = stream_id
        
        # ìŠ¤íŠ¸ë¦¼ë³„ ë…ë¦½ ì»´í¬ë„ŒíŠ¸
        self.decoder = None
        self.inference_engine = TensorRTInferenceEngine(stream_id=stream_id)
        self.conditional_reid = ConditionalReID()
        self.tile_composer = GPUTileComposer()
        self.encoder = None
        
    def process_video_async(self, video_path: str) -> Future:
        """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        
        def _process():
            with torch.cuda.stream(self.stream):
                try:
                    # ì´ˆê¸°í™”
                    self.decoder = NvDecoder(video_path, gpu_id=0)
                    output_path = self._generate_output_path(video_path)
                    self.encoder = NvEncoder(output_path)
                    
                    # ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
                    frame_idx = 0
                    while True:
                        # ë°°ì¹˜ ë””ì½”ë”©
                        batch_frames = self.decoder.decode_batch(batch_size=8)
                        if not batch_frames:
                            break
                            
                        # ì¶”ë¡ 
                        detections = self.inference_engine.infer_face_detection(
                            batch_frames, self.stream
                        )
                        
                        # ì¶”ì 
                        tracked_faces = self.conditional_reid.track_faces(
                            detections, frame_idx, self.stream
                        )
                        
                        # í•©ì„± ë° ì¸ì½”ë”©
                        for frame_data in batch_frames:
                            left_frame, right_frame = self._split_faces(
                                frame_data, tracked_faces
                            )
                            
                            composed = self.tile_composer.compose_frames(
                                left_frame, right_frame, self.stream
                            )
                            
                            self.encoder.encode_frame(composed, self.stream)
                            
                        frame_idx += len(batch_frames)
                        
                    # ì™„ë£Œ ì²˜ë¦¬
                    self.encoder.finalize()
                    return output_path
                    
                except Exception as e:
                    logger.error(f"ìŠ¤íŠ¸ë¦¼ {self.stream_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    raise
                    
        # ThreadPoolExecutorë¡œ ë¹„ë™ê¸° ì‹¤í–‰
        executor = ThreadPoolExecutor(max_workers=1)
        future = executor.submit(_process)
        return future
```

---

ì´ ì•„í‚¤í…ì²˜ ê°€ì´ë“œëŠ” ê° ì»´í¬ë„ŒíŠ¸ì˜ ìƒì„¸ êµ¬í˜„ ë°©ì•ˆê³¼ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  ì»´í¬ë„ŒíŠ¸ëŠ” ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ/í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•˜ë©°, ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬ì™€ ì—ëŸ¬ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.