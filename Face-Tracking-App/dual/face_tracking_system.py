#!/usr/bin/env python3
"""
Phase 5: Dual-Face Tracking System
ì™„ì „í•œ ì–¼êµ´ ê²€ì¶œ, ì¶”ì , í¬ë¡­ íŒŒì´í”„ë¼ì¸

ê¸°ëŠ¥:
- 2ëª…ì˜ ì–¼êµ´ ê²€ì¶œ ë° ID í• ë‹¹
- ì‹¤ì‹œê°„ ì–¼êµ´ ì¶”ì  (OpenCV CSRT)
- ì–¼êµ´ ì¤‘ì‹¬ ê¸°ì¤€ í¬ë¡­ (2.5ë°° ë§ˆì§„)
- 1920x1080 ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ì¶œë ¥

Author: Dual-Face System v5.0
Date: 2025.08.16
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Tuple, Optional, List, Dict, Any
import cv2
import numpy as np
from PIL import Image
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root.parent / "src"))

# GPU ì„¤ì • ë° ë”¥ëŸ¬ë‹ ëª¨ë¸
import torch
import torch.nn.functional as F
from torchvision import transforms
if torch.cuda.is_available():
    torch.cuda.set_device(0)
    print(f"ğŸ–¥ï¸ GPU ì„¤ì •: {torch.cuda.get_device_name(0)}")

# í”„ë¡œì íŠ¸ ëª¨ë¸ import (conditional)
try:
    from src.face_tracker.core.models import ModelManager
    from src.face_tracker.core.embeddings import SmartEmbeddingManager
    from src.face_tracker.utils.similarity import (
        find_matching_id_with_best_fallback_enhanced,
        calculate_face_similarity
    )
    MODEL_MANAGER_AVAILABLE = True
    print("âœ… ModelManager + SmartEmbeddingManager + ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ ë©”ì¸ í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    MODEL_MANAGER_AVAILABLE = False


class FaceDetection:
    """ì–¼êµ´ ê²€ì¶œ ê²°ê³¼ (ì„ë² ë”© ì§€ì›)"""
    def __init__(self, bbox: Tuple[int, int, int, int], confidence: float):
        self.x1, self.y1, self.x2, self.y2 = bbox
        self.confidence = confidence
        self.width = self.x2 - self.x1
        self.height = self.y2 - self.y1
        self.center_x = (self.x1 + self.x2) / 2
        self.center_y = (self.y1 + self.y2) / 2
        self.area = self.width * self.height
        
        # ì–¼êµ´ ì¸ì‹ìš© ì„ë² ë”© (FaceNet)
        self.embedding = None  # torch.Tensor
        self.p1_score = 0.0    # Person1ê³¼ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        self.p2_score = 0.0    # Person2ì™€ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        
    @property
    def bbox(self) -> Tuple[int, int, int, int]:
        return (self.x1, self.y1, self.x2, self.y2)
    
    @property
    def center(self) -> Tuple[float, float]:
        return (self.center_x, self.center_y)


class FaceTracker:
    """ê²€ì¶œ ê¸°ë°˜ ì–¼êµ´ íŠ¸ë˜ì»¤ (OpenCV 4.13 í˜¸í™˜)"""
    
    def __init__(self, person_id: str, smoothing_alpha: float = 0.3):
        self.person_id = person_id
        self.smoothing_alpha = smoothing_alpha
        
        # ê²€ì¶œ ê¸°ë°˜ ìƒíƒœ
        self.face_detection: Optional[FaceDetection] = None
        self.face_center: Optional[Tuple[float, float]] = None
        self.smooth_center: Optional[Tuple[float, float]] = None
        self.last_good_bbox: Optional[Tuple[int, int, int, int]] = None
        self.tracking_confidence = 0.0
        
        # ê²€ì¶œ íˆìŠ¤í† ë¦¬ (ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì )
        self.detection_history: List[Tuple[float, float]] = []  # (center_x, center_y) íˆìŠ¤í† ë¦¬
        self.max_history = 30  # ìµœê·¼ 30ê°œ ìœ„ì¹˜ ê¸°ì–µ (10 â†’ 30ìœ¼ë¡œ ì¦ê°€)
        self.position_threshold = 50  # ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ì¸ì‹í•  ê±°ë¦¬ ì„ê³„ê°’ (ë” ì—„ê²©í•˜ê²Œ)
        
        # ê³ ì • í¬ë¡­ í¬ê¸° ì„¤ì • (ë“¤ì­‰ë‚ ì­‰ ë¬¸ì œ í•´ê²°)
        self.fixed_crop_size = None  # ë™ì ìœ¼ë¡œ ê³„ì‚°ë¨
        self.crop_size_history = []  # í¬ë¡­ í¬ê¸° íˆìŠ¤í† ë¦¬ (ìŠ¤ë¬´ë”©ìš©)
        self.max_crop_size_history = 10
        self.crop_size_smoothing_alpha = 0.2  # í¬ë¡­ í¬ê¸° ìŠ¤ë¬´ë”© ê³„ìˆ˜
        
        # í†µê³„
        self.detection_success_count = 0
        self.detection_fail_count = 0
        self.total_frames = 0
        
        # ë™ì  ì¬í• ë‹¹ ì‹œìŠ¤í…œ (ì‹ ë¢°ë„ ê¸°ë°˜)
        self.confidence_history = []  # ìµœê·¼ ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬
        self.max_confidence_history = 20
        self.low_confidence_threshold = 0.6  # ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’
        self.reassignment_trigger_count = 10  # ì¬í• ë‹¹ ê²€í† ë¥¼ ìœ„í•œ ë‚®ì€ ì‹ ë¢°ë„ ì—°ì† íšŸìˆ˜
        
    def update_detection(self, detection: Optional[FaceDetection]) -> bool:
        """ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸ (íŠ¸ë˜ì»¤ ì—†ìŒ)"""
        self.total_frames += 1
        
        if detection is None:
            self.detection_fail_count += 1
            return False
            
        # ê²€ì¶œ ì„±ê³µ
        self.face_detection = detection
        self.face_center = detection.center
        self.last_good_bbox = detection.bbox
        self.tracking_confidence = detection.confidence
        
        # ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ì¬í• ë‹¹ ê²€í† ìš©)
        self.confidence_history.append(detection.confidence)
        if len(self.confidence_history) > self.max_confidence_history:
            self.confidence_history.pop(0)
        
        # íˆìŠ¤í† ë¦¬ì— ìœ„ì¹˜ ì¶”ê°€
        self.detection_history.append(self.face_center)
        if len(self.detection_history) > self.max_history:
            self.detection_history.pop(0)
        
        # ìŠ¤ë¬´ë”© ì ìš©
        if self.smooth_center is None:
            self.smooth_center = self.face_center
        else:
            self.smooth_center = self._apply_smoothing(self.face_center)
        
        self.detection_success_count += 1
        return True
    
    def get_distance_to_detection(self, detection: FaceDetection) -> float:
        """ê²€ì¶œëœ ì–¼êµ´ê³¼ì˜ ê±°ë¦¬ ê³„ì‚° (ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­ìš©, íˆìŠ¤í† ë¦¬ ê°•í™”)"""
        if not self.detection_history:
            return float('inf')
        
        # ìµœê·¼ ìœ„ì¹˜ë“¤ê³¼ì˜ ê°€ì¤‘ í‰ê·  ê±°ë¦¬ ê³„ì‚° (ë” ë§ì€ íˆìŠ¤í† ë¦¬ ì‚¬ìš©)
        det_center = detection.center
        distances = []
        weights = []
        
        # ìµœê·¼ 5ê°œ ìœ„ì¹˜ ì‚¬ìš© (3 â†’ 5ë¡œ ì¦ê°€), ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        recent_history = self.detection_history[-5:]
        for i, hist_center in enumerate(recent_history):
            dist = ((det_center[0] - hist_center[0]) ** 2 + 
                   (det_center[1] - hist_center[1]) ** 2) ** 0.5
            distances.append(dist)
            # ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜: 0.1, 0.15, 0.2, 0.25, 0.3
            weight = 0.1 + (i * 0.05)
            weights.append(weight)
        
        if distances:
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weighted_sum = sum(d * w for d, w in zip(distances, weights))
            total_weight = sum(weights)
            return weighted_sum / total_weight
        else:
            return float('inf')
    
    def _get_fixed_crop_size(self, frame: np.ndarray, detected_face_size: Optional[int] = None) -> int:
        """í”„ë ˆì„ í¬ê¸° ê¸°ë°˜ ê³ ì • í¬ë¡­ í¬ê¸° ê³„ì‚° (ë“¤ì­‰ë‚ ì­‰ ë°©ì§€)"""
        h, w = frame.shape[:2]
        
        # ê¸°ë³¸ ê³ ì • í¬ê¸°: í”„ë ˆì„ ë†’ì´ì˜ 40%
        base_crop_size = int(h * 0.4)
        
        # ìµœì†Œ/ìµœëŒ€ í¬ê¸° ì œí•œ
        min_crop_size = int(h * 0.25)  # ìµœì†Œ 25%
        max_crop_size = int(h * 0.6)   # ìµœëŒ€ 60%
        
        if detected_face_size is not None:
            # ì–¼êµ´ì´ ê²€ì¶œëœ ê²½ìš°: ì–¼êµ´ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì ì‘
            adapted_size = max(detected_face_size * 3, base_crop_size)
            adapted_size = max(min_crop_size, min(max_crop_size, adapted_size))
            
            # í¬ë¡­ í¬ê¸° íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.crop_size_history.append(adapted_size)
            if len(self.crop_size_history) > self.max_crop_size_history:
                self.crop_size_history.pop(0)
            
            # í¬ë¡­ í¬ê¸° ìŠ¤ë¬´ë”©
            if self.fixed_crop_size is None:
                self.fixed_crop_size = adapted_size
            else:
                alpha = self.crop_size_smoothing_alpha
                self.fixed_crop_size = int(alpha * adapted_size + (1 - alpha) * self.fixed_crop_size)
        else:
            # ì–¼êµ´ì´ ì—†ëŠ” ê²½ìš°: ê¸°ì¡´ í¬ê¸° ìœ ì§€ ë˜ëŠ” ê¸°ë³¸ê°’
            if self.fixed_crop_size is None:
                self.fixed_crop_size = base_crop_size
        
        # ë²”ìœ„ ì œí•œ
        self.fixed_crop_size = max(min_crop_size, min(max_crop_size, self.fixed_crop_size))
        
        return self.fixed_crop_size
    
    def get_crop_region(self, frame: np.ndarray, margin_factor: float = 2.5) -> np.ndarray:
        """ê³ ì • í¬ê¸° í¬ë¡­ ì˜ì—­ ë°˜í™˜ (ë“¤ì­‰ë‚ ì­‰ ë¬¸ì œ í•´ê²°)"""
        h, w = frame.shape[:2]
        
        if self.smooth_center is None or self.last_good_bbox is None:
            # ì–¼êµ´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì˜ì—­ ë°˜í™˜ (ì¢Œìš° ë¶„í• )
            if self.person_id == "Person1":
                crop = frame[0:h, 0:w//2]
            else:
                crop = frame[0:h, w//2:w]
                
            # 960x1080ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            return cv2.resize(crop, (960, 1080))
        
        # ì–¼êµ´ í¬ê¸° ê³„ì‚° (ê³ ì • í¬ê¸° ê³„ì‚°ìš©)
        x1, y1, x2, y2 = self.last_good_bbox
        face_width = x2 - x1
        face_height = y2 - y1
        face_size = max(face_width, face_height)
        
        # ê³ ì • í¬ê¸° í¬ë¡­ í¬ê¸° ê³„ì‚° (ë“¤ì­‰ë‚ ì­‰ ë°©ì§€)
        crop_size = self._get_fixed_crop_size(frame, face_size)
        
        # í¬ë¡­ ì¤‘ì‹¬ì  (ìŠ¤ë¬´ë”©ëœ ì–¼êµ´ ì¤‘ì‹¬)
        center_x, center_y = self.smooth_center
        
        # í¬ë¡­ ì˜ì—­ ê³„ì‚°
        crop_x1 = max(0, int(center_x - crop_size // 2))
        crop_y1 = max(0, int(center_y - crop_size // 2))
        crop_x2 = min(w, crop_x1 + crop_size)
        crop_y2 = min(h, crop_y1 + crop_size)
        
        # ê²½ê³„ ì¡°ì • (ì •ì‚¬ê°í˜• ìœ ì§€)
        if crop_x2 - crop_x1 < crop_size:
            crop_x1 = max(0, crop_x2 - crop_size)
        if crop_y2 - crop_y1 < crop_size:
            crop_y1 = max(0, crop_y2 - crop_size)
        
        # í¬ë¡­ ì¶”ì¶œ
        cropped = frame[crop_y1:crop_y2, crop_x1:crop_x2]
        
        # ì •ì‚¬ê°í˜• íŒ¨ë”© (í•„ìš”ì‹œ) - ê³ ì • í¬ê¸° ë³´ì¥
        ch, cw = cropped.shape[:2]
        if ch != cw:
            max_size = max(ch, cw)
            pad_h = (max_size - ch) // 2
            pad_w = (max_size - cw) // 2
            cropped = cv2.copyMakeBorder(
                cropped, pad_h, max_size - ch - pad_h, 
                pad_w, max_size - cw - pad_w, 
                cv2.BORDER_CONSTANT, value=[0, 0, 0]
            )
        
        # 960x1080ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ìµœì¢… ê³ ì • í¬ê¸°)
        return cv2.resize(cropped, (960, 1080))
    
    def _apply_smoothing(self, center: Tuple[float, float]) -> Tuple[float, float]:
        """EMA ìŠ¤ë¬´ë”© ì ìš©"""
        if self.smooth_center is None:
            return center
            
        alpha = self.smoothing_alpha
        smooth_x = alpha * center[0] + (1 - alpha) * self.smooth_center[0]
        smooth_y = alpha * center[1] + (1 - alpha) * self.smooth_center[1]
        
        return (smooth_x, smooth_y)
    
    def should_consider_reassignment(self) -> bool:
        """ì¬í• ë‹¹ì„ ê²€í† í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨ (ì‹ ë¢°ë„ ê¸°ë°˜)"""
        if len(self.confidence_history) < self.reassignment_trigger_count:
            return False
        
        # ìµœê·¼ Nê°œ í”„ë ˆì„ì˜ ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ì€ì§€ í™•ì¸
        recent_confidences = self.confidence_history[-self.reassignment_trigger_count:]
        low_confidence_count = sum(1 for conf in recent_confidences 
                                 if conf < self.low_confidence_threshold)
        
        # 80% ì´ìƒì´ ë‚®ì€ ì‹ ë¢°ë„ë©´ ì¬í• ë‹¹ ê²€í† 
        return low_confidence_count >= (self.reassignment_trigger_count * 0.8)
    
    def get_average_confidence(self) -> float:
        """í‰ê·  ì‹ ë¢°ë„ ë°˜í™˜"""
        if not self.confidence_history:
            return 0.0
        return sum(self.confidence_history) / len(self.confidence_history)
    
    def get_stats(self) -> Dict[str, Any]:
        """ê²€ì¶œ í†µê³„ ë°˜í™˜"""
        total_detections = self.detection_success_count + self.detection_fail_count
        success_rate = (self.detection_success_count / max(1, total_detections)) * 100
        
        return {
            'person_id': self.person_id,
            'total_frames': self.total_frames,
            'detection_success': self.detection_success_count,
            'detection_fail': self.detection_fail_count,
            'success_rate': success_rate,
            'has_detection': self.face_detection is not None,
            'detection_active': len(self.detection_history) > 0,
            'smooth_center': self.smooth_center,
            'history_length': len(self.detection_history),
            'average_confidence': self.get_average_confidence(),
            'should_reassign': self.should_consider_reassignment(),
            'fixed_crop_size': self.fixed_crop_size
        }


class FaceEmbeddingTracker(FaceTracker):
    """ê³ ê¸‰ ì–¼êµ´ ì„ë² ë”© ê¸°ë°˜ íŠ¸ë˜ì»¤ (SmartEmbeddingManager + ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ í™œìš©)"""
    
    def __init__(self, person_id: str, smoothing_alpha: float = 0.3):
        super().__init__(person_id, smoothing_alpha)
        
        # ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ì (SmartEmbeddingManager í™œìš©)
        if MODEL_MANAGER_AVAILABLE:
            self.smart_embedding_manager = SmartEmbeddingManager(max_size=10, ttl_seconds=300)
            print(f"âœ… {person_id}: SmartEmbeddingManager ì´ˆê¸°í™” (max_size=10, ttl=300s)")
        else:
            self.smart_embedding_manager = None
            print(f"âš ï¸ {person_id}: SmartEmbeddingManager ë¹„í™œì„±í™”")
        
        # ê°œë³„ ì„ë² ë”© ì¶”ì  (ë””ë²„ê¹…ìš©)
        self.face_embeddings = []  # ë°±ì—… íˆìŠ¤í† ë¦¬
        self.reference_embedding = None  # ëŒ€í‘œ ì„ë² ë”© (í‰ê· )
        self.max_embeddings = 10  # ìµœëŒ€ 10ê°œ ì„ë² ë”© ìœ ì§€
        self.embedding_threshold = 0.75  # 0.6 â†’ 0.75 (ë” ì—„ê²©í•œ ì„ê³„ê°’)
        
        # ê³ ê¸‰ í†µê³„
        self.embedding_updates = 0
        self.similarity_scores = []  # ìµœê·¼ ìœ ì‚¬ë„ ì ìˆ˜ë“¤
        self.l2_normalization_enabled = True  # L2 ì •ê·œí™” ì‚¬ìš©
        
    def add_face_embedding(self, embedding: torch.Tensor) -> None:
        """ìƒˆ ì„ë² ë”© ì¶”ê°€ ë° ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ì ì—…ë°ì´íŠ¸"""
        if embedding is None:
            return
            
        # L2 ì •ê·œí™” (ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš©)
        normalized_embedding = torch.nn.functional.normalize(embedding, p=2, dim=-1)
        
        # ê³ ê¸‰ ì„ë² ë”© ê´€ë¦¬ìì— ì¶”ê°€ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if self.smart_embedding_manager is not None:
            track_id = f"{self.person_id}_{self.embedding_updates}"
            self.smart_embedding_manager.add_embedding(track_id, normalized_embedding)
        
        # ë°±ì—… íˆìŠ¤í† ë¦¬ì—ë„ ì¶”ê°€ (í˜¸í™˜ì„±)
        self.face_embeddings.append(normalized_embedding)
        if len(self.face_embeddings) > self.max_embeddings:
            self.face_embeddings.pop(0)  # ì˜¤ë˜ëœ ê²ƒ ì œê±°
        
        # ëŒ€í‘œ ì„ë² ë”© ì—…ë°ì´íŠ¸ (í‰ê· )
        if len(self.face_embeddings) > 0:
            stacked_embeddings = torch.stack(self.face_embeddings)
            self.reference_embedding = torch.mean(stacked_embeddings, dim=0)
            self.reference_embedding = torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1)
            
        self.embedding_updates += 1
        
    def compute_face_similarity(self, new_embedding: torch.Tensor) -> float:
        """ìƒˆ ì–¼êµ´ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš©)"""
        if self.reference_embedding is None or new_embedding is None:
            return 0.0
        
        # ê³ ê¸‰ ìœ ì‚¬ë„ í•¨ìˆ˜ ì‚¬ìš© (MODEL_MANAGER_AVAILABLE í™•ì¸)
        if MODEL_MANAGER_AVAILABLE:
            try:
                # calculate_face_similarity í•¨ìˆ˜ ì‚¬ìš© (L2 ì •ê·œí™” ìë™ ì ìš©)
                score = calculate_face_similarity(
                    self.reference_embedding.unsqueeze(0), 
                    new_embedding.unsqueeze(0), 
                    use_l2_norm=self.l2_normalization_enabled
                )
            except Exception as e:
                print(f"âš ï¸ ê³ ê¸‰ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}, ê¸°ë³¸ ë°©ë²• ì‚¬ìš©")
                # ë°±ì—… ë°©ë²•: ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                score = torch.cosine_similarity(
                    torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1).unsqueeze(0),
                    torch.nn.functional.normalize(new_embedding, p=2, dim=-1).unsqueeze(0),
                    dim=-1
                ).item()
        else:
            # ê¸°ë³¸ ë°©ë²•: L2 ì •ê·œí™” + ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            score = torch.cosine_similarity(
                torch.nn.functional.normalize(self.reference_embedding, p=2, dim=-1).unsqueeze(0),
                torch.nn.functional.normalize(new_embedding, p=2, dim=-1).unsqueeze(0),
                dim=-1
            ).item()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.similarity_scores.append(score)
        if len(self.similarity_scores) > 50:  # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
            self.similarity_scores.pop(0)
            
        return score
    
    def compute_hybrid_score(self, face_detection: 'FaceDetection') -> float:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì„ë² ë”© ìœ ì‚¬ë„ 70% + ìœ„ì¹˜ ê±°ë¦¬ 30%)"""
        if face_detection is None:
            return 0.0
            
        # 1. ì–¼êµ´ ìœ ì‚¬ë„ (0.0 ~ 1.0)
        face_similarity = 0.0
        if face_detection.embedding is not None and self.reference_embedding is not None:
            face_similarity = self.compute_face_similarity(face_detection.embedding)
        
        # 2. ìœ„ì¹˜ ê±°ë¦¬ (0.0 ~ 1.0, ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        position_distance = self.get_distance_to_detection(face_detection)
        position_score = 1.0 / (1.0 + position_distance / 100.0)  # 100px ê¸°ì¤€ ì •ê·œí™”
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        # ì„ë² ë”©ì´ ì—†ê±°ë‚˜ referenceê°€ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ ì ìˆ˜ë§Œ ì‚¬ìš©
        if face_detection.embedding is not None and self.reference_embedding is not None:
            hybrid_score = face_similarity * 0.7 + position_score * 0.3
        else:
            hybrid_score = position_score  # ìœ„ì¹˜ ê¸°ë°˜ë§Œ ì‚¬ìš©
        
        return hybrid_score
    
    def is_same_person(self, face_detection: 'FaceDetection') -> bool:
        """ê°™ì€ ì‚¬ëŒì¸ì§€ íŒë‹¨ (ì„ë² ë”© ê¸°ë°˜)"""
        if face_detection.embedding is None:
            # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
            distance = self.get_distance_to_detection(face_detection)
            return distance <= self.position_threshold
            
        # ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ íŒë‹¨
        similarity = self.compute_face_similarity(face_detection.embedding)
        return similarity >= self.embedding_threshold
    
    def update_with_embedding(self, detection: Optional['FaceDetection']) -> bool:
        """ì„ë² ë”©ì„ í¬í•¨í•œ ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸"""
        success = self.update_detection(detection)
        
        # ì„ë² ë”© ì¶”ê°€ (ê²€ì¶œ ì„±ê³µ ì‹œì—ë§Œ)
        if success and detection is not None and detection.embedding is not None:
            self.add_face_embedding(detection.embedding)
            
        return success
    
    def get_embedding_stats(self) -> Dict[str, Any]:
        """ê³ ê¸‰ ì„ë² ë”© ê´€ë ¨ í†µê³„ ë°˜í™˜ (SmartEmbeddingManager í¬í•¨)"""
        avg_similarity = 0.0
        if len(self.similarity_scores) > 0:
            avg_similarity = sum(self.similarity_scores) / len(self.similarity_scores)
        
        # SmartEmbeddingManager í†µê³„ ì¶”ê°€
        smart_stats = {}
        if self.smart_embedding_manager is not None:
            smart_stats = self.smart_embedding_manager.get_stats()
            
        return {
            'embeddings_count': len(self.face_embeddings),
            'embedding_updates': self.embedding_updates,
            'has_reference_embedding': self.reference_embedding is not None,
            'avg_similarity': avg_similarity,
            'recent_similarities': self.similarity_scores[-5:],  # ìµœê·¼ 5ê°œ
            'embedding_threshold': self.embedding_threshold,
            'l2_normalization_enabled': self.l2_normalization_enabled,
            'smart_embedding_manager_enabled': self.smart_embedding_manager is not None,
            'smart_embedding_stats': smart_stats  # SmartEmbeddingManager í†µê³„
        }


class DualFaceTrackingSystem:
    """í†µí•© ì–¼êµ´ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, args):
        self.input_path = args.input
        self.output_path = args.output
        self.mode = args.mode
        self.gpu_id = args.gpu
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_path).parent.mkdir(exist_ok=True)
        
        # ì²˜ë¦¬ ì„¤ì •
        self.detection_interval = 1   # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì–¼êµ´ ê²€ì¶œ (íŠ¸ë˜ì»¤ ì—†ìœ¼ë¯€ë¡œ)
        self.margin_factor = 5.0      # ì–¼êµ´ í¬ê¸° ëŒ€ë¹„ í¬ë¡­ ë°°ìœ¨ (ìƒë°˜ì‹  í¬í•¨)
        self.confidence_threshold = 0.1  # 0.3â†’0.1 (ë§¤ìš° ê´€ëŒ€í•œ ì„ê³„ê°’)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.detection_method = None
        self._debug_detection_logged = False
        self._initialize_models()
        
        # íŠ¸ë˜ì»¤ ì´ˆê¸°í™” (ì„ë² ë”© ê¸°ë°˜ íŠ¸ë˜ì»¤ ì‚¬ìš©)
        self.person1_tracker = FaceEmbeddingTracker("Person1", smoothing_alpha=0.1)
        self.person2_tracker = FaceEmbeddingTracker("Person2", smoothing_alpha=0.1)
        
        # í¬ê¸° ê¸°ë°˜ ê°„ë‹¨í•œ í• ë‹¹ ì‹œìŠ¤í…œ
        self.debug_mode = False  # ë””ë²„ê·¸ ëª¨ë“œ (í¬ê¸° ì •ë³´ ì¶œë ¥)
        self.size_stabilize = False  # í¬ê¸° ê¸°ë°˜ ì•ˆì •í™” ì‚¬ìš© ì—¬ë¶€
        
        # FaceNet ëª¨ë¸ ì´ˆê¸°í™” (ì–¼êµ´ ì„ë² ë”©ìš©)
        self.model_manager = None
        self.resnet = None
        self.face_transform = None
        self._initialize_facenet()
        
        # í†µê³„
        self.stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'detection_calls': 0,
            'start_time': time.time(),
            'processing_time': 0.0
        }
        
        print(f"ğŸ—ï¸ DualFaceTrackingSystem ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ“¥ ì…ë ¥: {self.input_path}")
        print(f"   ğŸ“¤ ì¶œë ¥: {self.output_path}")
        print(f"   ğŸ”§ ê²€ì¶œ ê°„ê²©: {self.detection_interval}í”„ë ˆì„")
        print(f"   ğŸ“ í¬ë¡­ ë°°ìœ¨: {self.margin_factor}x")
        
    def _initialize_models(self):
        """ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ğŸ—ï¸ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°©ë²• 1: OpenCV Haar Cascade (ê°€ì¥ ì•ˆì •ì )
        try:
            # í™•ì¸ëœ ê²½ë¡œë¥¼ ì§ì ‘ ì‚¬ìš©
            cascade_path = "/usr/local/share/opencv4/haarcascades/haarcascade_frontalface_default.xml"
            
            if Path(cascade_path).exists():
                self.face_cascade = cv2.CascadeClassifier(cascade_path)
                if not self.face_cascade.empty():
                    self.detection_method = "haar"
                    print(f"âœ… OpenCV Haar Cascade ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                    print(f"   ğŸ“ ê²½ë¡œ: {cascade_path}")
                    return
                else:
                    print(f"âš ï¸ Haar Cascade ìƒì„± ì‹¤íŒ¨: {cascade_path}")
            else:
                print(f"âš ï¸ Haar Cascade íŒŒì¼ ì—†ìŒ: {cascade_path}")
                
        except Exception as e:
            print(f"âš ï¸ Haar Cascade ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: ê¸°ì¡´ í”„ë¡œì íŠ¸ì˜ MTCNN ì‹œë„ (ìƒìœ„ ë””ë ‰í† ë¦¬)
        try:
            # ìƒìœ„ í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
            parent_project = Path(__file__).parent.parent
            sys.path.insert(0, str(parent_project))
            sys.path.insert(0, str(parent_project / "src"))
            
            from face_tracker.core.models import ModelManager
            self.mtcnn, self.resnet = ModelManager.get_models()
            self.detection_method = "mtcnn"
            print("âœ… ìƒìœ„ í”„ë¡œì íŠ¸ MTCNN + FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ ìƒìœ„ í”„ë¡œì íŠ¸ MTCNN ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 3: ê°„ë‹¨í•œ MTCNN ì§ì ‘ ë¡œë“œ
        try:
            from facenet_pytorch import MTCNN
            self.mtcnn = MTCNN(device='cuda' if torch.cuda.is_available() else 'cpu')
            self.detection_method = "mtcnn_direct"
            print("âœ… facenet-pytorch MTCNN ì§ì ‘ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ ì§ì ‘ MTCNN ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 4: MediaPipe ì–¼êµ´ ê²€ì¶œ
        try:
            import mediapipe as mp
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_drawing = mp.solutions.drawing_utils
            self.face_detection = self.mp_face_detection.FaceDetection(
                model_selection=1, min_detection_confidence=0.3)
            self.detection_method = "mediapipe"
            print("âœ… MediaPipe ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ MediaPipe ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        raise RuntimeError("âŒ ëª¨ë“  ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
    
    def _initialize_facenet(self):
        """FaceNet ëª¨ë¸ ì´ˆê¸°í™” (ì–¼êµ´ ì„ë² ë”©ìš©)"""
        print("ğŸ§  FaceNet ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if not MODEL_MANAGER_AVAILABLE:
            print("âš ï¸ ModelManagerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ. ì„ë² ë”© ê¸°ëŠ¥ ë¹„í™œì„±í™”")
            return
            
        try:
            # ModelManagerë¡œ FaceNet ëª¨ë¸ ë¡œë“œ
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.model_manager = ModelManager(device)
            self.resnet = self.model_manager.get_resnet()
            
            # ì–¼êµ´ ì „ì²˜ë¦¬ ë³€í™˜ (FaceNetìš©)
            self.face_transform = transforms.Compose([
                transforms.Resize((160, 160)),  # FaceNet ì…ë ¥ í¬ê¸°
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            print("âœ… FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"   ğŸ“ ë””ë°”ì´ìŠ¤: {device}")
            print("   ğŸ§  ì–¼êµ´ ì„ë² ë”© ê¸°ëŠ¥ í™œì„±í™”")
            
        except Exception as e:
            print(f"âš ï¸ FaceNet ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("   ğŸ”„ ì„ë² ë”© ì—†ì´ ìœ„ì¹˜ ê¸°ë°˜ ì¶”ì ë§Œ ì‚¬ìš©")
            self.model_manager = None
            self.resnet = None
            self.face_transform = None
    
    def generate_face_embedding(self, face_crop: np.ndarray) -> Optional[torch.Tensor]:
        """ì–¼êµ´ í¬ë¡­ì—ì„œ ì„ë² ë”© ìƒì„±"""
        if self.resnet is None or self.face_transform is None:
            return None
            
        try:
            # OpenCV BGR -> PIL RGB ë³€í™˜
            rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_crop)
            
            # ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            face_tensor = self.face_transform(pil_image).unsqueeze(0)
            
            # GPUë¡œ ì´ë™ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                face_tensor = face_tensor.cuda()
            
            # ì„ë² ë”© ìƒì„± (ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”)
            with torch.no_grad():
                embedding = self.resnet(face_tensor)
                # L2 ì •ê·œí™”
                embedding = F.normalize(embedding, p=2, dim=1)
            
            return embedding.squeeze(0)  # ë°°ì¹˜ ì°¨ì› ì œê±°
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def detect_faces(self, frame: np.ndarray) -> List[FaceDetection]:
        """í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        faces = []
        
        try:
            # ë””ë²„ê¹…: ê²€ì¶œ ë°©ë²• í™•ì¸
            if not self._debug_detection_logged:
                print(f"ğŸ”§ ê²€ì¶œ ë°©ë²•: {self.detection_method}")
                self._debug_detection_logged = True
            
            if self.detection_method == "mtcnn":
                faces = self._detect_faces_mtcnn(frame)
            elif self.detection_method == "mtcnn_direct":
                faces = self._detect_faces_mtcnn_direct(frame)
            elif self.detection_method == "haar":
                faces = self._detect_faces_haar(frame)
            elif self.detection_method == "mediapipe":
                faces = self._detect_faces_mediapipe(frame)
            elif self.detection_method == "dnn":
                faces = self._detect_faces_dnn(frame)
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ê²€ì¶œ ë°©ë²•: {self.detection_method}")
                return []
            
            # ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°)
            faces.sort(key=lambda x: x.confidence, reverse=True)
            
            # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì„ íƒ
            faces = faces[:2]
            
            # ê° ì–¼êµ´ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
            for face in faces:
                try:
                    # ì–¼êµ´ ì˜ì—­ í¬ë¡­
                    x1, y1, x2, y2 = face.bbox
                    face_crop = frame[y1:y2, x1:x2]
                    
                    # ìœ íš¨í•œ í¬ë¡­ì¸ì§€ í™•ì¸
                    if face_crop.size > 0:
                        # ì„ë² ë”© ìƒì„±
                        embedding = self.generate_face_embedding(face_crop)
                        face.embedding = embedding
                        
                except Exception as e:
                    print(f"âš ï¸ ì–¼êµ´ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    face.embedding = None
            
            return faces
            
        except Exception as e:
            print(f"âš ï¸ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _detect_faces_mtcnn(self, frame: np.ndarray) -> List[FaceDetection]:
        """MTCNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        boxes_list, probs_list = self.mtcnn.detect([pil_image])
        faces = []
        
        if boxes_list[0] is not None and probs_list[0] is not None:
            boxes = boxes_list[0]
            probs = probs_list[0]
            
            for box, prob in zip(boxes, probs):
                if prob > self.confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    faces.append(FaceDetection((x1, y1, x2, y2), prob))
        
        return faces
    
    def _detect_faces_haar(self, frame: np.ndarray) -> List[FaceDetection]:
        """Haar Cascadeë¡œ ì–¼êµ´ ê²€ì¶œ"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # ê· í˜•ì¡íŒ íŒŒë¼ë¯¸í„°ë¡œ ì•ˆì •ì ì¸ ì–¼êµ´ ê²€ì¶œ
        detected_faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.08,    # 1.1â†’1.08 (ì¡°ê¸ˆ ë” ì„¸ë°€í•œ ê²€ì¶œ)
            minNeighbors=4,      # 5â†’4 (ì•½ê°„ ë” ê´€ëŒ€í•˜ê²Œ)
            minSize=(25, 25),    # 30â†’25 (ì¡°ê¸ˆ ë” ì‘ì€ ì–¼êµ´ë„ í¬í•¨)
            maxSize=(180, 180),  # 200â†’180 (ë” ì ì ˆí•œ ìµœëŒ€ í¬ê¸°)
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        faces = []
        for (x, y, w, h) in detected_faces:
            # ì–¼êµ´ í¬ê¸° ë° ì¢…íš¡ë¹„ ê²€ì¦
            if w > 20 and h > 20 and w < 150 and h < 150:
                # ì¢…íš¡ë¹„ ì²´í¬ (ì–¼êµ´ì€ ëŒ€ëµ ì •ì‚¬ê°í˜•)
                aspect_ratio = w / h
                if 0.7 < aspect_ratio < 1.3:
                    confidence = 0.9  # HaarëŠ” ì‹ ë¢°ë„ê°€ ì—†ìœ¼ë¯€ë¡œ ê³ ì •ê°’
                    faces.append(FaceDetection((x, y, x + w, y + h), confidence))
        
        return faces
    
    def _detect_faces_dnn(self, frame: np.ndarray) -> List[FaceDetection]:
        """DNNìœ¼ë¡œ ì–¼êµ´ ê²€ì¶œ"""
        h, w = frame.shape[:2]
        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), [104, 117, 123])
        self.dnn_net.setInput(blob)
        detections = self.dnn_net.forward()
        
        faces = []
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            
            if confidence > self.confidence_threshold:
                x1 = int(detections[0, 0, i, 3] * w)
                y1 = int(detections[0, 0, i, 4] * h)
                x2 = int(detections[0, 0, i, 5] * w)
                y2 = int(detections[0, 0, i, 6] * h)
                
                faces.append(FaceDetection((x1, y1, x2, y2), confidence))
        
        return faces
    
    def _detect_faces_mtcnn_direct(self, frame: np.ndarray) -> List[FaceDetection]:
        """facenet-pytorch MTCNNìœ¼ë¡œ ì§ì ‘ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # MTCNN ê²€ì¶œ
        boxes, probs = self.mtcnn.detect(pil_image)
        faces = []
        
        if boxes is not None and probs is not None:
            for box, prob in zip(boxes, probs):
                if prob > self.confidence_threshold:
                    x1, y1, x2, y2 = map(int, box)
                    faces.append(FaceDetection((x1, y1, x2, y2), prob))
        
        return faces
    
    def _detect_faces_mediapipe(self, frame: np.ndarray) -> List[FaceDetection]:
        """MediaPipeë¡œ ì–¼êµ´ ê²€ì¶œ"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_detection.process(rgb_frame)
        
        faces = []
        if results.detections:
            h, w, _ = frame.shape
            
            for detection in results.detections:
                # MediaPipe bounding box (normalized coordinates)
                bbox = detection.location_data.relative_bounding_box
                x1 = int(bbox.xmin * w)
                y1 = int(bbox.ymin * h)
                x2 = int((bbox.xmin + bbox.width) * w)
                y2 = int((bbox.ymin + bbox.height) * h)
                
                # ì‹ ë¢°ë„ ì ìˆ˜
                confidence = detection.score[0]
                
                if confidence > self.confidence_threshold:
                    faces.append(FaceDetection((x1, y1, x2, y2), confidence))
        
        return faces
    
    def _assign_by_size(self, faces: List[FaceDetection]) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ Person1(ê°€ì¥ í° ì–¼êµ´), Person2(ë‘ ë²ˆì§¸ í° ì–¼êµ´) í• ë‹¹"""
        if not faces:
            return None, None
        
        # ì–¼êµ´ í¬ê¸°(area)ë¡œ ì •ë ¬ - í° ìˆœì„œëŒ€ë¡œ
        sorted_faces = sorted(faces, key=lambda f: f.area, reverse=True)
        
        # ê°€ì¥ í° ì–¼êµ´ â†’ Person1 (ì™¼ìª½)
        person1_face = sorted_faces[0] if len(sorted_faces) >= 1 else None
        
        # ë‘ ë²ˆì§¸ë¡œ í° ì–¼êµ´ â†’ Person2 (ì˜¤ë¥¸ìª½)
        person2_face = sorted_faces[1] if len(sorted_faces) >= 2 else None
        
        # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        if self.debug_mode and person1_face and person2_face:
            size_ratio = person1_face.area / person2_face.area
            print(f"ğŸ“Š í¬ê¸° ë¹„êµ: P1={person1_face.area:.0f}, P2={person2_face.area:.0f}, ë¹„ìœ¨={size_ratio:.2f}")
        
        return person1_face, person2_face
    
    def assign_face_ids(self, faces: List[FaceDetection], frame_idx: int) -> Tuple[Optional[FaceDetection], Optional[FaceDetection]]:
        """í¬ê¸° ê¸°ë°˜ ë‹¨ìˆœ í• ë‹¹: ê°€ì¥ í° ì–¼êµ´=Person1, ë‘ ë²ˆì§¸=Person2"""
        if len(faces) == 0:
            return None, None
        
        # ë‹¨ìˆœí•˜ê²Œ í¬ê¸°ë§Œìœ¼ë¡œ í• ë‹¹
        person1_face, person2_face = self._assign_by_size(faces)
        
        # ê° íŠ¸ë˜ì»¤ì— ì—…ë°ì´íŠ¸
        if person1_face:
            self.person1_tracker.update_detection(person1_face)
        if person2_face:
            self.person2_tracker.update_detection(person2_face)
        
        # ë””ë²„ê·¸ ì •ë³´ (30í”„ë ˆì„ë§ˆë‹¤)
        if self.debug_mode and frame_idx % 30 == 0:
            if person1_face and person2_face:
                size_ratio = person1_face.area / person2_face.area
                print(f"ğŸ“Š í”„ë ˆì„ {frame_idx}: P1={person1_face.area:.0f}, P2={person2_face.area:.0f}, ë¹„ìœ¨={size_ratio:.2f}")
            elif person1_face:
                print(f"ğŸ“Š í”„ë ˆì„ {frame_idx}: P1={person1_face.area:.0f}, P2=ì—†ìŒ")
        
        return person1_face, person2_face
    
    # ì»´íŒ©íŠ¸í•œ í¬ê¸° ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ êµì²´ëœ ë³µì¡í•œ ë¡œì§ë“¤ì€ ì œê±°ë¨
    
    def create_split_screen(self, crop1: np.ndarray, crop2: np.ndarray) -> np.ndarray:
        """ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ìƒì„± (1920x1080)"""
        # 1920x1080 ìº”ë²„ìŠ¤ ìƒì„±
        split_screen = np.zeros((1080, 1920, 3), dtype=np.uint8)
        
        # ì™¼ìª½: Person1 (0:960)
        split_screen[0:1080, 0:960] = crop1
        
        # ì˜¤ë¥¸ìª½: Person2 (960:1920)
        split_screen[0:1080, 960:1920] = crop2
        
        return split_screen
    
    def add_overlay_info(self, frame: np.ndarray, frame_idx: int, fps: float) -> np.ndarray:
        """ì˜¤ë²„ë ˆì´ ì •ë³´ ì¶”ê°€"""
        # ì§„í–‰ë¥  ê³„ì‚°
        progress = (frame_idx / max(1, self.stats['total_frames'])) * 100
        
        # íŠ¸ë˜ì»¤ í†µê³„
        p1_stats = self.person1_tracker.get_stats()
        p2_stats = self.person2_tracker.get_stats()
        
        # í…ìŠ¤íŠ¸ ì •ë³´
        texts = [
            f"Frame: {frame_idx}/{self.stats['total_frames']}",
            f"Progress: {progress:.1f}%",
            f"FPS: {fps:.1f}",
            f"P1 Track: {p1_stats['success_rate']:.1f}%",
            f"P2 Track: {p2_stats['success_rate']:.1f}%",
            "Dual-Face Tracking v5.0"
        ]
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        for i, text in enumerate(texts):
            y_pos = 30 + i * 35
            cv2.putText(frame, text, (10, y_pos), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        return frame
    
    def process(self):
        """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘")
        
        # ë¹„ë””ì˜¤ ì—´ê¸°
        cap = cv2.VideoCapture(self.input_path)
        if not cap.isOpened():
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {self.input_path}")
        
        # ë¹„ë””ì˜¤ ì •ë³´
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / fps if fps > 0 else 0
        
        self.stats['total_frames'] = total_frames
        
        print(f"   ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps:.1f}fps")
        print(f"   â±ï¸ ì§€ì†ì‹œê°„: {duration:.1f}ì´ˆ, {total_frames}í”„ë ˆì„")
        
        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • (1920x1080 ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦°)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(self.output_path, fourcc, fps, (1920, 1080))
        
        if not writer.isOpened():
            cap.release()
            raise RuntimeError(f"âŒ ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨: {self.output_path}")
        
        # ì§„í–‰ë¥  í‘œì‹œ
        pbar = tqdm(total=total_frames, desc="ğŸ¯ ì–¼êµ´ íŠ¸ë˜í‚¹", ncols=80, leave=True)
        
        try:
            frame_idx = 0
            start_time = time.time()
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_idx += 1
                
                # 1. ì–¼êµ´ ê²€ì¶œ (ë§¤ í”„ë ˆì„ ë˜ëŠ” Ní”„ë ˆì„ë§ˆë‹¤)
                if self.detection_interval == 1 or frame_idx % self.detection_interval == 0:
                    # ë””ë²„ê¹…: ê²€ì¶œ ì „ í”„ë ˆì„ ì •ë³´
                    if frame_idx <= 10:
                        print(f"ğŸ” í”„ë ˆì„ {frame_idx} ê²€ì¶œ ì‹œë„ - í”„ë ˆì„ í¬ê¸°: {frame.shape}")
                    
                    faces = self.detect_faces(frame)
                    self.stats['detection_calls'] += 1
                    
                    # ë””ë²„ê¹…: ì–¼êµ´ ê²€ì¶œ ê²°ê³¼ ë¡œê·¸ (ë§¤ í”„ë ˆì„ ì¶œë ¥)
                    if frame_idx <= 50 or frame_idx % 200 == 1:  # ì²˜ìŒ 50í”„ë ˆì„ê³¼ 200í”„ë ˆì„ë§ˆë‹¤
                        print(f"ğŸ” í”„ë ˆì„ {frame_idx}: {len(faces)}ê°œ ì–¼êµ´ ê²€ì¶œ (ë°©ë²•: {self.detection_method})")
                        for i, face in enumerate(faces):
                            print(f"   ì–¼êµ´ {i+1}: ({face.x1},{face.y1})-({face.x2},{face.y2}) conf={face.confidence:.2f}")
                    
                    # ì–¼êµ´ ID í• ë‹¹ (ì¼ê´€ì„± ê°•í™”)
                    person1_face, person2_face = self.assign_face_ids(faces, frame_idx)
                    
                    # ë””ë²„ê¹…: ID í• ë‹¹ ê²°ê³¼ (ì¼ê´€ì„± ì¶”ì )
                    if frame_idx <= 60:  # ì²˜ìŒ 60í”„ë ˆì„ ë™ì•ˆ ìƒì„¸ ë¡œê·¸
                        p1_assigned = person1_face is not None
                        p2_assigned = person2_face is not None
                        print(f"   ğŸ¯ ID í• ë‹¹ (í”„ë ˆì„ {frame_idx}): P1={p1_assigned}, P2={p2_assigned}")
                        
                        if person1_face:
                            print(f"     P1 ì–¼êµ´: center={person1_face.center} conf={person1_face.confidence:.2f}")
                        if person2_face:
                            print(f"     P2 ì–¼êµ´: center={person2_face.center} conf={person2_face.confidence:.2f}")
                            
                        # ê¸°ì¡´ ì´ˆê¸° ìœ„ì¹˜ ê¸°ë°˜ ì½”ë“œ ì œê±°ë¨ (í¬ê¸° ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œ ë¶ˆí•„ìš”)
                    
                    # ê²€ì¶œ ê¸°ë°˜ ì—…ë°ì´íŠ¸ (íŠ¸ë˜ì»¤ ì—†ìŒ)
                    if person1_face:
                        result1 = self.person1_tracker.update_detection(person1_face)
                        if frame_idx <= 50:
                            print(f"   âœ… P1 ê²€ì¶œ ì—…ë°ì´íŠ¸: {result1}")
                    else:
                        self.person1_tracker.update_detection(None)
                        
                    if person2_face:
                        result2 = self.person2_tracker.update_detection(person2_face)
                        if frame_idx <= 50:
                            print(f"   âœ… P2 ê²€ì¶œ ì—…ë°ì´íŠ¸: {result2}")
                    else:
                        self.person2_tracker.update_detection(None)
                
                # 3. í¬ë¡­ ì˜ì—­ ìƒì„±
                crop1 = self.person1_tracker.get_crop_region(frame, self.margin_factor)
                crop2 = self.person2_tracker.get_crop_region(frame, self.margin_factor)
                
                # 4. ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦° ìƒì„±
                split_screen = self.create_split_screen(crop1, crop2)
                
                # 5. ì˜¤ë²„ë ˆì´ ì •ë³´ ì¶”ê°€
                current_fps = frame_idx / max(0.01, time.time() - start_time)
                split_screen = self.add_overlay_info(split_screen, frame_idx, current_fps)
                
                # 6. ì¶œë ¥
                writer.write(split_screen)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                self.stats['processed_frames'] = frame_idx
                pbar.update(1)
                
                # ì¤‘ê°„ ì§„í–‰ë¥  ì¶œë ¥ (100í”„ë ˆì„ë§ˆë‹¤)
                if frame_idx % 100 == 0:
                    p1_rate = self.person1_tracker.get_stats()['success_rate']
                    p2_rate = self.person2_tracker.get_stats()['success_rate']
                    pbar.set_postfix({
                        'P1': f'{p1_rate:.1f}%',
                        'P2': f'{p2_rate:.1f}%',
                        'FPS': f'{current_fps:.1f}'
                    })
        
        finally:
            cap.release()
            writer.release()
            pbar.close()
        
        # ìµœì¢… í†µê³„
        self.stats['processing_time'] = time.time() - start_time
        self._print_final_stats()
    
    def _print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š Dual-Face Tracking ìµœì¢… ê²°ê³¼")
        print("=" * 60)
        
        # ì „ì²´ í†µê³„
        total_time = self.stats['processing_time']
        total_frames = self.stats['processed_frames']
        detection_calls = self.stats['detection_calls']
        avg_fps = total_frames / max(0.01, total_time)
        
        print(f"ğŸ¯ ì²˜ë¦¬ ì™„ë£Œ: {self.output_path}")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ğŸ“Š í‰ê·  FPS: {avg_fps:.1f}")
        print(f"ğŸ” ì–¼êµ´ ê²€ì¶œ í˜¸ì¶œ: {detection_calls}íšŒ")
        
        # ê°œë³„ ê²€ì¶œ í†µê³„
        p1_stats = self.person1_tracker.get_stats()
        p2_stats = self.person2_tracker.get_stats()
        
        print(f"\nğŸ‘¤ Person1 ê²€ì¶œ:")
        print(f"   ì„±ê³µë¥ : {p1_stats['success_rate']:.1f}%")
        print(f"   ì„±ê³µ/ì‹¤íŒ¨: {p1_stats['detection_success']}/{p1_stats['detection_fail']}")
        print(f"   ê²€ì¶œ ìƒíƒœ: {'âœ…' if p1_stats['has_detection'] else 'âŒ'}")
        print(f"   íˆìŠ¤í† ë¦¬ ê¸¸ì´: {p1_stats['history_length']}")
        
        print(f"\nğŸ‘¤ Person2 ê²€ì¶œ:")
        print(f"   ì„±ê³µë¥ : {p2_stats['success_rate']:.1f}%")
        print(f"   ì„±ê³µ/ì‹¤íŒ¨: {p2_stats['detection_success']}/{p2_stats['detection_fail']}")
        print(f"   ê²€ì¶œ ìƒíƒœ: {'âœ…' if p2_stats['has_detection'] else 'âŒ'}")
        print(f"   íˆìŠ¤í† ë¦¬ ê¸¸ì´: {p2_stats['history_length']}")
        
        print(f"\nğŸ¬ ì¶œë ¥ ì •ë³´:")
        if Path(self.output_path).exists():
            file_size = Path(self.output_path).stat().st_size / 1024**2
            print(f"   ğŸ“„ íŒŒì¼: {self.output_path}")
            print(f"   ğŸ’¾ í¬ê¸°: {file_size:.1f}MB")
            print(f"   ğŸ“º í•´ìƒë„: 1920x1080 (ìŠ¤í”Œë¦¿ ìŠ¤í¬ë¦°)")
        else:
            print(f"   âŒ ì¶œë ¥ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="Phase 5: Dual-Face Tracking System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰ (í¬ê¸° ê¸°ë°˜ ìë™ í• ë‹¹)
  python3 face_tracking_system.py
  
  # ë””ë²„ê·¸ ëª¨ë“œë¡œ í¬ê¸° ì •ë³´ í™•ì¸
  python3 face_tracking_system.py --debug
  
  # ì‚¬ìš©ì ì§€ì • ë¹„ë””ì˜¤
  python3 face_tracking_system.py --input tests/videos/sample.mp4 --output output/tracked.mp4 --debug
  
  # í¬ê¸° ê¸°ë°˜ ì•ˆì •í™” ì‚¬ìš©
  python3 face_tracking_system.py --size-stabilize --debug
        """
    )
    
    parser.add_argument("--input", 
                       default="tests/videos/2people_sample1.mp4",
                       help="ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ê¸°ë³¸ê°’: tests/videos/2people_sample1.mp4)")
    parser.add_argument("--output", 
                       default="output/2people_sample1_tracked.mp4",
                       help="ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ (ê¸°ë³¸ê°’: output/2people_sample1_tracked.mp4)")
    parser.add_argument("--mode", 
                       default="dual_face", 
                       choices=["dual_face", "single"],
                       help="ì²˜ë¦¬ ëª¨ë“œ (ê¸°ë³¸ê°’: dual_face)")
    parser.add_argument("--gpu", 
                       type=int, 
                       default=0,
                       help="GPU ID (ê¸°ë³¸ê°’: 0)")
    parser.add_argument("--debug", 
                       action="store_true",
                       help="ë””ë²„ê·¸ ëª¨ë“œ (í¬ê¸° ë¹„êµ ì •ë³´ ì¶œë ¥)")
    parser.add_argument("--size-stabilize", 
                       action="store_true",
                       help="í¬ê¸° ê¸°ë°˜ í• ë‹¹ ì•ˆì •í™” ì‚¬ìš©")
    
    args = parser.parse_args()
    
    print("ğŸš€ Dual-Face Tracking System v6.0 (í¬ê¸° ê¸°ë°˜)")
    print("=" * 50)
    print(f"   ğŸ“¥ ì…ë ¥: {args.input}")
    print(f"   ğŸ“¤ ì¶œë ¥: {args.output}")
    print(f"   ğŸ”§ ëª¨ë“œ: {args.mode}")
    print(f"   ğŸ–¥ï¸ GPU: {args.gpu}")
    print(f"   ğŸ” ë””ë²„ê·¸: {args.debug}")
    print(f"   âš™ï¸ ì•ˆì •í™”: {args.size_stabilize}")
    print("=" * 50)
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not Path(args.input).exists():
        print(f"âŒ ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {args.input}")
        sys.exit(1)
    
    # ì‹œìŠ¤í…œ ì‹¤í–‰
    try:
        system = DualFaceTrackingSystem(args)
        system.debug_mode = args.debug
        system.size_stabilize = args.size_stabilize
        system.process()
        
        print("\nğŸ‰ Phase 5 ì™„ë£Œ: ì–¼êµ´ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()