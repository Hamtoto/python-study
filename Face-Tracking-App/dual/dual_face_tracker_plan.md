# Dual-Face High-Speed Video Processing System (Full GPU Pipeline)

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

**í”„ë¡œì íŠ¸ëª…**: Dual-Face High-Speed Video Processing System  
**í•µì‹¬ ëª©í‘œ**: **PyAV NVDEC â†’ TensorRT â†’ NVENC í’€ GPU íŒŒì´í”„ë¼ì¸**

**ì£¼ìš” ëª©í‘œ**:  
- ì…ë ¥ëœ ì˜ìƒì—ì„œ 2ëª…ì˜ ì–¼êµ´ì„ ê²€ì¶œí•˜ì—¬ ì¢Œìš° ë¶„ê¸° ì²˜ë¦¬
- **CUDA Stream ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬**ë¡œ ì—¬ëŸ¬ ì˜ìƒ ë™ì‹œ ì²˜ë¦¬
- **PyAV(hwaccel=cuda) â†’ TensorRT â†’ NVENC(H.264)** ì œë¡œì¹´í”¼ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ê¸°ì¡´ ëŒ€ë¹„ **5-8ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ** ë‹¬ì„± (ë””ì½”ë”©+ì¸ì½”ë”© ë³‘ëª© ì™„ì „ ì œê±°)
- í´ë¼ì´ì–¸íŠ¸ ìš”êµ¬ì‚¬í•­: ìµœëŒ€í•œ ë§ì€ ì˜ìƒì„ ìµœëŒ€í•œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ì—¬ ë‚©í’ˆ

**ì²˜ë¦¬ ë°©ì‹ í˜ì‹ **:
```
ê¸°ì¡´ CPU íŒŒì´í”„ë¼ì¸: ì˜ìƒ1(23ë¶„) â†’ ì˜ìƒ2(23ë¶„) â†’ ì˜ìƒ3(23ë¶„) = 69ë¶„
ì‹ ê·œ Full GPU íŒŒì´í”„ë¼ì¸: [ì˜ìƒ1,2,3,4] CUDA Stream ë³‘ë ¬ = 12-15ë¶„
```

---

## ğŸš€ Full GPU ì œë¡œì¹´í”¼ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë‹¨ì¼ GPU í”„ë¡œì„¸ìŠ¤ (RTX 5090)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚CUDA Stream 1â”‚    â”‚CUDA Stream 2â”‚    â”‚CUDA Stream 3â”‚    â”‚CUDA Stream 4â”‚    â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚
â”‚  â”‚PyAV NVDEC   â”‚    â”‚PyAV NVDEC   â”‚    â”‚PyAV NVDEC   â”‚    â”‚PyAV NVDEC   â”‚    â”‚
â”‚  â”‚(hwaccel=cuda)â”‚   â”‚(hwaccel=cuda)â”‚   â”‚(hwaccel=cuda)â”‚   â”‚(hwaccel=cuda)â”‚   â”‚
â”‚  â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚
â”‚  â”‚GPU ì „ì²˜ë¦¬     â”‚    â”‚GPU ì „ì²˜ë¦¬     â”‚    â”‚GPU ì „ì²˜ë¦¬    â”‚    â”‚GPU ì „ì²˜ë¦¬     â”‚    â”‚
â”‚  â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚
â”‚  â”‚TensorRT     â”‚    â”‚TensorRT     â”‚    â”‚TensorRT     â”‚    â”‚TensorRT     â”‚    â”‚
â”‚  â”‚YOLO/SCRFD   â”‚    â”‚YOLO/SCRFD   â”‚    â”‚YOLO/SCRFD   â”‚    â”‚YOLO/SCRFD   â”‚    â”‚
â”‚  â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚
â”‚  â”‚ì¡°ê±´ë¶€ ReID   â”‚    â”‚ì¡°ê±´ë¶€ ReID   â”‚    â”‚ì¡°ê±´ë¶€ ReID   â”‚    â”‚ì¡°ê±´ë¶€ ReID   â”‚    â”‚
â”‚  â”‚(ByteTrack+) â”‚    â”‚(ByteTrack+) â”‚    â”‚(ByteTrack+) â”‚    â”‚(ByteTrack+) â”‚    â”‚
â”‚  â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚
â”‚  â”‚GPU ë¦¬ì‚¬ì´ì¦ˆ+  â”‚     â”‚GPU ë¦¬ì‚¬ì´ì¦ˆ+  â”‚    â”‚GPU ë¦¬ì‚¬ì´ì¦ˆ+  â”‚    â”‚GPU ë¦¬ì‚¬ì´ì¦ˆ+  â”‚    â”‚
â”‚  â”‚íƒ€ì¼ í•©ì„±      â”‚     â”‚íƒ€ì¼ í•©ì„±      â”‚    â”‚íƒ€ì¼ í•©ì„±      â”‚    â”‚íƒ€ì¼ í•©ì„±      â”‚    â”‚
â”‚  â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚    â†“        â”‚    â”‚
â”‚  â”‚NVENC H.264  â”‚    â”‚NVENC H.264  â”‚    â”‚NVENC H.264  â”‚    â”‚NVENC H.264  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ë™ì¼ í•´ìƒë„ ê·¸ë£¹í•‘ â†’ ì†Œë°°ì¹˜(4-8) â†’ TensorRT ë³‘ë ¬ ì¶”ë¡  â†’ GPU íš¨ìœ¨ ê·¹ëŒ€í™”               â”‚
â”‚  CPUâ†”GPU ë©”ëª¨ë¦¬ ë³µì‚¬ ì™„ì „ ì œê±° â†’ ì œë¡œì¹´í”¼ íŒŒì´í”„ë¼ì¸                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ì§€ëŠ¥í˜• ê´€ë¦¬ ì‹œìŠ¤í…œ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”§ HybridConfigManager  â”‚  âš™ï¸ ConditionalReID  â”‚  ğŸ›¡ï¸ TileErrorPolicy      â”‚
â”‚  - ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì •       â”‚  - ID ìŠ¤ì™‘ ê°ì§€        â”‚  - ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ì²˜ë¦¬         â”‚
â”‚  - ìë™ í•˜ë“œì›¨ì–´ í”„ë¡œë¹™   â”‚  - ê²½ëŸ‰ ReID í™œì„±í™”    â”‚  - ëŒ€ì²´ í”„ë ˆì„ ìƒì„±         â”‚
â”‚  - ì•ˆì „í•œ ê¸°ë³¸ê°’ í´ë°±     â”‚  - ì„±ëŠ¥ ìµœì í™”         â”‚  - ì‹¤íŒ¨ ë¶„ì„ ë° ë³µêµ¬        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» ì‹œìŠ¤í…œ í™˜ê²½ ë° ë¦¬ì†ŒìŠ¤ í• ë‹¹ (ëŸ°íƒ€ì„ í”„ë¡œë¹™ ê¸°ë°˜)

### âš™ï¸ **í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ** (ìœ ì—°ì„± + ì•ˆì •ì„±)

**í•µì‹¬ í˜ì‹ **: **ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì •** â†’ **ìë™ í”„ë¡œë¹™** â†’ **ê¸°ë³¸ê°’** 3ë‹¨ê³„ ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ

**ConfigManager ì•„í‚¤í…ì²˜**:

```python
class HybridConfigManager:
    def __init__(self):
        self.config_priority = [
            'manual_config.yaml',      # 1ìˆœìœ„: ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì •
            'auto_detected.yaml',      # 2ìˆœìœ„: ìë™ í”„ë¡œë¹™ ê²°ê³¼
            'fallback_config.yaml'     # 3ìˆœìœ„: ì•ˆì „í•œ ê¸°ë³¸ê°’
        ]
        self.hardware_prober = HardwareProber()
        self.current_config = None
        
    def load_optimal_config(self):
        """ìµœì  ì„¤ì • ë¡œë“œ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)"""
        print("ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ì„¤ì • ê´€ë¦¬ ì‹œì‘...")
        
        # 1ë‹¨ê³„: ìˆ˜ë™ ì„¤ì • íŒŒì¼ í™•ì¸
        if self.exists_and_valid('manual_config.yaml'):
            print("âœ… ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì • ë°œê²¬ - ìµœìš°ì„  ì ìš©")
            self.current_config = self.load_yaml('manual_config.yaml')
            return self.current_config
            
        # 2ë‹¨ê³„: ìë™ í”„ë¡œë¹™ ì‹¤í–‰
        print("ğŸ” í•˜ë“œì›¨ì–´ ìë™ í”„ë¡œë¹™ ì‹¤í–‰ ì¤‘...")
        try:
            auto_config = self.hardware_prober.generate_optimal_config()
            self.save_yaml('auto_detected.yaml', auto_config)
            print("âœ… ìë™ í”„ë¡œë¹™ ì„±ê³µ - ê°ì§€ëœ ì„¤ì • ì ìš©")
            self.current_config = auto_config
            return self.current_config
        except Exception as e:
            print(f"âš ï¸ ìë™ í”„ë¡œë¹™ ì‹¤íŒ¨: {e}")
            
        # 3ë‹¨ê³„: ì•ˆì „í•œ ê¸°ë³¸ê°’ ì‚¬ìš©
        print("ğŸ›¡ï¸ ê¸°ë³¸ ì•ˆì „ ì„¤ì • ì ìš©")
        self.current_config = self.load_yaml('fallback_config.yaml')
        return self.current_config
        
    def allow_user_override(self, section, key, value):
        """ì‚¬ìš©ì ì„¤ì • ì¬ì •ì˜ í—ˆìš©"""
        override_config = {
            section: {key: value},
            'override_timestamp': datetime.now().isoformat(),
            'override_reason': f'User manual override for {section}.{key}'
        }
        
        # manual_config.yamlì— ì¶”ê°€
        if os.path.exists('manual_config.yaml'):
            existing_config = self.load_yaml('manual_config.yaml')
            existing_config.update(override_config)
        else:
            existing_config = override_config
            
        self.save_yaml('manual_config.yaml', existing_config)
        print(f"âœ… ì‚¬ìš©ì ì¬ì •ì˜ ì €ì¥: {section}.{key} = {value}")
        
    def get_setting(self, section, key, default=None):
        """ì„¤ì •ê°’ ì¡°íšŒ (ìš°ì„ ìˆœìœ„ ì ìš©)"""
        if self.current_config is None:
            self.load_optimal_config()
            
        return self.current_config.get(section, {}).get(key, default)

class HardwareProber:
    """í•˜ë“œì›¨ì–´ ìë™ í”„ë¡œë¹™ (í•„ìš”ì‹œì—ë§Œ ì‹¤í–‰)"""
    def __init__(self):
        self.probe_results = None
        
    def generate_optimal_config(self):
        """ìµœì  ì„¤ì • ìƒì„±"""
        # GPU ëŠ¥ë ¥ ì¸¡ì •
        gpu_info = self.probe_gpu_capabilities()
        
        # ìµœì í™”ëœ ì„¤ì • ìƒì„±
        optimal_config = {
            'hardware': gpu_info,
            'performance': {
                'max_concurrent_streams': self.calculate_optimal_streams(gpu_info),
                'batch_size_analyze': self.calculate_optimal_batch_size(gpu_info),
                'vram_safety_margin': 0.15,  # 15% ì•ˆì „ ë§ˆì§„
                'target_gpu_utilization': 0.85  # 85% ëª©í‘œ
            },
            'nvdec_settings': {
                'max_sessions': gpu_info['nvdec_max_sessions'],
                'preferred_format': 'nv12'
            },
            'nvenc_settings': {
                'max_sessions': gpu_info['nvenc_max_sessions'],
                'preset': 'medium',
                'rc_mode': 'cbr'
            },
            'generated_timestamp': datetime.now().isoformat(),
            'gpu_driver_version': gpu_info.get('driver_version'),
            'cuda_version': gpu_info.get('cuda_version')
        }
        
        return optimal_config
        
    def probe_gpu_capabilities(self):
        """GPU í•˜ë“œì›¨ì–´ ëŠ¥ë ¥ ì¸¡ì • (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        import pynvml
        pynvml.nvmlInit()
        
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        gpu_name = pynvml.nvmlDeviceGetName(handle).decode()
        
        # NVDEC/NVENC ì„¸ì…˜ í•œë„ í…ŒìŠ¤íŠ¸
        nvdec_sessions = self.test_concurrent_decoders()
        nvenc_sessions = self.test_concurrent_encoders()
        vram_total = pynvml.nvmlDeviceGetMemoryInfo(handle).total // (1024**3)
        
        # ë“œë¼ì´ë²„ ì •ë³´
        driver_version = pynvml.nvmlSystemGetDriverVersion().decode()
        
        return {
            'gpu_name': gpu_name,
            'nvdec_max_sessions': nvdec_sessions,
            'nvenc_max_sessions': nvenc_sessions,
            'vram_gb': vram_total,
            'driver_version': driver_version,
            'compute_capability': self.get_compute_capability(handle)
        }
        
    def calculate_optimal_streams(self, gpu_info):
        """ìµœì  ë™ì‹œ ìŠ¤íŠ¸ë¦¼ ìˆ˜ ê³„ì‚° (ë³´ìˆ˜ì  ì ‘ê·¼)"""
        # í•˜ë“œì›¨ì–´ ì œì•½ ê³ ë ¤
        nvdec_limit = gpu_info['nvdec_max_sessions']
        nvenc_limit = gpu_info['nvenc_max_sessions']
        vram_limit = max(1, (gpu_info['vram_gb'] * 0.75) // 8)  # VRAMì˜ 75%ë§Œ ì‚¬ìš©
        
        # ê°€ì¥ ì œí•œì ì¸ ìš”ì†Œì— ë§ì¶¤
        optimal_streams = min(nvdec_limit, nvenc_limit * 2, vram_limit, 4)
        
        print(f"ğŸ“Š ìµœì  ìŠ¤íŠ¸ë¦¼ ìˆ˜: {optimal_streams}ê°œ")
        print(f"   - NVDEC ì œí•œ: {nvdec_limit}ê°œ")
        print(f"   - NVENC ì œí•œ: {nvenc_limit}ê°œ (Ã—2 = {nvenc_limit*2})")
        print(f"   - VRAM ì œí•œ: {vram_limit}ê°œ")
        
        return optimal_streams
```

**ì„¤ì • íŒŒì¼ êµ¬ì¡° ì˜ˆì‹œ**:

```yaml
# manual_config.yaml (ì‚¬ìš©ì ìˆ˜ë™ ì„¤ì • - ìµœìš°ì„ )
hardware:
  gpu_name: "RTX 5090"
  nvdec_max_sessions: 4
  nvenc_max_sessions: 2

performance:
  max_concurrent_streams: 3  # ì‚¬ìš©ì ê°•ì œ ì œí•œ
  batch_size_analyze: 128    # ì„±ëŠ¥ vs ë©”ëª¨ë¦¬ trade-off
  vram_safety_margin: 0.2    # ë³´ìˆ˜ì  20% ë§ˆì§„

override_reason: "Production environment - conservative settings"
```

**ì‚¬ìš©ì ì¬ì •ì˜ ì˜ˆì‹œ**:

```python
# ëŸ°íƒ€ì„ì—ì„œ ì‚¬ìš©ìê°€ ì„¤ì • ë³€ê²½ ê°€ëŠ¥
config_manager = HybridConfigManager()

# ë°°ì¹˜ í¬ê¸° ìˆ˜ë™ ì¡°ì •
config_manager.allow_user_override('performance', 'batch_size_analyze', 64)

# ì•ˆì „ ë§ˆì§„ ì¦ê°€
config_manager.allow_user_override('performance', 'vram_safety_margin', 0.25)

# í˜„ì¬ ì„¤ì • í™•ì¸
current_batch_size = config_manager.get_setting('performance', 'batch_size_analyze', 256)
```

### ğŸ¦ **ë™ì  ë¦¬ì†ŒìŠ¤ í• ë‹¹í‘œ**

| êµ¬ì„±ìš”ì†Œ | ë™ì  ê°ì§€ ë°©ì‹ | í™œìš© ë°©ì‹ | ì˜ˆìƒ ì„±ëŠ¥ (ì˜ˆì‹œ) |
|----------|----------------|-----------|----------------------|
| **NVDEC** | ëŸ°íƒ€ì„ ì„¸ì…˜ í…ŒìŠ¤íŠ¸ | PyNvCodec/PyAV | **2-6ê°œ ì„¸ì…˜** (ëª¨ë¸ë³„ ê°€ë³€) |
| **NVENC** | ëŸ°íƒ€ì„ ì¸ì½”ë” í…ŒìŠ¤íŠ¸ | í•˜ë“œì›¨ì–´ ì¸ì½”ë”© | **1-3ê°œ ì„¸ì…˜** (ëª¨ë¸ë³„ ê°€ë³€) |
| **CUDA Cores** | ì»´í“¨íŠ¸ ëŠ¥ë ¥ ì¸¡ì • | TensorRT ë³‘ë ¬ ì¶”ë¡  | **SM í™œìš©ë¥  90%+** |
| **VRAM** | pynvml ë©”ëª¨ë¦¬ ì¡°íšŒ | ë™ì  ë°°ì¹˜ í¬ê¸° | **ì „ì²´ VRAMì˜ 87.5%** |
| **CPU** | ì½”ì–´ìˆ˜ ìë™ ê°ì§€ | ìŠ¤íŠ¸ë¦¼ ì œì–´, ì˜¤ë””ì˜¤ mux | **25% ì´í•˜ í™œìš©ë¥ ** |
| **System RAM** | psutil ë©”ëª¨ë¦¬ ì¡°íšŒ | í˜¸ìŠ¤íŠ¸ ë²„í¼, ë©”íƒ€ë°ì´í„° | **ì „ì²´ RAMì˜ 44%** |

### ğŸ“Š **VRAM ì‚¬ìš©ëŸ‰ ì •í™•í•œ ì‚°ì •**

| í•­ëª© | ìŠ¤íŠ¸ë¦¼ë‹¹ ì‚¬ìš©ëŸ‰ | 4ê°œ ìŠ¤íŠ¸ë¦¼ ì´í•© | ë¹„ê³  |
|------|----------------|----------------|------|
| **TensorRT ì—”ì§„** | 1.5-2GB | **6-8GB** | YOLOv8n/s ê¸°ì¤€ (x-faceëŠ” 3GB+) |
| **ë””ì½”ë“œ ë²„í¼** | 1-1.5GB | **4-6GB** | 1080p í”„ë ˆì„ ë²„í¼ë§ |
| **ì „ì²˜ë¦¬ ì›Œí¬ìŠ¤í˜ì´ìŠ¤** | 0.5GB | **2GB** | GPU ë¦¬ì‚¬ì´ì¦ˆ/ì •ê·œí™” |
| **ì¶”ë¡  ì›Œí¬ìŠ¤í˜ì´ìŠ¤** | 1GB | **4GB** | TensorRT ì‹¤í–‰ ê³µê°„ |
| **í•©ì„± ì›Œí¬ìŠ¤í˜ì´ìŠ¤** | 0.5GB | **2GB** | íƒ€ì¼ í•©ì„± ë²„í¼ |
| **NVENC ë²„í¼** | 0.5GB | **2GB** | ì¸ì½”ë”© ë²„í¼ |
| **ì—¬ìœ ë¶„/ì‹œìŠ¤í…œ** | - | **4GB** | CUDA ì»¨í…ìŠ¤íŠ¸, ê¸°íƒ€ |
| **ì´ VRAM ì‚¬ìš©ëŸ‰** | - | **24-28GB** | **32GB ì¤‘ 87.5% ì´í•˜** |

### ğŸ“Š **System RAM ì‚¬ìš©ëŸ‰**

| í•­ëª© | ì˜ˆìƒ ì‚¬ìš©ëŸ‰ | ë¹„ê³  |
|------|-------------|------|
| **Python í”„ë¡œì„¸ìŠ¤** | 2-4GB | ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ |
| **PyAV ë²„í¼** | 4-6GB | CPU ì¸¡ ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° |
| **ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼** | 1-2GB | ì›ë³¸ ì˜¤ë””ì˜¤ ë³´ì¡´ìš© |
| **ëª¨ë‹ˆí„°ë§/ë¡œê¹…** | 1GB | ì„±ëŠ¥ ì§€í‘œ, ë¡œê·¸ ë²„í¼ |
| **ì‹œìŠ¤í…œ ì—¬ìœ ë¶„** | 8GB | OS, ê¸°íƒ€ í”„ë¡œì„¸ìŠ¤ |
| **ì´ ì‚¬ìš©ëŸ‰** | **16-21GB** | **48GB ì¤‘ 44% ì´í•˜** |

---

## ğŸ§  í•µì‹¬ ê¸°ëŠ¥

### 1. ë¹„ë””ì˜¤ ë””ì½”ë”© (**PyAV ì œë¡œì¹´í”¼ ì‹¤ì „ ì„¤ì •**)

**PyNvCodec(VPF) í‘œì¤€ ì œë¡œì¹´í”¼ êµ¬í˜„** (ê¶Œì¥ ì†”ë£¨ì…˜):
```python
import PyNvCodec as nvc
import torch
import cupy as cp

def setup_pynvcodec_decoder(video_path, device_id=0):
    """PyNvCodec ê¸°ë°˜ ì§„ì§œ ì œë¡œì¹´í”¼ ë””ì½”ë” ì„¤ì •"""
    try:
        # PyNvCodec ë””ì½”ë” ìƒì„±
        decoder = nvc.PyDecodeHW(
            video_path,
            nvc.PixelFormat.NV12,  # GPU ë„¤ì´í‹°ë¸Œ í¬ë§·
            device_id
        )
        
        # ìƒ‰ê³µê°„ ë³€í™˜ê¸° (GPU ë‚´ë¶€ ì²˜ë¦¬)
        converter = nvc.PySurfaceConverter(
            decoder.Width(), decoder.Height(),
            nvc.PixelFormat.NV12, nvc.PixelFormat.RGB,
            device_id
        )
        
        return decoder, converter
        
    except Exception as e:
        print(f"PyNvCodec ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None

def decode_gpu_frames_zerocopy(decoder, converter):
    """ì§„ì§œ ì œë¡œì¹´í”¼ GPU í”„ë ˆì„ ë””ì½”ë”©"""
    while True:
        # GPU ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ NV12 ë””ì½”ë”©
        nv12_surface = decoder.DecodeSurface()
        if not nv12_surface:
            break
            
        # GPU ë‚´ë¶€ ìƒ‰ê³µê°„ ë³€í™˜ (NV12â†’RGB)
        rgb_surface = converter.Execute(nv12_surface)
        
        # DLPackì„ í†µí•œ ì œë¡œì¹´í”¼ í…ì„œ ë³€í™˜
        dlpack_tensor = rgb_surface.GetDLPackTensor()
        gpu_tensor = torch.from_dlpack(dlpack_tensor)
        
        yield gpu_tensor

# PyAV ë°±ì—… êµ¬í˜„ (hw_frames_ctx ëª…ì‹œì  ì„¤ì •)
def setup_pyav_backup(video_path, device_id=0):
    """PyAV ë°±ì—… êµ¬í˜„ - hw_frames_ctx ëª…ì‹œì  ì„¤ì •"""
    import av
    
    container = av.open(video_path)
    stream = container.streams.video[0]
    
    # í•˜ë“œì›¨ì–´ ë””ë°”ì´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    hw_device = av.cuda.Device(device_id)
    
    # ì½”ë± ì»¨í…ìŠ¤íŠ¸ì— í•˜ë“œì›¨ì–´ ì„¤ì •
    stream.codec_context.hw_device_ctx = hw_device
    stream.codec_context.options['hwaccel'] = 'cuda'
    stream.codec_context.options['hwaccel_output_format'] = 'cuda'
    
    # hw_frames_ctx ëª…ì‹œì  ì„¤ì •
    stream.codec_context.hw_frames_ctx = hw_device.create_hwframes_ctx(
        format='nv12',
        width=stream.width,
        height=stream.height
    )
    
    return container, stream
```

**ë””ì½”ë” ì„ íƒ ì „ëµ** (ìš°ì„ ìˆœìœ„ ê¸°ë°˜):
```python
class DecoderSelector:
    def __init__(self):
        self.decoder_priority = [
            ('pynvcodec', self.try_pynvcodec),
            ('pyav_hwframes', self.try_pyav_hwframes),
            ('pyav_basic', self.try_pyav_basic),
            ('cpu_fallback', self.try_cpu_fallback)
        ]
    
    def select_best_decoder(self, video_path, device_id=0):
        """ìµœì  ë””ì½”ë” ìë™ ì„ íƒ"""
        video_info = self.probe_video(video_path)
        
        for decoder_name, decoder_func in self.decoder_priority:
            try:
                decoder = decoder_func(video_path, device_id)
                if self.validate_decoder(decoder, video_info):
                    print(f"âœ… {decoder_name} ë””ì½”ë” ì„ íƒë¨")
                    return decoder
            except Exception as e:
                print(f"âš ï¸ {decoder_name} ì‹¤íŒ¨: {e}")
                continue
        
        raise RuntimeError("ëª¨ë“  ë””ì½”ë” ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    def should_prefer_pynvcodec(self, video_info):
        """PyNvCodec ìš°ì„  ì„ íƒ ì¡°ê±´"""
        conditions = [
            video_info.get('codec') in ['h264', 'hevc'],  # ì§€ì› ì½”ë±
            video_info.get('width', 0) * video_info.get('height', 0) > 2073600,  # 1080p ì´ìƒ
            video_info.get('bitrate', 0) > 10_000_000,  # 10Mbps ì´ìƒ
        ]
        return sum(conditions) >= 2  # 2ê°œ ì´ìƒ ì¡°ê±´ ë§Œì¡±
```

**ì „í™˜ ì „ëµ**:
- **1ìˆœìœ„**: PyNvCodec (VPF) - ê³ ì„±ëŠ¥, ì™„ì „ ì œë¡œì¹´í”¼
- **2ìˆœìœ„**: PyAV + hw_frames_ctx - í˜¸í™˜ì„± ì¢‹ìŒ
- **3ìˆœìœ„**: PyAV ê¸°ë³¸ hwaccel - ìµœì†Œ ê¸°ëŠ¥
- **4ìˆœìœ„**: CPU ë””ì½”ë”© - ìµœí›„ ë°±ì—…

**ëŸ°íƒ€ì„ í•˜ë“œì›¨ì–´ í”„ë¡œë¹™ í†µí•©**:

ìœ„ `HardwareProber` í´ë˜ìŠ¤ê°€ ëª¨ë“  í•˜ë“œì›¨ì–´ ê°ì§€ë¥¼ ë‹´ë‹¹í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤:

- âœ… **ëª¨ë¸ë³„ ì ì‘**: RTX 4090, RTX 5090, A100 ë“± ìë™ ìµœì í™”
- âœ… **ë“œë¼ì´ë²„ ë²„ì „ ëŒ€ì‘**: NVDEC/NVENC ì„¸ì…˜ í•œë„ ì‹¤ì‹œê°„ ì¸¡ì •  
- âœ… **ì•ˆì „í•œ ë°±ì—…**: ì¸¡ì • ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì  ê¸°ë³¸ê°’ ì‚¬ìš©
- âœ… **ë™ì  í ì„¤ì •**: ì¸¡ì •ëœ í•œë„ì— ë§ì¶° ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •

### 2. ì–¼êµ´ ê²€ì¶œ (**ëª¨ë¸ í›„ë³´êµ° í™•ëŒ€ + ë²¤ì¹˜ë§ˆí‚¹**)

**ëª¨ë¸ í›„ë³´êµ° ë° ë²¤ì¹˜ë§ˆí‚¹ ê³„íš**:

| ëª¨ë¸ | í¬ê¸° | ì˜ˆìƒ FPS | mAP | VRAM | TensorRT ì§€ì› | ìš°ì„ ìˆœìœ„ |
|------|------|---------|-----|------|---------------|----------|
| **YOLOv8n-face** | 6MB | **120+ FPS** | 85% | **1GB** | âœ… | **1ìˆœìœ„** |
| **YOLOv8s-face** | 22MB | **80+ FPS** | 88% | **1.5GB** | âœ… | **2ìˆœìœ„** |
| **SCRFD-2.5G** | 10MB | **100+ FPS** | 87% | **1.2GB** | âœ… | **3ìˆœìœ„** |
| **YOLOv8x-face** | 136MB | **40+ FPS** | 92% | **3GB** | âœ… | ë°±ì—…ìš© |
| **RT-DETR-S** | 20MB | **60+ FPS** | 89% | **1.8GB** | âœ… | ì‹¤í—˜ìš© |

**TensorRT ìµœì í™”**:
- **FP16 ìš°ì„ **: mAP ì†ì‹¤ < 3%, ì†ë„ 1.5-2ë°° í–¥ìƒ
#### ğŸ¯ **INT8 ì±„íƒ ê¸°ì¤€ êµ¬ì²´í™”** (ë“±ê¸‰ ê°œë…)

```python
# INT8 ì±„íƒ ë“±ê¸‰ ì‹œìŠ¤í…œ
class INT8AdoptionCriteria:
    def __init__(self):
        # í•„ìˆ˜ ì„±ëŠ¥ ì„ê³„ê°’
        self.precision_threshold = 1.5      # mAP ì†ì‹¤ â‰¤ 1.5%p
        self.miss_rate_threshold = 0.5      # ë¯¸ìŠ¤ìœ¨ ì¦ê°€ â‰¤ 0.5%p
        
        # êµì • ë°ì´í„°ì…‹ êµ¬ì„±
        self.calibration_spec = {
            'total_videos': 10,
            'duration_per_video': 600,  # 10ë¶„
            'scene_conditions': [
                'normal_lighting',    # ì¼ë°˜ ì¡°ëª…
                'low_lighting',      # ì €ì¡°ë„
                'backlight',         # ì—­ê´‘
                'face_occlusion',    # ê°€ë¦¼
                'motion_blur',       # ë™ì‘ ë¸”ëŸ¬
                'side_profile'       # ì¸¡ë©´ í”„ë¡œí•„
            ]
        }
        
        # ì•„í‹°íŒ©íŠ¸ ë° ì¬í˜„ì„± ë³´ì¥
        self.artifact_config = {
            'engine_hash_logging': True,        # TensorRT ì—”ì§„ í•´ì‹œ ë³´ê´€
            'calibration_log_retention': 90,    # êµì • ë¡œê·¸ 90ì¼ ë³´ê´€
            'reproducibility_seed': 42,         # ì¬í˜„ ê°€ëŠ¥ì„± ì‹œë“œ
            'benchmark_report_archive': True    # ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì•„ì¹´ì´ë¸Œ
        }
    
    def evaluate_int8_model(self, fp16_model, int8_model, test_dataset):
        """ì¢…í•©ì  INT8 ëª¨ë¸ í‰ê°€"""
        # 1. ì„±ëŠ¥ ë¹„êµ ë¹¤ì¹˜ë§ˆí¬
        fp16_metrics = self.comprehensive_benchmark(fp16_model, test_dataset)
        int8_metrics = self.comprehensive_benchmark(int8_model, test_dataset)
        
        # 2. ì£¼ìš” ì§€í‘œ ë¹„êµ
        precision_loss = fp16_metrics['mAP'] - int8_metrics['mAP']
        miss_rate_increase = int8_metrics['miss_rate'] - fp16_metrics['miss_rate']
        speed_improvement = int8_metrics['fps'] / fp16_metrics['fps']
        
        # 3. ì¢…í•© í‰ê°€
        evaluation_result = {
            'precision_loss_pct': precision_loss,
            'miss_rate_increase_pct': miss_rate_increase,
            'speed_improvement_ratio': speed_improvement,
            'meets_criteria': self.check_adoption_criteria(precision_loss, miss_rate_increase),
            'recommendation': self.generate_recommendation(precision_loss, miss_rate_increase, speed_improvement),
            'detailed_breakdown': self.analyze_by_scene_condition(int8_metrics, fp16_metrics)
        }
        
        # 4. ì•„í‹°íŒ©íŠ¸ ìƒì„±
        self.generate_artifacts(evaluation_result, fp16_model, int8_model)
        
        return evaluation_result
    
    def check_adoption_criteria(self, precision_loss, miss_rate_increase):
        """ê¸°ë³¸ ì±„íƒ ê¸°ì¤€ ê²€ì¦"""
        return (precision_loss <= self.precision_threshold and 
                miss_rate_increase <= self.miss_rate_threshold)
    
    def generate_recommendation(self, precision_loss, miss_rate_increase, speed_improvement):
        """ì±„íƒ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        if precision_loss <= 0.8 and miss_rate_increase <= 0.2:  # ìš°ìˆ˜
            return {
                'grade': 'EXCELLENT',
                'action': 'ADOPT_IMMEDIATELY',
                'reason': f'ì„±ëŠ¥ ì†ì‹¤ ìµœì†Œ({precision_loss:.2f}%p), ì†ë„ í–¥ìƒ {speed_improvement:.1f}ë°°'
            }
        elif self.check_adoption_criteria(precision_loss, miss_rate_increase):
            return {
                'grade': 'ACCEPTABLE',
                'action': 'ADOPT_WITH_MONITORING',
                'reason': f'ê¸°ì¤€ ì¶©ì¡±, ì†ë„ í–¥ìƒ {speed_improvement:.1f}ë°°, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í•„ìš”'
            }
        else:
            return {
                'grade': 'NOT_RECOMMENDED',
                'action': 'USE_FP16',
                'reason': f'ì„±ëŠ¥ ì†ì‹¤ ê³¼ë‹¤({precision_loss:.2f}%p > {self.precision_threshold}%p)'
            }
```

#### ğŸ“‹ **INT8 êµì • í”„ë¡œì„¸ìŠ¤**

```bash
# INT8 êµì • ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
echo "ğŸ¯ INT8 êµì • ë° ê²€ì¦ ì‹œì‘"

# 1. êµì • ë°ì´í„° ì¤€ë¹„
python prepare_calibration_dataset.py \
    --videos 10 \
    --duration 600 \
    --conditions "normal,low_light,backlight,occlusion,blur,profile"

# 2. FP16 ë² ì´ìŠ¤ë¼ì¸ ë²¤ì¹˜ë§ˆí¬
python benchmark_fp16.py \
    --model yolov8n-face \
    --dataset calibration_set \
    --output fp16_baseline.json

# 3. INT8 êµì • ë° ë²¤ì¹˜ë§ˆí¬
python calibrate_int8.py \
    --fp16-model yolov8n-face.onnx \
    --calibration-data calibration_set \
    --output yolov8n-face-int8.trt

python benchmark_int8.py \
    --model yolov8n-face-int8.trt \
    --dataset calibration_set \
    --output int8_results.json

# 4. ê²€ì¦ ë° ë³´ê³ ì„œ ìƒì„±
python evaluate_int8_adoption.py \
    --fp16-results fp16_baseline.json \
    --int8-results int8_results.json \
    --output-report int8_evaluation_report.html

echo "âœ… INT8 ê²€ì¦ ì™„ë£Œ: int8_evaluation_report.html í™•ì¸"
```
#### âš¡ **TensorRT ìµœì í™” ì˜µì…˜**

- **FP16 ê¸°ë³¸ ì „ëµ**: mAP ì†ì‹¤ < 3%, ì†ë„ 1.5-2ë°° í–¥ìƒ
- **INT8 ì„ íƒì  ì ìš©**: ìœ„ ê¸°ì¤€ ì¶œì¡± ì‹œì—ë§Œ ì‚¬ìš©
- **CUDA Graphs**: Launch overhead 90% ì ˆê°
- **Dynamic Shape ìµœì í™”**: ë°°ì¹˜ í¬ê¸° ê°€ë³€ì„± ì§€ì›

**ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”** (Tail Latency ë°©ì§€):
```python
class BatchFlusher:
    def __init__(self):
        self.max_batch = 8          # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
        self.max_wait_ms = 6        # ìµœëŒ€ ëŒ€ê¸° ì‹œê°„
        self.tick_ms = 4            # ì£¼ê¸°ì  flush ê°„ê²©
        self.max_jitter_ms = 2      # ì§€í„° í—ˆìš© ë²”ìœ„
        
        self.pending_frames = []
        self.stream_fairness = {}   # WFQ ê³µì •ì„± ê´€ë¦¬
        
    def add_frame(self, frame, stream_id, timestamp):
        """í”„ë ˆì„ì„ ë°°ì¹˜ì— ì¶”ê°€"""
        self.pending_frames.append({
            'frame': frame,
            'stream_id': stream_id, 
            'timestamp': timestamp,
            'wait_time': 0
        })
        
        # WFQ: ëŠë¦° ìŠ¤íŠ¸ë¦¼ ìš°ì„ ìˆœìœ„ ì¦ê°€
        if stream_id not in self.stream_fairness:
            self.stream_fairness[stream_id] = {'priority': 1.0, 'processed': 0}
            
        # Flush ì¡°ê±´ ì²´í¬
        if self.should_flush():
            return self.flush_batch()
        return None
        
    def should_flush(self):
        """ë°°ì¹˜ flush ì¡°ê±´"""
        if len(self.pending_frames) >= self.max_batch:
            return True
            
        if self.pending_frames:
            oldest_frame = min(self.pending_frames, key=lambda x: x['timestamp'])
            wait_time = time.time() - oldest_frame['timestamp']
            if wait_time >= self.max_wait_ms / 1000:
                return True
                
        return False
        
    def flush_batch(self):
        """WFQ ê¸°ë°˜ ê³µì •í•œ ë°°ì¹˜ êµ¬ì„±"""
        if not self.pending_frames:
            return None
            
        # ìŠ¤íŠ¸ë¦¼ë³„ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬
        sorted_frames = sorted(self.pending_frames, 
                              key=lambda x: -self.stream_fairness[x['stream_id']]['priority'])
        
        # ë°°ì¹˜ êµ¬ì„± (ê³µì •ì„± ê³ ë ¤)
        batch = sorted_frames[:self.max_batch]
        self.pending_frames = sorted_frames[self.max_batch:]
        
        # ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ëœ ìŠ¤íŠ¸ë¦¼ì€ ìš°ì„ ìˆœìœ„ ê°ì†Œ)
        for frame in batch:
            stream_id = frame['stream_id']
            self.stream_fairness[stream_id]['processed'] += 1
            self.stream_fairness[stream_id]['priority'] *= 0.9  # ì²˜ë¦¬ í›„ ìš°ì„ ìˆœìœ„ ê°ì†Œ
            
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìŠ¤íŠ¸ë¦¼ì€ ìš°ì„ ìˆœìœ„ ì¦ê°€
        for frame in self.pending_frames:
            stream_id = frame['stream_id']
            self.stream_fairness[stream_id]['priority'] *= 1.1
            
        return [f['frame'] for f in batch]
```

### 3. ì¡°ê±´ë¶€ Re-ID ì–¼êµ´ ì¶”ì  ì‹œìŠ¤í…œ (**ì§€ëŠ¥í˜• ID ì•ˆì •ì„±**)

**í•µì‹¬ í˜ì‹ **: ID ìŠ¤ì™‘ì´ ì˜ì‹¬ë˜ëŠ” **ê²°ì •ì  ìˆœê°„ì—ë§Œ** ê²½ëŸ‰ Re-ID ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ê²€ì¦

**ConditionalReID ì•„í‚¤í…ì²˜**:

```python
class ConditionalReID:
    def __init__(self):
        self.base_tracker = ByteTrack()           # ê¸°ë³¸: ê²½ëŸ‰ IoU ì¶”ì 
        self.reid_model = None                    # 128-D ê²½ëŸ‰ ReID (í•„ìš”ì‹œ ë¡œë“œ)
        self.id_swap_threshold = 0.3              # ID ìŠ¤ì™‘ ì˜ì‹¬ ì„ê³„ê°’
        self.confidence_history = {}              # IDë³„ ì‹ ë¢°ë„ íˆìŠ¤í† ë¦¬
        self.position_jump_threshold = 200        # ê¸‰ê²©í•œ ìœ„ì¹˜ ë³€í™” ì„ê³„ê°’
        
    def should_activate_reid(self, track_id, current_detection):
        """ID ìŠ¤ì™‘ ì˜ì‹¬ ìƒí™© ê°ì§€"""
        # 1. ì¶”ì  ì‹ ë¢°ë„ ê¸‰ë½
        confidence = self.base_tracker.get_confidence(track_id)
        if confidence < self.id_swap_threshold:
            return True
            
        # 2. ê¸‰ê²©í•œ ìœ„ì¹˜ ì í”„
        if track_id in self.last_positions:
            position_jump = self.calculate_position_jump(
                track_id, current_detection.bbox_center
            )
            if position_jump > self.position_jump_threshold:
                return True
                
        # 3. ì–¼êµ´ íŠ¹ì„± ê¸‰ë³€ (í¬ê¸°, ê°ë„)
        if self.detect_face_feature_anomaly(track_id, current_detection):
            return True
            
        return False
        
    def conditional_track(self, detections):
        """ì¡°ê±´ë¶€ Re-ID ì¶”ì """
        # 1. ê¸°ë³¸ ByteTrack ìˆ˜í–‰
        base_tracks = self.base_tracker.update(detections)
        
        # 2. ID ìŠ¤ì™‘ ì˜ì‹¬ ì¼€ì´ìŠ¤ ê²€ì¶œ
        suspicious_tracks = []
        for track in base_tracks:
            if self.should_activate_reid(track.track_id, track):
                suspicious_tracks.append(track)
                
        # 3. ì˜ì‹¬ ì¼€ì´ìŠ¤ì—ë§Œ ReID ì ìš©
        if suspicious_tracks and self.reid_model is None:
            self.reid_model = self.load_lightweight_reid()  # í•„ìš”ì‹œì—ë§Œ ë¡œë“œ
            
        verified_tracks = []
        for track in base_tracks:
            if track in suspicious_tracks:
                # ReIDë¡œ ì¬ê²€ì¦
                verified_track = self.verify_with_reid(track, detections)
                verified_tracks.append(verified_track)
            else:
                # ì˜ì‹¬ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                verified_tracks.append(track)
                
        return verified_tracks
        
    def verify_with_reid(self, suspicious_track, all_detections):
        """ReID ê¸°ë°˜ ID ì¬ê²€ì¦"""
        track_embedding = self.extract_reid_embedding(suspicious_track.detection)
        
        # ê¸°ì¡´ ID íˆìŠ¤í† ë¦¬ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        best_match_id = None
        best_similarity = 0
        
        for existing_id in self.embedding_history:
            similarity = self.cosine_similarity(
                track_embedding, 
                self.embedding_history[existing_id]
            )
            if similarity > best_similarity and similarity > 0.7:
                best_match_id = existing_id
                best_similarity = similarity
                
        # ID ì¬í• ë‹¹ ë˜ëŠ” ìœ ì§€
        if best_match_id and best_match_id != suspicious_track.track_id:
            print(f"ğŸ”„ ID ìˆ˜ì •: {suspicious_track.track_id} â†’ {best_match_id}")
            suspicious_track.track_id = best_match_id
            
        return suspicious_track
        
    def load_lightweight_reid(self):
        """ê²½ëŸ‰ 128-D ReID ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œì—ë§Œ)"""
        # MobileNet ê¸°ë°˜ < 50MB ê²½ëŸ‰ ëª¨ë¸
        import torchvision.models as models
        reid_model = models.mobilenet_v3_small(pretrained=True)
        reid_model.classifier = torch.nn.Linear(reid_model.classifier[0].in_features, 128)
        return reid_model.eval()
```

**ì„±ëŠ¥ ìµœì í™” ì „ëµ**:

1. **ê¸°ë³¸ ëª¨ë“œ**: ByteTrackë§Œ ì‚¬ìš© (GPU ì‚¬ìš©ëŸ‰ ìµœì†Œ)
2. **ì˜ì‹¬ ê°ì§€**: íŠ¹ì • ìƒí™©ì—ì„œë§Œ ReID í™œì„±í™”
3. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ReID ëª¨ë¸ì„ í•„ìš”ì‹œì—ë§Œ ë¡œë“œ
4. **ì„ë² ë”© ìºì‹œ**: ìµœê·¼ Ní”„ë ˆì„ì˜ ì„ë² ë”©ë§Œ GPU ë©”ëª¨ë¦¬ì— ë³´ê´€

**ID ì•ˆì •ì„± ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜**:

```python
class IDStabilityManager:
    def __init__(self):
        self.id_confidence_buffer = {}   # IDë³„ ì‹ ë¢°ë„ ë²„í¼
        self.min_stable_frames = 10      # ì•ˆì •í™” ìµœì†Œ í”„ë ˆì„
        self.max_unstable_frames = 5     # ë¶ˆì•ˆì • í—ˆìš© ìµœëŒ€ í”„ë ˆì„
        
    def is_id_stable(self, track_id):
        """ID ì•ˆì •ì„± íŒë‹¨"""
        if track_id not in self.id_confidence_buffer:
            return False
            
        recent_confidences = self.id_confidence_buffer[track_id][-10:]
        stable_count = sum(1 for conf in recent_confidences if conf > 0.7)
        
        return stable_count >= self.min_stable_frames
        
    def update_id_confidence(self, track_id, confidence):
        """ID ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸"""
        if track_id not in self.id_confidence_buffer:
            self.id_confidence_buffer[track_id] = []
            
        self.id_confidence_buffer[track_id].append(confidence)
        
        # ìµœëŒ€ 30í”„ë ˆì„ë§Œ ìœ ì§€
        if len(self.id_confidence_buffer[track_id]) > 30:
            self.id_confidence_buffer[track_id].pop(0)
```

**íŠ¸ë˜í‚¹ ì •í™•ë„ vs ì„±ëŠ¥ trade-off**:
- **ì¼ë°˜ ìƒí™©**: ByteTrackë§Œ ì‚¬ìš© â†’ **ë†’ì€ ì„±ëŠ¥**
- **ì˜ì‹¬ ìƒí™©**: ReID ì¶”ê°€ í™œì„±í™” â†’ **ë†’ì€ ì •í™•ë„**
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: í‰ìƒì‹œ ìµœì†Œ, í•„ìš”ì‹œì—ë§Œ ì¦ê°€

### 4. ì¢Œìš° ë¶„ê¸° ë¡œì§ (**ì•ˆì •ì„± ëŒ€í­ ê°•í™”**)

**ê°œì„ ëœ ë¶„ê¸° ì•Œê³ ë¦¬ì¦˜** (EMA + ì²´ë¥˜ì‹œê°„ + íˆìŠ¤í…Œë¦¬ì‹œìŠ¤):

```python
class StablePersonAssigner:
    def __init__(self):
        self.position_history = {}  # IDë³„ ìœ„ì¹˜ íˆìŠ¤í† ë¦¬
        self.residence_time = {}    # IDë³„ ì²´ë¥˜ì‹œê°„
        self.assignment = {}        # IDë³„ í˜„ì¬ í• ë‹¹ (left/right)
        
    def assign_person(self, track_id, bbox_center_x, frame_width):
        # 1. ìœ„ì¹˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (EMA)
        if track_id not in self.position_history:
            self.position_history[track_id] = []
        
        self.position_history[track_id].append(bbox_center_x)
        if len(self.position_history[track_id]) > 30:  # ìµœê·¼ 30í”„ë ˆì„
            self.position_history[track_id].pop(0)
            
        # 2. EMA ê³„ì‚° (ì§€ìˆ˜ ì´ë™ í‰ê· )
        ema_position = self.calculate_ema(self.position_history[track_id])
        
        # 3. ì²´ë¥˜ì‹œê°„ ìš°ì„  ê·œì¹™
        frame_center = frame_width / 2
        current_side = "left" if ema_position < frame_center else "right"
        
        # 4. íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ (ìµœì†Œ Ní”„ë ˆì„ ìœ ì§€)
        if track_id in self.assignment:
            if self.residence_time[track_id] < 15:  # 15í”„ë ˆì„ ìµœì†Œ ìœ ì§€
                current_side = self.assignment[track_id]  # ê¸°ì¡´ í• ë‹¹ ìœ ì§€
            else:
                # ì¶©ë¶„í•œ ì²´ë¥˜ì‹œê°„ í›„ ì¬í• ë‹¹ í—ˆìš©
                margin = frame_width * 0.1  # 10% ì—¬ìœ ë¶„
                if abs(ema_position - frame_center) > margin:
                    self.residence_time[track_id] = 0  # ì¬í• ë‹¹ ì‹œ ì‹œê°„ ë¦¬ì…‹
                    
        self.assignment[track_id] = current_side
        self.residence_time[track_id] = self.residence_time.get(track_id, 0) + 1
        
        return current_side
```

### 5. ì˜ìƒ í•©ì„± ë° ì¶œë ¥ (**CPU â†’ GPU ì™„ì „ ì „í™˜**)

**Full GPU í•©ì„± íŒŒì´í”„ë¼ì¸**:
```python
# ê¸°ì¡´ (ë³‘ëª©): GPU â†’ CPU â†’ GPU
gpu_frame â†’ cpu_resize() â†’ cpu_tile_compose() â†’ gpu_encode()

# ì‹ ê·œ (ì œë¡œì¹´í”¼): GPU ë‚´ì—ì„œ ì™„ê²°  
gpu_frame â†’ cuda_resize() â†’ cuda_tile_compose() â†’ nvenc_encode()
```

**CUDA í•©ì„± êµ¬í˜„**:
- **GPU ë¦¬ì‚¬ì´ì¦ˆ**: `cv2.cuda.resize()` ì‚¬ìš©
- **íƒ€ì¼ í•©ì„±**: CUDA kernel ì§ì ‘ êµ¬í˜„ or `cv2.cuda.copyMakeBorder()`
- **NVENC ì§ì ‘ ì—°ê²°**: í•©ì„±ëœ í”„ë ˆì„ì„ ë°”ë¡œ NVENCë¡œ

### ğŸ›¡ï¸ **íƒ€ì¼ í•©ì„± ì—ëŸ¬ ì²˜ë¦¬ ì •ì±…** (ë‹¨ì¼ ì‹¤íŒ¨ì  ë°©ì§€)

**í•µì‹¬ ë¬¸ì œ**: 4ê°œ ìŠ¤íŠ¸ë¦¼ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ **ì „ì²´ íƒ€ì¼ í•©ì„±ì´ ë¶ˆê°€ëŠ¥**

**TileCompositionErrorPolicy ì•„í‚¤í…ì²˜**:

```python
class TileCompositionErrorPolicy:
    def __init__(self):
        self.failure_strategies = {
            'critical_stream': self.handle_critical_stream_failure,
            'normal_stream': self.handle_normal_stream_failure,
            'multiple_streams': self.handle_multiple_stream_failures
        }
        self.black_frame_cache = {}  # í•´ìƒë„ë³„ ë¸”ë™ í”„ë ˆì„ ìºì‹œ
        self.last_good_frames = {}   # ìŠ¤íŠ¸ë¦¼ë³„ ë§ˆì§€ë§‰ ì •ìƒ í”„ë ˆì„
        
    def handle_stream_failure(self, failed_streams, all_streams, batch_id):
        """ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ ì •ì±…"""
        failure_count = len(failed_streams)
        total_streams = len(all_streams)
        
        print(f"âš ï¸ ë°°ì¹˜ {batch_id}: {failure_count}/{total_streams} ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨")
        
        # 1. ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜
        if failure_count >= total_streams * 0.5:  # 50% ì´ìƒ ì‹¤íŒ¨
            return self.handle_multiple_stream_failures(failed_streams, all_streams, batch_id)
        elif self.is_critical_stream(failed_streams[0]):
            return self.handle_critical_stream_failure(failed_streams[0], all_streams, batch_id)
        else:
            return self.handle_normal_stream_failure(failed_streams[0], all_streams, batch_id)
            
    def handle_critical_stream_failure(self, failed_stream_id, all_streams, batch_id):
        """ì¤‘ìš” ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ì‹œ - ì „ì²´ ë°°ì¹˜ ìŠ¤í‚µ"""
        print(f"âŒ ì¤‘ìš” ìŠ¤íŠ¸ë¦¼ {failed_stream_id} ì‹¤íŒ¨ - ë°°ì¹˜ {batch_id} ìŠ¤í‚µ")
        
        # ì‹¤íŒ¨ ì›ì¸ ë¡œê¹…
        failure_reason = self.diagnose_failure(failed_stream_id)
        self.log_failure_analytics(failed_stream_id, failure_reason, batch_id)
        
        # ì „ì²´ ë°°ì¹˜ ìŠ¤í‚µ ì²˜ë¦¬
        self.increment_skip_counter(batch_id)
        return {
            'action': 'skip_batch',
            'reason': f'Critical stream {failed_stream_id} failure: {failure_reason}',
            'affected_streams': all_streams,
            'recovery_strategy': 'wait_for_next_batch'
        }
        
    def handle_normal_stream_failure(self, failed_stream_id, all_streams, batch_id):
        """ì¼ë°˜ ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ì‹œ - ëŒ€ì²´ í”„ë ˆì„ ì‚¬ìš©"""
        print(f"ğŸ”„ ì¼ë°˜ ìŠ¤íŠ¸ë¦¼ {failed_stream_id} ì‹¤íŒ¨ - ëŒ€ì²´ í”„ë ˆì„ ì ìš©")
        
        replacement_strategy = self.select_replacement_strategy(failed_stream_id)
        
        if replacement_strategy == 'last_good_frame':
            replacement_frame = self.get_last_good_frame(failed_stream_id)
        elif replacement_strategy == 'black_frame':
            replacement_frame = self.generate_black_frame(failed_stream_id)
        else:  # 'interpolated_frame'
            replacement_frame = self.interpolate_frame(failed_stream_id)
            
        # íƒ€ì¼ í•©ì„± ê³„ì† ì§„í–‰
        return {
            'action': 'continue_with_replacement',
            'failed_stream_id': failed_stream_id,
            'replacement_frame': replacement_frame,
            'replacement_type': replacement_strategy,
            'warning_logged': True
        }
        
    def handle_multiple_stream_failures(self, failed_streams, all_streams, batch_id):
        """ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ì‹œ - ê¸´ê¸‰ ì²˜ë¦¬"""
        print(f"ğŸš¨ ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ ì‹¤íŒ¨ ({len(failed_streams)}/{len(all_streams)}) - ê¸´ê¸‰ ì²˜ë¦¬")
        
        # ì‹œìŠ¤í…œ ì•ˆì •ì„± ìš°ì„ 
        if len(failed_streams) >= 3:
            return {
                'action': 'emergency_shutdown',
                'reason': 'Multiple critical failures detected',
                'failed_streams': failed_streams,
                'recovery_strategy': 'restart_pipeline'
            }
        else:
            # ë¶€ë¶„ ë³µêµ¬ ì‹œë„
            return self.attempt_partial_recovery(failed_streams, all_streams, batch_id)
            
    def select_replacement_strategy(self, failed_stream_id):
        """ëŒ€ì²´ ì „ëµ ì„ íƒ"""
        # ì‹¤íŒ¨ ë¹ˆë„ì— ë”°ë¥¸ ì „ëµ
        failure_history = self.get_failure_history(failed_stream_id)
        
        if failure_history['consecutive_failures'] < 3:
            return 'last_good_frame'  # ìµœê·¼ ì •ìƒ í”„ë ˆì„ ì‚¬ìš©
        elif failure_history['total_failures'] < 10:
            return 'interpolated_frame'  # ë³´ê°„ í”„ë ˆì„ ìƒì„±
        else:
            return 'black_frame'  # ë¸”ë™ í”„ë ˆì„ (ìµœí›„ ìˆ˜ë‹¨)
            
    def generate_black_frame(self, stream_id):
        """í•´ìƒë„ë³„ ë¸”ë™ í”„ë ˆì„ ìƒì„± (ìºì‹œ í™œìš©)"""
        stream_resolution = self.get_stream_resolution(stream_id)
        
        cache_key = f"{stream_resolution['width']}x{stream_resolution['height']}"
        if cache_key not in self.black_frame_cache:
            # GPUì—ì„œ ë¸”ë™ í”„ë ˆì„ ìƒì„±
            black_frame = torch.zeros(
                (3, stream_resolution['height'], stream_resolution['width']),
                dtype=torch.uint8,
                device='cuda'
            )
            self.black_frame_cache[cache_key] = black_frame
            
        return self.black_frame_cache[cache_key].clone()
        
    def get_last_good_frame(self, stream_id):
        """ë§ˆì§€ë§‰ ì •ìƒ í”„ë ˆì„ ë°˜í™˜"""
        if stream_id in self.last_good_frames:
            return self.last_good_frames[stream_id].clone()
        else:
            # ì •ìƒ í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ë¸”ë™ í”„ë ˆì„
            return self.generate_black_frame(stream_id)
            
    def update_last_good_frame(self, stream_id, frame):
        """ì •ìƒ í”„ë ˆì„ ì—…ë°ì´íŠ¸"""
        self.last_good_frames[stream_id] = frame.clone()
        
    def diagnose_failure(self, stream_id):
        """ì‹¤íŒ¨ ì›ì¸ ì§„ë‹¨"""
        possible_causes = []
        
        # GPU ë©”ëª¨ë¦¬ í™•ì¸
        if torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.9:
            possible_causes.append('GPU_MEMORY_PRESSURE')
            
        # ë””ì½”ë” ìƒíƒœ í™•ì¸
        decoder_status = self.check_decoder_health(stream_id)
        if not decoder_status['healthy']:
            possible_causes.append(f'DECODER_ERROR: {decoder_status["error"]}')
            
        # ë„¤íŠ¸ì›Œí¬/íŒŒì¼ I/O í™•ì¸
        io_status = self.check_io_health(stream_id)
        if not io_status['healthy']:
            possible_causes.append(f'IO_ERROR: {io_status["error"]}')
            
        return possible_causes if possible_causes else ['UNKNOWN_ERROR']
        
    def is_critical_stream(self, stream_id):
        """ì¤‘ìš” ìŠ¤íŠ¸ë¦¼ íŒë‹¨ ê¸°ì¤€"""
        # ì˜ˆì‹œ: stream_0, stream_1ì€ ì¤‘ìš”, stream_2, stream_3ì€ ì¼ë°˜
        return stream_id in ['stream_0', 'stream_1']
```

**ì—ëŸ¬ ì²˜ë¦¬ ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤**:

| ì‹¤íŒ¨ ìƒí™© | ì‹¤íŒ¨ ìŠ¤íŠ¸ë¦¼ ìˆ˜ | ì²˜ë¦¬ ë°©ì‹ | íƒ€ì¼ í•©ì„± ì—¬ë¶€ |
|-----------|---------------|-----------|----------------|
| **ë‹¨ì¼ ì¼ë°˜ ìŠ¤íŠ¸ë¦¼** | 1ê°œ | ëŒ€ì²´ í”„ë ˆì„ ì‚¬ìš© | âœ… ê³„ì† |
| **ë‹¨ì¼ ì¤‘ìš” ìŠ¤íŠ¸ë¦¼** | 1ê°œ | ì „ì²´ ë°°ì¹˜ ìŠ¤í‚µ | âŒ ìŠ¤í‚µ |
| **ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ (2ê°œ)** | 2ê°œ | ë¶€ë¶„ ë³µêµ¬ ì‹œë„ | ğŸ”„ ì¡°ê±´ë¶€ |
| **ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ (3ê°œ+)** | 3ê°œ+ | ê¸´ê¸‰ ì²˜ë¦¬ ëª¨ë“œ | ğŸš¨ ì¤‘ë‹¨ |

**ì‹¤íŒ¨ ë¶„ì„ ë° ë³µêµ¬ ì§€í‘œ**:

```python
class FailureAnalytics:
    def __init__(self):
        self.failure_metrics = {
            'total_batches': 0,
            'failed_batches': 0,
            'skipped_batches': 0,
            'recovered_batches': 0,
            'failure_rate': 0.0
        }
        
    def update_failure_metrics(self, batch_result):
        """ì‹¤íŒ¨ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        self.failure_metrics['total_batches'] += 1
        
        if batch_result['action'] == 'skip_batch':
            self.failure_metrics['skipped_batches'] += 1
        elif batch_result['action'] == 'continue_with_replacement':
            self.failure_metrics['recovered_batches'] += 1
        elif batch_result['action'] == 'emergency_shutdown':
            self.failure_metrics['failed_batches'] += 1
            
        # ì‹¤íŒ¨ìœ¨ ê³„ì‚°
        self.failure_metrics['failure_rate'] = (
            (self.failure_metrics['failed_batches'] + self.failure_metrics['skipped_batches']) /
            max(self.failure_metrics['total_batches'], 1)
        )
        
    def get_health_report(self):
        """ì‹œìŠ¤í…œ ê±´ê°•ì„± ë³´ê³ """
        return {
            'overall_health': 'HEALTHY' if self.failure_metrics['failure_rate'] < 0.05 else 'DEGRADED',
            'failure_rate_percent': self.failure_metrics['failure_rate'] * 100,
            'recovery_rate_percent': (
                self.failure_metrics['recovered_batches'] / 
                max(self.failure_metrics['total_batches'], 1)
            ) * 100,
            'recommendations': self.generate_recommendations()
        }
```

**NVENC ë™ì‹œ ì„¸ì…˜ í•œë„ ê²€ì¦ ë° ìš°íšŒì±…**:
```python
class NVENCManager:
    def __init__(self):
        self.max_sessions = self.detect_nvenc_limit()
        self.active_sessions = []
        self.encoding_queue = []
        
    def detect_nvenc_limit(self):
        """NVENC ì‹¤ì œ ì„¸ì…˜ í•œë„ ì¸¡ì •"""
        import subprocess
        
        # nvidia-smië¡œ ê¸°ë³¸ ì •ë³´ í™•ì¸
        try:
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=encoder.max_sessions', 
                '--format=csv,noheader'
            ], capture_output=True, text=True, timeout=5)
            theoretical_max = int(result.stdout.strip())
        except:
            theoretical_max = 2  # RTX 5090 ê¸°ë³¸ê°’
            
        # ì‹¤ì œ ë™ì‹œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
        actual_max = 0
        test_sessions = []
        
        for i in range(theoretical_max + 1):
            try:
                # ë”ë¯¸ NVENC ì„¸ì…˜ ìƒì„±
                session = self.create_test_nvenc_session()
                test_sessions.append(session)
                actual_max += 1
            except Exception as e:
                print(f"NVENC ì„¸ì…˜ í•œë„: {actual_max}ê°œ (ì´ë¡ ê°’: {theoretical_max})")
                break
                
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ì •ë¦¬
        for session in test_sessions:
            session.close()
            
        return min(actual_max, 4)  # ì•ˆì „ ë§ˆì§„ ê³ ë ¤
        
    def request_encoding_slot(self, stream_id, priority='normal'):
        """ì¸ì½”ë”© ìŠ¬ë¡¯ ìš”ì²­ (ëŒ€ê¸°ì—´ ê´€ë¦¬)"""
        if len(self.active_sessions) < self.max_sessions:
            session = self.create_nvenc_session(stream_id)
            self.active_sessions.append(session)
            return session
        else:
            # ëŒ€ê¸°ì—´ì— ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜)
            self.encoding_queue.append({
                'stream_id': stream_id,
                'priority': priority,
                'timestamp': time.time()
            })
            return None
            
    def apply_bitrate_adjustment(self, session, quality_level):
        """ë¹„íŠ¸ë ˆì´íŠ¸ ë™ì  ì¡°ì • (ì„¸ì…˜ ìˆ˜ì— ë”°ë¼)"""
        base_bitrate = 8_000_000  # 8Mbps
        
        if len(self.active_sessions) >= 3:
            # 3ê°œ ì´ìƒ ì‹œ CBR â†’ VBR ì „í™˜, ë¹„íŠ¸ë ˆì´íŠ¸ ê°ì†Œ
            adjusted_bitrate = int(base_bitrate * 0.7)
            session.set_bitrate_mode('VBR')
            session.set_target_bitrate(adjusted_bitrate)
        else:
            # ì—¬ìœ  ìˆì„ ë•ŒëŠ” CBR ê³ í’ˆì§ˆ
            session.set_bitrate_mode('CBR') 
            session.set_target_bitrate(base_bitrate)
```

**ì˜¤ë””ì˜¤ ë™ê¸°í™” ë“œë¦¬í”„íŠ¸ ëŒ€ì‘** (VFR ì…ë ¥ ì§€ì›):
```python
class AudioSyncManager:
    def __init__(self):
        self.original_audio = None
        self.video_pts_history = []
        self.audio_pts_history = []
        self.sync_drift_threshold = 40  # 40ms ì´ìƒ ë“œë¦¬í”„íŠ¸ ì‹œ ë³´ì •
        
    def extract_original_audio(self, video_path):
        """ì›ë³¸ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¶”ì¶œ ë° ë¶„ì„"""
        container = av.open(video_path)
        
        if container.streams.audio:
            audio_stream = container.streams.audio[0]
            self.original_audio = {
                'stream': audio_stream,
                'sample_rate': audio_stream.sample_rate,
                'channels': audio_stream.channels,
                'duration': audio_stream.duration,
                'time_base': audio_stream.time_base
            }
            
            # VFR ë¹„ë””ì˜¤ ê°ì§€
            video_stream = container.streams.video[0]
            if hasattr(video_stream, 'average_rate') and hasattr(video_stream, 'base_rate'):
                if video_stream.average_rate != video_stream.base_rate:
                    print("VFR ë¹„ë””ì˜¤ ê°ì§€ - ê³ ê¸‰ ë™ê¸°í™” ëª¨ë“œ í™œì„±í™”")
                    self.vfr_mode = True
                    
    def track_av_sync(self, video_pts, audio_pts):
        """A/V ë™ê¸°í™” ë“œë¦¬í”„íŠ¸ ì¶”ì """
        self.video_pts_history.append(video_pts)
        self.audio_pts_history.append(audio_pts)
        
        # ìµœê·¼ 100í”„ë ˆì„ë§Œ ìœ ì§€
        if len(self.video_pts_history) > 100:
            self.video_pts_history.pop(0)
            self.audio_pts_history.pop(0)
            
        # ë“œë¦¬í”„íŠ¸ ê³„ì‚°
        if len(self.video_pts_history) >= 10:
            video_duration = self.video_pts_history[-1] - self.video_pts_history[0]
            audio_duration = self.audio_pts_history[-1] - self.audio_pts_history[0]
            drift = abs(video_duration - audio_duration) * 1000  # ms
            
            if drift > self.sync_drift_threshold:
                return self.suggest_sync_correction(drift, video_duration, audio_duration)
                
        return None
        
    def suggest_sync_correction(self, drift_ms, video_dur, audio_dur):
        """ë™ê¸°í™” ë³´ì • ë°©ë²• ì œì•ˆ"""
        if video_dur > audio_dur:
            # ë¹„ë””ì˜¤ê°€ ë” ê¸¸ìŒ - ì˜¤ë””ì˜¤ ëŠ˜ë¦¬ê¸° ë˜ëŠ” ë¹„ë””ì˜¤ ìë¥´ê¸°
            return {
                'type': 'audio_stretch',
                'factor': video_dur / audio_dur,
                'method': 'time_stretch' if drift_ms < 100 else 'frame_drop'
            }
        else:
            # ì˜¤ë””ì˜¤ê°€ ë” ê¸¸ìŒ - ì˜¤ë””ì˜¤ ìë¥´ê¸° ë˜ëŠ” ë¹„ë””ì˜¤ ëŠ˜ë¦¬ê¸°
            return {
                'type': 'audio_trim',
                'trim_ms': drift_ms,
                'method': 'precise_cut'
            }
            
    def apply_sync_correction(self, correction):
        """PTS ê¸°ë°˜ ì •ë°€ ë™ê¸°í™” ì ìš©"""
        if correction['type'] == 'audio_stretch':
            if correction['method'] == 'time_stretch' and correction['factor'] < 1.05:
                # 5% ì´ë‚´ëŠ” íƒ€ì„ ìŠ¤íŠ¸ë ˆì¹˜ (í’ˆì§ˆ ìœ ì§€)
                return self.apply_time_stretch(correction['factor'])
            else:
                # í° ì°¨ì´ëŠ” í”„ë ˆì„ ë“œë¡­/ë³µì œ
                return self.apply_frame_adjustment(correction['factor'])
                
        elif correction['type'] == 'audio_trim':
            return self.apply_precise_audio_trim(correction['trim_ms'])
```

---

## ğŸ“Š ì„±ëŠ¥ ëª©í‘œ ë° ë³‘ëª© í•´ê²° (VRAM/RAM ë¶„ë¦¬)

### ğŸ¯ Full GPU íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ëª©í‘œ (ì¼ê´€ì„± í™•ë³´)

#### ğŸ“Š **ì²˜ë¦¬ëŸ‰ ë¹„êµí‘œ** (í˜¼ë™ ë°©ì§€)

| ì²˜ë¦¬ ë°©ì‹ | ë™ì‹œ ìŠ¤íŠ¸ë¦¼ | ì²˜ë¦¬ëŸ‰(ê°œ/ì‹œê°„) | 4ê°œ ì˜ìƒ ì´ì‹œê°„ | ë‹¨ì¼ ì˜ìƒ ì‹œê°„ | VRAM ì‚¬ìš©ëŸ‰ |
|----------|-------------|----------------|----------------|----------------|-------------|
| **ê¸°ì¡´ CPU** | 1ê°œ | 2.6ê°œ/ì‹œê°„ | 92ë¶„ (ìˆœì°¨) | 23ë¶„/ê°œ | ~8GB |
| **ë³´ìˆ˜ì  GPU** | 2ê°œ | 12ê°œ/ì‹œê°„ | 20ë¶„ (ë³‘ë ¬) | 10ë¶„/ê°œ | **12-14GB** |
| **ëª©í‘œ GPU** | 3ê°œ | 15ê°œ/ì‹œê°„ | 16ë¶„ (ë³‘ë ¬) | 5.3ë¶„/ê°œ | **18-21GB** |
| **ìµœì í™” GPU** | 4ê°œ | 16ê°œ/ì‹œê°„ | 15ë¶„ (ë³‘ë ¬) | 3.75ë¶„/ê°œ | **24-28GB** |

#### ğŸš€ **ì„±ëŠ¥ í–¥ìƒ ì§€í‘œ** (ëª…í™•í•œ ë‹¨ìœ„ êµ¬ë¶„)

| ì§€í‘œ ìœ í˜• | ê¸°ì¡´ â†’ ìµœì í™” | í–¥ìƒ ë°°ìˆ˜ | ë¹„ê³  |
|----------|---------------|-----------|------|
| **ì²˜ë¦¬ëŸ‰** | 2.6 â†’ 16ê°œ/ì‹œê°„ | **6.2ë°°** | ì‹œê°„ë‹¹ ì²˜ë¦¬ ê°€ëŠ¥ ì˜ìƒ ìˆ˜ |
| **ë‹¨ì¼ ì˜ìƒ** | 23ë¶„ â†’ 3.75ë¶„ | **6.1ë°°** | ê°œë³„ ì˜ìƒ ì²˜ë¦¬ ì‹œê°„ |
| **4ê°œ ë™ì‹œ** | 92ë¶„ â†’ 15ë¶„ | **6.1ë°°** | ë³‘ë ¬ ì²˜ë¦¬ ì´ ì‹œê°„ |

### âš¡ ì£¼ìš” ë³‘ëª© ì™„ì „ í•´ê²°

| ë³‘ëª© ì§€ì  | ê¸°ì¡´ ë¬¸ì œ | Full GPU í•´ê²°ì±… | ì„±ëŠ¥ í–¥ìƒ |
|-----------|----------|-----------------|-----------|
| **ë””ì½”ë”©** | CPU ë””ì½”ë”© | PyAV NVDEC | **15-20ë°°** |
| **ì¶”ë¡ ** | PyTorch FP32 | TensorRT FP16 | **2-3ë°°** |
| **ë©”ëª¨ë¦¬ ë³µì‚¬** | CPUâ†”GPU ì „ì†¡ | Zero-Copy íŒŒì´í”„ë¼ì¸ | **ì§€ì—°ì‹œê°„ 80% ê°ì†Œ** |
| **í•©ì„±** | CPU ë¦¬ì‚¬ì´ì¦ˆ/íƒ€ì¼ë§ | CUDA í•©ì„± | **10-15ë°°** |
| **ì¸ì½”ë”©** | FFmpeg CPU | NVENC H.264 | **8-12ë°°** |

### ğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ì¸¡ì • ì§€í‘œ

**NVIDIA í•˜ë“œì›¨ì–´ í™œìš©ë¥ **:
- **NVDEC í™œìš©ë¥ **: 85% ì´ìƒ ëª©í‘œ (4ê°œ ì—”ì§„ ê°œë³„ ì¶”ì )
- **NVENC í™œìš©ë¥ **: 75% ì´ìƒ ëª©í‘œ (2ê°œ ì—”ì§„ ê°œë³„ ì¶”ì )
- **GPU Compute**: 92% ì´ìƒ ëª©í‘œ (SM ë‹¨ìœ„ ì¸¡ì •)
- **VRAM íš¨ìœ¨ì„±**: 32GB ì¤‘ **87.5% ì´í•˜** ì‚¬ìš©
- **PCIe ëŒ€ì—­í­**: < 20% ì‚¬ìš© (ì œë¡œì¹´í”¼ íš¨ê³¼)

**íŒŒì´í”„ë¼ì¸ ì„¸ë¶€ ì§€í‘œ**:
- **Per-Stage Latency**: ë””ì½”ë“œ(5ms), ì¶”ë¡ (15ms), í•©ì„±(3ms), ì¸ì½”ë”©(7ms)
- **Per-Stream FPS**: ìŠ¤íŠ¸ë¦¼ë‹¹ ì‹¤ì‹œê°„ FPS (30+ ëª©í‘œ)
- **Frame Queue Depth**: ëŒ€ê¸° í”„ë ˆì„ < 5ê°œ
- **ID Switch Rate**: ë¶„ë‹¹ < 0.05íšŒ (íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ íš¨ê³¼)
- **Batch Efficiency**: TensorRT ë°°ì¹˜ í™œìš©ë¥  > 90%

**ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì§€í‘œ**:
- **System RAM**: 48GB ì¤‘ **44% ì´í•˜** ì‚¬ìš©
- **CPU í™œìš©ë¥ **: 32ìŠ¤ë ˆë“œ ì¤‘ 25% ì´í•˜ (ì œì–´ ë¡œì§ë§Œ)
- **ë””ìŠ¤í¬ I/O**: ìˆœì°¨ ì½ê¸° ìœ„ì£¼, ëœë¤ I/O ìµœì†Œí™”

---

## ğŸš€ ë‹¨ê³„ë³„ ê°œë°œ ê³„íš

### Phase 1: PyAV NVDEC + TensorRT íŒŒì´í”„ë¼ì¸ (4ì¼)

| ë‹¨ê³„ | ì‘ì—… ë‚´ìš© | ì™„ë£Œ ì¡°ê±´ | ì‹œê°„ |
|------|-----------|-----------|------|
| **1.1** | PyAV NVDEC í™˜ê²½ êµ¬ì¶• | hwaccel=cuda ë””ì½”ë”© ì„±ê³µ | 1ì¼ |
| **1.2** | ëª¨ë¸ í›„ë³´êµ° TensorRT ë³€í™˜ | YOLOv8n/s, SCRFD ì—”ì§„ ìƒì„± | 1ì¼ |
| **1.3** | ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ GPU íŒŒì´í”„ë¼ì¸ | NVDECâ†’TensorRTâ†’NVENC ì—°ê²° | 1.5ì¼ |
| **1.4** | ByteTrack í†µí•© + ì„±ëŠ¥ ì¸¡ì • | ë‹¨ì¼ ì˜ìƒ ì²˜ë¦¬ + ë²¤ì¹˜ë§ˆí‚¹ | 0.5ì¼ |

**ì™„ë£Œ ê¸°ì¤€**: ë‹¨ì¼ ì˜ìƒì„ **5ë¶„ ì´ë‚´** ì²˜ë¦¬ (ê¸°ì¡´ 23ë¶„ â†’ 78% ë‹¨ì¶•)

### Phase 2: ë©€í‹° ìŠ¤íŠ¸ë¦¼ + GPU í•©ì„± (4ì¼)

| ë‹¨ê³„ | ì‘ì—… ë‚´ìš© | ì™„ë£Œ ì¡°ê±´ | ì‹œê°„ |
|------|-----------|-----------|------|
| **2.1** | 4x CUDA Stream êµ¬í˜„ | ë…ë¦½ì ì¸ 4ê°œ ìŠ¤íŠ¸ë¦¼ ë™ì‘ | 1.5ì¼ |
| **2.2** | CUDA í•©ì„± íŒŒì´í”„ë¼ì¸ | GPU ë¦¬ì‚¬ì´ì¦ˆ+íƒ€ì¼ë§ êµ¬í˜„ | 1.5ì¼ |
| **2.3** | ì¢Œìš° ë¶„ê¸° ì•ˆì •í™” | EMA+íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì ìš© | 0.5ì¼ |
| **2.4** | VRAM ê´€ë¦¬ ì‹œìŠ¤í…œ | ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ + ë°±í”„ë ˆì…” | 0.5ì¼ |

**ì™„ë£Œ ê¸°ì¤€**: 3ê°œ ì˜ìƒì„ **12ë¶„ ì´ë‚´** ë™ì‹œ ì²˜ë¦¬

### Phase 3: ê³ ê¸‰ ìµœì í™” + ëª¨ë‹ˆí„°ë§ (2ì¼)

| ë‹¨ê³„ | ì‘ì—… ë‚´ìš© | ì™„ë£Œ ì¡°ê±´ | ì‹œê°„ |
|------|-----------|-----------|------|
| **3.1** | ì˜¤ë””ì˜¤ ì²˜ë¦¬ í†µí•© | PyAV ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ë³´ì¡´ | 0.5ì¼ |
| **3.2** | ìƒì„¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ | per-stage latency ì¶”ì  | 0.5ì¼ |
| **3.3** | ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ | OOM/í¬ë˜ì‹œ ìë™ ì¬ì‹œì‘ | 0.5ì¼ |
| **3.4** | ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ | 4ê°œ ìŠ¤íŠ¸ë¦¼ ì•ˆì •ì„± ê²€ì¦ | 0.5ì¼ |

**ìµœì¢… ëª©í‘œ**: 4ê°œ ì˜ìƒì„ **15ë¶„ ì´ë‚´** ì²˜ë¦¬ (**8-10ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ**)

> **ì´ ì˜ˆìƒ ê¸°ê°„: 10ì¼**

---

## ğŸ—‚ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° (Full GPU íŒŒì´í”„ë¼ì¸)

```
dual_face_tracker/
â”œâ”€â”€ core/                          # í•µì‹¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pyav_decoder.py           # PyAV NVDEC wrapper
â”‚   â”œâ”€â”€ tensorrt_detector.py       # TensorRT ë‹¤ì¤‘ ëª¨ë¸ ì—”ì§„
â”‚   â”œâ”€â”€ cuda_tracker.py            # ByteTrack/StrongSORT
â”‚   â”œâ”€â”€ cuda_compositor.py         # GPU í•©ì„± íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ nvenc_encoder.py           # NVENC wrapper
â”‚   â””â”€â”€ stream_manager.py          # ë©€í‹° ìŠ¤íŠ¸ë¦¼ ê´€ë¦¬
â”œâ”€â”€ utils/                         # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py                  # í†µí•© ë¡œê¹…
â”‚   â”œâ”€â”€ hw_monitor.py              # NVDEC/NVENC/VRAM ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ memory_manager.py          # VRAM/RAM ê´€ë¦¬
â”‚   â”œâ”€â”€ performance_tracker.py     # per-stage latency ì¶”ì 
â”‚   â””â”€â”€ audio_processor.py         # ì˜¤ë””ì˜¤ ë³´ì¡´/ë™ê¸°í™”
â”œâ”€â”€ models/                        # ëª¨ë¸ ê´€ë ¨
â”‚   â”œâ”€â”€ tensorrt_engines/          # TensorRT ì—”ì§„ë“¤
â”‚   â”‚   â”œâ”€â”€ yolov8n_face.trt      # ê²½ëŸ‰ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ yolov8s_face.trt      # ì¤‘ê°„ ëª¨ë¸  
â”‚   â”‚   â””â”€â”€ scrfd_2.5g.trt        # ëŒ€ì•ˆ ëª¨ë¸
â”‚   â””â”€â”€ model_converter.py         # ëª¨ë¸ â†’ TensorRT ë³€í™˜ê¸°
â”œâ”€â”€ config/                        # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ hardware_config.yaml       # NVDEC/NVENC ì„¤ì •
â”‚   â”œâ”€â”€ model_config.yaml          # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„°
â”‚   â”œâ”€â”€ stream_config.yaml         # ìŠ¤íŠ¸ë¦¼ë³„ ì„¤ì •
â”‚   â””â”€â”€ performance_config.yaml    # ì„±ëŠ¥ ì„ê³„ê°’
â”œâ”€â”€ monitoring/                    # ëª¨ë‹ˆí„°ë§
â”‚   â”œâ”€â”€ grafana_dashboard.json     # ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ prometheus_config.yml      # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
â”‚   â””â”€â”€ alerting_rules.yml         # ì¥ì•  ì•Œë¦¼
â”œâ”€â”€ recovery/                      # ì¥ì•  ë³µêµ¬
â”‚   â”œâ”€â”€ checkpoint_manager.py      # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
â”‚   â”œâ”€â”€ stream_recovery.py         # ìŠ¤íŠ¸ë¦¼ ì¬ì‹œì‘
â”‚   â””â”€â”€ health_checker.py          # í—¬ìŠ¤ì²´í¬
â”œâ”€â”€ tests/                         # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_pyav_nvdec.py        # NVDEC í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_tensorrt_models.py   # ëª¨ë¸ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ test_cuda_composition.py  # GPU í•©ì„± í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ test_full_pipeline.py     # í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ benchmark/                     # ë²¤ì¹˜ë§ˆí‚¹
â”‚   â”œâ”€â”€ model_comparison.py        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
â”‚   â”œâ”€â”€ hardware_profiling.py      # í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼ë§
â”‚   â””â”€â”€ scalability_test.py        # í™•ì¥ì„± í…ŒìŠ¤íŠ¸
â”œâ”€â”€ logs/                          # ë¡œê·¸
â”‚   â””â”€â”€ full_gpu_tracker.log       # í†µí•© ë¡œê·¸
â”œâ”€â”€ main.py                        # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ requirements.txt               # ì˜ì¡´ì„±
â””â”€â”€ README.md                      # ê°€ì´ë“œ
```

---

## ğŸ”§ í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± (PyAV ì¤‘ì‹¬)

### ğŸ“¦ ê¸°ì¡´ íŒ¨í‚¤ì§€ (ì¬ì‚¬ìš©)

| íŒ¨í‚¤ì§€ëª… | ë²„ì „ | ìš©ë„ | ìƒíƒœ |
|----------|------|------|------|
| **torch** | 2.7.1+cu128 | TensorRT ë°±ì—”ë“œ | âœ… í™œìš© |
| **numpy** | 1.26.4 | ìˆ˜ì¹˜ ì—°ì‚° | âœ… í™œìš© |
| **opencv** | 4.13.0-dev | CUDA í•©ì„± ë³´ì¡° | âœ… ì œí•œì  í™œìš© |
| **psutil** | 6.1.1 | ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ | âœ… í™œìš© |

### ğŸ“¥ Full GPU íŒŒì´í”„ë¼ì¸ ì „ìš© íŒ¨í‚¤ì§€

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source /home/hamtoto/work/python-study/Face-Tracking-App/.venv/bin/activate

# PyNvCodec (1ìˆœìœ„ - ì§„ì§œ ì œë¡œì¹´í”¼)
pip install pynvcodec               # PyNvCodec (VPF) 
pip install av                     # PyAV (ë°±ì—…ìš©)

# TensorRT ìµœì í™”
pip install tensorrt               # TensorRT Python API
pip install torch2trt              # PyTorch â†’ TensorRT

# ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
pip install ultralytics            # YOLOv8 variants
pip install insightface            # SCRFD models

# íŠ¸ë˜ì»¤ (ì„ íƒì )
pip install deep-sort-realtime     # ByteTrack
pip install motpy                  # ëŒ€ì•ˆ íŠ¸ë˜ì»¤

# ëª¨ë‹ˆí„°ë§ & ë³µêµ¬
pip install py3nvml                # NVIDIA GPU ëª¨ë‹ˆí„°ë§
pip install prometheus-client      # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
pip install watchdog               # íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

# ê³ ì„±ëŠ¥ I/O
pip install aiofiles               # ë¹„ë™ê¸° I/O
pip install uvloop                 # ê³ ì„±ëŠ¥ ì´ë²¤íŠ¸ ë£¨í”„
```

### ğŸ” **PyNvCodec í™˜ê²½ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸** (ë¦¬ìŠ¤í¬ ì¡°ê¸° ì°¨ë‹¨)

**í•µì‹¬ í˜ì‹ **: ê°œë°œ ì°©ìˆ˜ **ì „**ì— ì‹¤í–‰í•˜ì—¬ PyNvCodec ì˜ì¡´ì„± ë¬¸ì œë¥¼ ì™„ì „ ì°¨ë‹¨

**validate_environment.sh** (í•„ìˆ˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸):

```bash
#!/bin/bash
# validate_environment.sh - PyNvCodec í™˜ê²½ ì™„ì „ ê²€ì¦
set -e

echo "ğŸ” PyNvCodec í™˜ê²½ ê²€ì¦ ì‹œì‘..."
echo "=================================================="

# 1ë‹¨ê³„: ì‹œìŠ¤í…œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ í™•ì¸
echo "1ï¸âƒ£ ì‹œìŠ¤í…œ ê¸°ë³¸ ìš”êµ¬ì‚¬í•­ í™•ì¸"
echo "----------------------------"

# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | head -1)
echo "âœ“ NVIDIA ë“œë¼ì´ë²„: $DRIVER_VERSION"

if [[ $(echo "$DRIVER_VERSION >= 525.0" | bc -l) -eq 0 ]]; then
    echo "âŒ ë“œë¼ì´ë²„ ë²„ì „ ë¶€ì¡± (ìµœì†Œ 525.0 í•„ìš”)"
    exit 1
fi

# CUDA ë²„ì „ í™•ì¸
CUDA_VERSION=$(nvidia-smi --query-gpu=cuda_version --format=csv,noheader,nounits | head -1)
echo "âœ“ CUDA ë²„ì „: $CUDA_VERSION"

# GPU ëª¨ë¸ í™•ì¸
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)
echo "âœ“ GPU ëª¨ë¸: $GPU_NAME"

# 2ë‹¨ê³„: PyNvCodec ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦
echo -e "\n2ï¸âƒ£ PyNvCodec ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²€ì¦"
echo "----------------------------"

# PyNvCodec ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
python3 -c "
try:
    import PyNvCodec as nvc
    print('âœ“ PyNvCodec ì„í¬íŠ¸ ì„±ê³µ')
    print(f'  ë²„ì „: {nvc.__version__ if hasattr(nvc, \"__version__\") else \"Unknown\"}')
except ImportError as e:
    print(f'âŒ PyNvCodec ì„í¬íŠ¸ ì‹¤íŒ¨: {e}')
    exit(1)
"

# 3ë‹¨ê³„: ì‹¤ì œ ë””ì½”ë”© í…ŒìŠ¤íŠ¸
echo -e "\n3ï¸âƒ£ ì‹¤ì œ NVDEC ë””ì½”ë”© í…ŒìŠ¤íŠ¸"
echo "----------------------------"

# í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ìƒì„± (í•„ìš”ì‹œ)
if [[ ! -f "test_sample.mp4" ]]; then
    echo "ğŸ“¹ í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘..."
    ffmpeg -f lavfi -i testsrc=duration=5:size=1920x1080:rate=30 -c:v libx264 -y test_sample.mp4 &>/dev/null
fi

# PyNvCodec ë””ì½”ë”© í…ŒìŠ¤íŠ¸
python3 << 'EOF'
import PyNvCodec as nvc
import sys

try:
    # NVDEC ë””ì½”ë” ìƒì„± í…ŒìŠ¤íŠ¸
    decoder = nvc.PyDecodeHW("test_sample.mp4", nvc.PixelFormat.NV12, 0)
    print(f"âœ“ NVDEC ë””ì½”ë” ìƒì„± ì„±ê³µ")
    print(f"  í•´ìƒë„: {decoder.Width()}x{decoder.Height()}")
    
    # ìƒ‰ê³µê°„ ë³€í™˜ê¸° í…ŒìŠ¤íŠ¸
    converter = nvc.PySurfaceConverter(
        decoder.Width(), decoder.Height(),
        nvc.PixelFormat.NV12, nvc.PixelFormat.RGB, 0
    )
    print("âœ“ ìƒ‰ê³µê°„ ë³€í™˜ê¸° ìƒì„± ì„±ê³µ")
    
    # ì‹¤ì œ í”„ë ˆì„ ë””ì½”ë”© í…ŒìŠ¤íŠ¸ (5í”„ë ˆì„ë§Œ)
    for i in range(5):
        surface = decoder.DecodeSurface()
        if surface:
            rgb_surface = converter.Execute(surface)
            if rgb_surface:
                print(f"âœ“ í”„ë ˆì„ {i+1} ë””ì½”ë”©/ë³€í™˜ ì„±ê³µ")
            else:
                print(f"âŒ í”„ë ˆì„ {i+1} ë³€í™˜ ì‹¤íŒ¨")
                sys.exit(1)
        else:
            print(f"âœ“ ë””ì½”ë”© ì™„ë£Œ ({i}í”„ë ˆì„ ì²˜ë¦¬)")
            break
            
    print("ğŸ‰ NVDEC ë””ì½”ë”© í…ŒìŠ¤íŠ¸ ì™„ì „ ì„±ê³µ!")
    
except Exception as e:
    print(f"âŒ NVDEC ë””ì½”ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    sys.exit(1)
EOF

# 4ë‹¨ê³„: TensorRT ì—°ë™ í…ŒìŠ¤íŠ¸
echo -e "\n4ï¸âƒ£ TensorRT ì—°ë™ í…ŒìŠ¤íŠ¸"
echo "----------------------------"

python3 -c "
import tensorrt as trt
import torch

try:
    print(f'âœ“ TensorRT ë²„ì „: {trt.__version__}')
    print(f'âœ“ PyTorch ë²„ì „: {torch.__version__}')
    print(f'âœ“ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}')
    print(f'âœ“ GPU ê°œìˆ˜: {torch.cuda.device_count()}')
    
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f'âœ“ GPU 0: {device_name}')
    
except Exception as e:
    print(f'âŒ TensorRT/PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}')
    exit(1)
"

# 5ë‹¨ê³„: NVENC ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ (ì„ íƒì )
echo -e "\n5ï¸âƒ£ NVENC ì¸ì½”ë”© í…ŒìŠ¤íŠ¸"
echo "----------------------------"

# FFmpeg NVENC í…ŒìŠ¤íŠ¸
ffmpeg -f lavfi -i testsrc=duration=2:size=640x480:rate=30 \
    -c:v h264_nvenc -preset fast -b:v 2M -f null - &>/dev/null

if [[ $? -eq 0 ]]; then
    echo "âœ“ NVENC H.264 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ"
else
    echo "âš ï¸ NVENC ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ë¹„ì¹˜ëª…ì )"
fi

# 6ë‹¨ê³„: ìµœì¢… ê²€ì¦ ê²°ê³¼ ì €ì¥
echo -e "\n6ï¸âƒ£ ê²€ì¦ ê²°ê³¼ ì €ì¥"
echo "----------------------------"

cat > environment_validation_report.json << EOF
{
    "validation_timestamp": "$(date -Iseconds)",
    "system_info": {
        "gpu_name": "$GPU_NAME",
        "driver_version": "$DRIVER_VERSION",
        "cuda_version": "$CUDA_VERSION"
    },
    "validation_results": {
        "pynvcodec_import": "PASSED",
        "nvdec_decoding": "PASSED",
        "tensorrt_integration": "PASSED",
        "nvenc_encoding": "$([ $? -eq 0 ] && echo 'PASSED' || echo 'WARNING')"
    },
    "environment_status": "READY_FOR_DEVELOPMENT"
}
EOF

echo "âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ!"
echo "ğŸ“„ ìƒì„¸ ê²°ê³¼: environment_validation_report.json"
echo -e "\nğŸš€ PyNvCodec ê°œë°œ í™˜ê²½ì´ ì™„ë²½íˆ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!"

# ì„ì‹œ íŒŒì¼ ì •ë¦¬
rm -f test_sample.mp4

echo "=================================================="
```

**ìë™ ì‹¤í–‰ í†µí•©**:

```python
# main.pyì—ì„œ ìë™ ì‹¤í–‰
import subprocess
import sys
import os

def validate_environment_on_startup():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í™˜ê²½ ê²€ì¦"""
    print("ğŸ”§ ì‹œì‘ ì „ í™˜ê²½ ê²€ì¦ ì‹¤í–‰...")
    
    if not os.path.exists("environment_validation_report.json"):
        print("âš ï¸ í™˜ê²½ ê²€ì¦ ê¸°ë¡ ì—†ìŒ - ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        
        try:
            result = subprocess.run(
                ["bash", "validate_environment.sh"],
                capture_output=True,
                text=True,
                timeout=120  # 2ë¶„ ì œí•œ
            )
            
            if result.returncode != 0:
                print("âŒ í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨:")
                print(result.stderr)
                sys.exit(1)
                
        except subprocess.TimeoutExpired:
            print("âŒ í™˜ê²½ ê²€ì¦ ì‹œê°„ ì´ˆê³¼")
            sys.exit(1)
            
    else:
        print("âœ… í™˜ê²½ ê²€ì¦ ê¸°ë¡ í™•ì¸ - ê²€ì¦ í†µê³¼")

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì „ ì‹¤í–‰
if __name__ == "__main__":
    validate_environment_on_startup()
    # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘...
```

**ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì¥ì **:

1. **ì¡°ê¸° ì°¨ë‹¨**: ê°œë°œ ì‹œì‘ ì „ì— ëª¨ë“  ì˜ì¡´ì„± í™•ì¸
2. **êµ¬ì²´ì  í…ŒìŠ¤íŠ¸**: ì‹¤ì œ ë””ì½”ë”©/ì¸ì½”ë”© ë™ì‘ ê²€ì¦
3. **ìë™í™”**: ìŠ¤í¬ë¦½íŠ¸ í•œ ë²ˆ ì‹¤í–‰ìœ¼ë¡œ ì™„ì „ ê²€ì¦
4. **ê²°ê³¼ ì €ì¥**: JSON í˜•íƒœë¡œ ê²€ì¦ ê²°ê³¼ ê¸°ë¡
5. **ì¬í˜„ì„±**: ë™ì¼í•œ í™˜ê²½ì—ì„œ ë°˜ë³µ ê°€ëŠ¥í•œ ê²€ì¦

### ğŸ³ **í™˜ê²½ ê³ ì • ë° ì¬í˜„ì„± ë³´ì¥**

**Docker ë² ì´ìŠ¤ í™˜ê²½** (ì™„ì „ ì¬í˜„ ê°€ëŠ¥):
```dockerfile
FROM nvidia/cuda:12.8-devel-ubuntu22.04

# ì •í™•í•œ ë²„ì „ ë§¤íŠ¸ë¦­ìŠ¤ (ê²€ì¦ëœ ì¡°í•©)
ARG CUDA_VERSION=12.8
ARG TENSORRT_VERSION=10.7.0
ARG FFMPEG_VERSION=6.1.1
ARG PYAV_VERSION=12.3.0

ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,video,utility

# í•„ìˆ˜ ì‹œìŠ¤í…œ ì˜ì¡´ì„± (ë²„ì „ ê³ ì •)
RUN apt-get update && apt-get install -y \
    python3.10=3.10.12-1~22.04.1 \
    python3.10-dev=3.10.12-1~22.04.1 \
    ffmpeg=${FFMPEG_VERSION}-* \
    && apt-get clean

# Python íŒ¨í‚¤ì§€ (ì •í™•í•œ ë²„ì „)
COPY requirements_frozen.txt /tmp/
RUN pip install -r /tmp/requirements_frozen.txt

# TensorRT ì„¤ì¹˜ (CUDA ë²„ì „ê³¼ ë§¤ì¹­)
RUN pip install tensorrt==${TENSORRT_VERSION}

# PyAV NVDEC ì§€ì› í™•ì¸
RUN python3 -c "import av; print('NVDEC codecs:', [c for c in av.codec.codecs_available if 'nvdec' in c])"
```

### ğŸ¨ **ë²„ì „ ë§¤íŠ¸ë¦­ìŠ¤ í˜„ì‹¤í™”** (ê²€ì¦ëœ ì¡°í•©)

#### ğŸŸ¢ **Production ì•ˆì • ì¡°í•©** (ê²€ì¦ ì™„ë£Œ)

```yaml
stable_production_2024:
  cuda: "12.4.1"                    # LTS ë²„ì „
  tensorrt: "10.0.1.6"              # ì•ˆì • ë¦´ë¦¬ì¦ˆ
  torch: "2.4.1+cu124"              # ì•ˆì • ë¦´ë¦¬ì¦ˆ
  pyav: "12.0.0"                    # NVDEC ì•ˆì • ì§€ì›
  pynvcodec: "12.2.0"               # VPF ì§€ì›
  status: "PRODUCTION_READY"
  tested_gpus: ["RTX 4090", "RTX 4080", "A6000"]
  
stable_production_2025:
  cuda: "12.6.0"                    # ìµœì‹  ì•ˆì •
  tensorrt: "10.5.0"                # í•™ìƒ ì§€ì› ê°œì„ 
  torch: "2.5.1+cu126"              # ì•ˆì • ë¦´ë¦¬ì¦ˆ
  pyav: "12.1.0"                    # ë¹„ë””ì˜¤ ê°œì„ 
  pynvcodec: "12.3.0"               # VPF ê°œì„ 
  status: "PRODUCTION_READY"
  tested_gpus: ["RTX 4090", "RTX 5090"]
```

#### ğŸ”¶ **Experimental ìµœì‹  ì¡°í•©** (ê°œë°œìš©)

```yaml  
cutting_edge_2025:
  cuda: "12.8.0"                    # ìµœì‹  ë¦´ë¦¬ì¦ˆ
  tensorrt: "10.7.0"                # ìµœì‹  ê¸°ëŠ¥
  torch: "2.7.1+cu128"              # Nightly/RC ë²„ì „
  pyav: "12.3.0"                    # ìµœì‹  ê¸°ëŠ¥
  pynvcodec: "12.4.0"               # ìµœì‹  VPF
  status: "EXPERIMENTAL"
  tested_gpus: ["RTX 5090"]
  caution: "RTX 5090 ì „ìš©, ì•ˆì •ì„± ë¯¸ë³´ì¥"
```

#### ğŸ“„ **requirements_production.txt** (ì•ˆì • ì¡°í•©)

```txt
# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (2025 ì•ˆì • ì¡°í•©)
torch==2.5.1+cu126
torchvision==0.20.1+cu126  
numpy==1.26.4
opencv-python==4.10.0.84

# ì œë¡œì¹´í”¼ ë””ì½”ë” (1ìˆœìœ„: PyNvCodec)
pynvcodec==12.3.0
av==12.1.0                     # ë°±ì—…

# TensorRT ìµœì í™”  
tensorrt==10.5.0
torch2trt==0.4.0

# ì–¼êµ´ ê²€ì¶œ
ultralytics==8.2.0
insightface==0.7.3

# íŠ¸ë˜ì»¤ & ëª¨ë‹ˆí„°ë§
deep-sort-realtime==1.3.2
py3nvml==0.2.8
psutil==6.1.1

# ê³ ì„±ëŠ¥ I/O
cupy-cuda12x==12.3.0           # DLPack ì§€ì›
aiofiles==24.1.0
```

#### âš™ï¸ **í•˜ë“œì›¨ì–´ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤**

| GPU ëª¨ë¸ | VRAM | NVDEC | NVENC | ì¶”ì²œ ì¡°í•© | ë™ì‹œ ìŠ¤íŠ¸ë¦¼ |
|----------|------|-------|-------|------------|---------------|
| **RTX 4090** | 24GB | 2-3ê°œ | 2ê°œ | Production 2025 | **2-3ê°œ** |
| **RTX 5090** | 32GB | 4-6ê°œ | 2-3ê°œ | Experimental 2025 | **3-4ê°œ** |
| **RTX 4080** | 16GB | 2ê°œ | 1ê°œ | Production 2024 | **1-2ê°œ** |
| **A6000** | 48GB | 3-4ê°œ | 1ê°œ | Production 2024 | **3-4ê°œ** |

#### ğŸ”§ **ìë™ í™˜ê²½ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**

```bash
#!/bin/bash
# validate_environment.sh - ìë™í™”ëœ í˜¸í™˜ì„± ê²€ì¦

echo "ğŸ” í•˜ë“œì›¨ì–´ í˜¸í™•ì„± ê²€ì¦ ì‹œì‘..."

# 1. GPU ëª¨ë¸ ê°ì§€
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)
echo "GPU: $GPU_NAME"

# 2. ë™ì‹œ ì„¸ì…˜ í•œë„ ì¸¡ì • (HardwareProber ì‚¬ìš©)
python3 -c "
from hardware_prober import HardwareProber
prober = HardwareProber()
print(f'í™•ì¸ëœ ì„¸ì…˜: NVDEC={prober.gpu_info[\"nvdec_max_sessions\"]}, NVENC={prober.gpu_info[\"nvenc_max_sessions\"]}')
print(f'ìµœì  ë™ì‹œ ìŠ¤íŠ¸ë¦¼: {prober.optimal_streams}ê°œ')
"

# 3. ì¡°í•© í˜¸í™˜ì„± í™•ì¸
if [[ "$GPU_NAME" == *"RTX 5090"* ]]; then
    echo "âœ… RTX 5090 ê°ì§€: Experimental 2025 ì¡°í•© ì‚¬ìš©"
    export REQUIREMENTS_FILE="requirements_experimental.txt"
elif [[ "$GPU_NAME" == *"RTX 4090"* ]]; then
    echo "âœ… RTX 4090 ê°ì§€: Production 2025 ì¡°í•© ì‚¬ìš©"
    export REQUIREMENTS_FILE="requirements_production.txt"
else
    echo "âš ï¸ ë¯¸ì§€ì› GPU: Production 2024 ì¡°í•© ì‚¬ìš©"
    export REQUIREMENTS_FILE="requirements_stable.txt"
fi

echo "âœ… í™˜ê²½ ê²€ì¦ ì™„ë£Œ: $REQUIREMENTS_FILE ì‚¬ìš© ë°”ëë‹ˆë‹¤"
```

**í™˜ê²½ ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸**:
```bash
#!/bin/bash
# setup_environment.sh - ì™„ì „ ìë™í™”ëœ í™˜ê²½ êµ¬ì„±

set -e

echo "ğŸš€ Dual-Face Tracker í™˜ê²½ ì„¤ì • ì‹œì‘"

# 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦
echo "1ï¸âƒ£  ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì¤‘..."
./scripts/validate_hardware.sh

# 2. NVIDIA ë“œë¼ì´ë²„ í™•ì¸
echo "2ï¸âƒ£  NVIDIA ë“œë¼ì´ë²„ í™•ì¸ ì¤‘..."
if ! nvidia-smi | grep -q "565.57"; then
    echo "âŒ NVIDIA ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸ í•„ìš” (565.57.01+)"
    exit 1
fi

# 3. Docker í™˜ê²½ êµ¬ì¶•
echo "3ï¸âƒ£  Docker í™˜ê²½ êµ¬ì¶• ì¤‘..."
docker build -t dual-face-tracker:latest \
    --build-arg CUDA_VERSION=12.8 \
    --build-arg TENSORRT_VERSION=10.7.0 \
    --build-arg FFMPEG_VERSION=6.1.1 \
    -f Dockerfile .

# 4. ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
echo "4ï¸âƒ£  ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì¤‘..."
docker run --rm --gpus all dual-face-tracker:latest \
    python3 /app/tests/test_environment.py

# 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
echo "5ï¸âƒ£  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘..."
docker run --rm --gpus all -v $(pwd)/benchmark:/benchmark \
    dual-face-tracker:latest python3 /app/benchmark/system_benchmark.py

echo "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
echo "ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼: benchmark/results.json"
```

### âš ï¸ **ë²„ì „ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤**

| êµ¬ì„±ìš”ì†Œ | ê²€ì¦ëœ ë²„ì „ | í˜¸í™˜ ë²”ìœ„ | ì•Œë ¤ì§„ ì´ìŠˆ |
|----------|-------------|-----------|-------------|
| **CUDA** | 12.8 | 12.6-13.0 | 12.5 ì´í•˜ëŠ” RTX 5090 ë¯¸ì§€ì› |
| **TensorRT** | 10.7.0 | 10.5-11.0 | 10.4 ì´í•˜ëŠ” CUDA 12.8 ë¹„í˜¸í™˜ |
| **PyAV** | 12.3.0 | 12.0-13.0 | 11.xëŠ” NVDEC hwaccel ë¶ˆì•ˆì • |
| **FFmpeg** | 6.1.1 | 6.0-7.0 | 5.xëŠ” NVDEC ì„±ëŠ¥ ì €í•˜ |
| **PyTorch** | 2.7.1+cu128 | 2.5-3.0 | 2.4 ì´í•˜ëŠ” TensorRT ë³€í™˜ ì˜¤ë¥˜ |

**ì•Œë ¤ì§„ ë²„ì „ ì¶©ëŒ**:
```python
# í”¼í•´ì•¼ í•  ì¡°í•©
INCOMPATIBLE_COMBINATIONS = [
    ("tensorrt==8.x", "cuda==12.8"),     # TensorRT 8.xëŠ” CUDA 12.8 ë¯¸ì§€ì›  
    ("av==11.x", "hwaccel=cuda"),        # PyAV 11.x NVDEC ë¶ˆì•ˆì •
    ("torch==2.3.x", "tensorrt==10.x"),  # PyTorch 2.3.x TensorRT í˜¸í™˜ì„± ì´ìŠˆ
]
```

---

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¥ì•  ë³µêµ¬ (êµ¬ì²´í™”)

### ğŸ” ì‹¤ì‹œê°„ í•˜ë“œì›¨ì–´ ëª¨ë‹ˆí„°ë§

```python
class HardwareMonitor:
    def track_nvdec_usage(self):
        """NVDEC 4ê°œ ì—”ì§„ë³„ í™œìš©ë¥  ì¶”ì """
        return {
            'nvdec_0': self.get_decoder_utilization(0),
            'nvdec_1': self.get_decoder_utilization(1), 
            'nvdec_2': self.get_decoder_utilization(2),
            'nvdec_3': self.get_decoder_utilization(3),
        }
        
    def track_nvenc_usage(self):
        """NVENC 2ê°œ ì—”ì§„ë³„ í™œìš©ë¥  ì¶”ì """
        return {
            'nvenc_0': self.get_encoder_utilization(0),
            'nvenc_1': self.get_encoder_utilization(1),
        }
        
    def track_per_stage_latency(self):
        """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì§€ì—°ì‹œê°„"""
        return {
            'decode_latency': self.measure_decode_time(),      # ëª©í‘œ: <5ms
            'inference_latency': self.measure_inference_time(), # ëª©í‘œ: <15ms
            'compose_latency': self.measure_compose_time(),     # ëª©í‘œ: <3ms
            'encode_latency': self.measure_encode_time(),       # ëª©í‘œ: <7ms
        }
```

### ğŸ“ˆ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¶”ì 

```python
class PerformanceTracker:
    def __init__(self):
        self.stream_fps = {}        # ìŠ¤íŠ¸ë¦¼ë³„ ì‹¤ì‹œê°„ FPS
        self.queue_depth = {}       # í”„ë ˆì„ í ê¹Šì´
        self.id_switch_count = {}   # ID ìŠ¤ìœ„ì¹˜ ë°œìƒë¥ 
        self.batch_efficiency = 0   # ë°°ì¹˜ í™œìš©ë¥ 
        
    def track_stream_performance(self, stream_id):
        return {
            'fps': self.calculate_fps(stream_id),
            'queue_depth': len(self.frame_queues[stream_id]),
            'processing_latency': self.get_end_to_end_latency(stream_id),
            'id_switches_per_minute': self.calculate_id_switch_rate(stream_id),
        }
        
    def detect_bottlenecks(self):
        """ìë™ ë³‘ëª© ê°ì§€"""
        if self.queue_depth > 10:
            return "processing_bottleneck"
        elif self.get_vram_usage() > 0.9:
            return "memory_bottleneck" 
        elif self.get_nvdec_utilization() < 0.5:
            return "decode_underutilization"
```

### ğŸš¨ ì¥ì•  ë³µêµ¬ ì‹œìŠ¤í…œ (êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤)

```python
class StreamRecoveryManager:
    def handle_oom_error(self, stream_id):
        """GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë³µêµ¬"""
        # 1. ì¦‰ì‹œ ëŒ€ì‘
        self.pause_stream(stream_id)
        torch.cuda.empty_cache()
        
        # 2. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        self.reduce_batch_size(stream_id, factor=0.7)
        
        # 3. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
        last_checkpoint = self.get_last_checkpoint(stream_id)
        self.restart_stream_from_checkpoint(stream_id, last_checkpoint)
        
        # 4. ì‹¤íŒ¨ ì‹œ ìŠ¤íŠ¸ë¦¼ ë¹„í™œì„±í™”
        if self.retry_count[stream_id] > 3:
            self.disable_stream(stream_id)
            self.alert_admin(f"Stream {stream_id} disabled due to repeated OOM")
    
    def handle_nvdec_failure(self, stream_id):
        """NVDEC í•˜ë“œì›¨ì–´ ì‹¤íŒ¨ ì‹œ fallback"""
        # 1. CPU ë””ì½”ë”©ìœ¼ë¡œ ì„ì‹œ ì „í™˜
        self.enable_cpu_fallback(stream_id)
        
        # 2. ë‹¤ë¥¸ NVDEC ì—”ì§„ìœ¼ë¡œ ì¬í• ë‹¹ ì‹œë„
        available_decoder = self.find_available_nvdec_engine()
        if available_decoder:
            self.reassign_decoder(stream_id, available_decoder)
            self.disable_cpu_fallback(stream_id)
            
    def health_check_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í—¬ìŠ¤ì²´í¬"""
        checks = {
            'nvdec_responsive': self.check_nvdec_health(),
            'tensorrt_engines': self.check_tensorrt_health(),
            'nvenc_responsive': self.check_nvenc_health(),
            'vram_available': self.get_vram_usage() < 0.9,
            'streams_active': len(self.active_streams) > 0,
        }
        
        if not all(checks.values()):
            self.trigger_recovery_procedure(checks)
            
    def handle_conditional_reid_failure(self, stream_id):
        """ì¡°ê±´ë¶€ ReID ì‹¤íŒ¨ ì‹œ ë³µêµ¬"""
        print(f"ğŸ”„ ì¡°ê±´ë¶€ ReID ì‹¤íŒ¨: ìŠ¤íŠ¸ë¦¼ {stream_id}")
        
        # 1. ReID ëª¨ë¸ ì¬ë¡œë“œ ì‹œë„
        try:
            self.conditional_reid.reload_reid_model()
            print(f"âœ… ReID ëª¨ë¸ ì¬ë¡œë“œ ì„±ê³µ: ìŠ¤íŠ¸ë¦¼ {stream_id}")
        except Exception as e:
            print(f"âŒ ReID ëª¨ë¸ ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
            # 2. ByteTrack ì „ìš© ëª¨ë“œë¡œ ì „í™˜
            self.conditional_reid.disable_reid_for_stream(stream_id)
            print(f"ğŸ”„ ìŠ¤íŠ¸ë¦¼ {stream_id} ByteTrack ì „ìš© ëª¨ë“œ ì „í™˜")
            
    def handle_config_manager_failure(self):
        """ì„¤ì • ê´€ë¦¬ì ì‹¤íŒ¨ ì‹œ ë³µêµ¬"""
        print("ğŸ”§ ì„¤ì • ê´€ë¦¬ì ì‹¤íŒ¨ ê°ì§€")
        
        # 1. ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ì¬ê²€ì¦
        config_status = self.config_manager.validate_all_configs()
        
        if not config_status['manual_config_valid']:
            print("âš ï¸ ìˆ˜ë™ ì„¤ì • ì˜¤ë¥˜ - ìë™ í”„ë¡œë¹™ìœ¼ë¡œ ì „í™˜")
            self.config_manager.fallback_to_auto_probing()
            
        if not config_status['auto_config_valid']:
            print("âš ï¸ ìë™ ì„¤ì • ì˜¤ë¥˜ - ê¸°ë³¸ê°’ìœ¼ë¡œ ì „í™˜")
            self.config_manager.fallback_to_defaults()
            
    def handle_environment_validation_failure(self):
        """í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë³µêµ¬"""
        print("ğŸ” í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨ - ìë™ ë³µêµ¬ ì‹œë„")
        
        # 1. PyNvCodec ìƒíƒœ ì¬í™•ì¸
        pynvcodec_status = self.check_pynvcodec_health()
        if not pynvcodec_status['available']:
            print("âŒ PyNvCodec ì‚¬ìš© ë¶ˆê°€ - PyAV ë°±ì—…ìœ¼ë¡œ ì „í™˜")
            self.switch_to_pyav_fallback()
            
        # 2. TensorRT ì—”ì§„ ìƒíƒœ í™•ì¸
        tensorrt_status = self.check_tensorrt_engines()
        if not tensorrt_status['all_engines_loaded']:
            print("ğŸ”„ TensorRT ì—”ì§„ ì¬ë¡œë“œ ì¤‘...")
            self.reload_tensorrt_engines()
            
        # 3. í™˜ê²½ ê²€ì¦ ì¬ì‹¤í–‰
        try:
            subprocess.run(["bash", "validate_environment.sh"], check=True, timeout=60)
            print("âœ… í™˜ê²½ ê²€ì¦ ë³µêµ¬ ì„±ê³µ")
        except subprocess.CalledProcessError:
            print("âŒ í™˜ê²½ ê²€ì¦ ë³µêµ¬ ì‹¤íŒ¨ - ìˆ˜ë™ ê°œì… í•„ìš”")
            self.notify_admin_intervention_required()
            
    def handle_tile_composition_system_failure(self, failure_analytics):
        """íƒ€ì¼ í•©ì„± ì‹œìŠ¤í…œ ì „ì²´ ì‹¤íŒ¨ ì‹œ ë³µêµ¬"""
        print("ğŸ›¡ï¸ íƒ€ì¼ í•©ì„± ì‹œìŠ¤í…œ ì‹¤íŒ¨ - ì‹œìŠ¤í…œ ë ˆë²¨ ë³µêµ¬")
        
        # ì‹¤íŒ¨ìœ¨ì´ 50% ì´ìƒì´ë©´ ì‹œìŠ¤í…œ ì¬ì‹œì‘
        if failure_analytics.failure_metrics['failure_rate'] > 0.5:
            print("ğŸš¨ ì‹¤íŒ¨ìœ¨ 50% ì´ˆê³¼ - íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘")
            return self.restart_entire_pipeline()
            
        # ì‹¤íŒ¨ìœ¨ì´ 20-50%ë©´ ë¶€ë¶„ ë³µêµ¬
        elif failure_analytics.failure_metrics['failure_rate'] > 0.2:
            print("âš ï¸ ì‹¤íŒ¨ìœ¨ 20% ì´ˆê³¼ - ë¶€ë¶„ ë³µêµ¬ ëª¨ë“œ")
            return self.enable_degraded_mode()
            
        # ì‹¤íŒ¨ìœ¨ì´ 20% ì´í•˜ë©´ ê°œë³„ ìŠ¤íŠ¸ë¦¼ ë³µêµ¬
        else:
            print("ğŸ”„ ê°œë³„ ìŠ¤íŠ¸ë¦¼ ë³µêµ¬ ëª¨ë“œ")
            return self.recover_individual_streams()
            
    def integrated_recovery_procedure(self, failure_type, context):
        """í†µí•© ë³µêµ¬ í”„ë¡œì‹œì €"""
        recovery_strategy = {
            'conditional_reid_failure': self.handle_conditional_reid_failure,
            'config_manager_failure': self.handle_config_manager_failure,
            'environment_validation_failure': self.handle_environment_validation_failure,
            'tile_composition_failure': self.handle_tile_composition_system_failure,
            'oom_error': self.handle_oom_error,
            'nvdec_failure': self.handle_nvdec_failure
        }
        
        if failure_type in recovery_strategy:
            print(f"ğŸ”§ í†µí•© ë³µêµ¬ ì‹œì‘: {failure_type}")
            recovery_strategy[failure_type](context)
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤íŒ¨ ìœ í˜•: {failure_type}")
            self.handle_unknown_failure(failure_type, context)
```

### ğŸ“Š ëª¨ë‹ˆí„°ë§ ì§€í‘œ ë‹¨ìˆœí™” (6ê°€ì§€ í•µì‹¬)

#### âœ… **í•„ìˆ˜ ì§€í‘œ** (ê³¼ì„¸ë¶„í™” ì œê±°)

```yaml
# 6ê°€ì§€ í•µì‹¬ ì§€í‘œ (ì—”ì§„ë³„ ë¶„ë¦¬ ì—†ìŒ)
essential_metrics:
  1. gpu_utilization_percent           # GPU ì „ì²´ í™œìš©ë¥ 
  2. vram_used_gb                      # VRAM ì‚¬ìš©ëŸ‰ (ì´ëŸ‰ë§Œ)
  3. nvdec_utilization_percent         # NVDEC í†µí•© í™œìš©ë¥ 
  4. nvenc_utilization_percent         # NVENC í†µí•© í™œìš©ë¥ 
  5. processing_latency_p95_ms         # ì „ì²´ íŒŒì´í”„ë¼ì¸ p95 ë ˆì´í„´ì‹œ
  6. queue_depth_p99_frames            # ëŒ€ê¸° í p99 ê¹Šì´

# ì„ íƒì  ë””ë²„ê¹… ì§€í‘œ (ê°œë°œ ì‹œì—ë§Œ)
debug_metrics:
  - h2d_copy_bytes_per_sec            # CPUâ†’GPU ë³µì‚¬ (ì œë¡œì¹´í”¼ ê²€ì¦)
  - d2h_copy_bytes_per_sec            # GPUâ†’CPU ë³µì‚¬ (ì œë¡œì¹´í”¼ ê²€ì¦)
  - model_inference_fps               # ëª¨ë¸ ì¶”ë¡  FPS
```

#### ğŸš¨ **ì•ŒëŒ ê¸°ì¤€** (ë‹¨ìˆœí™”)

```python
# ê°„ë‹¨í•œ ì•ŒëŒ ê¸°ì¤€
ALERT_THRESHOLDS = {
    'gpu_utilization_percent': 95,      # 95% ì´ìƒì‹œ ì•ŒëŒ
    'vram_used_gb': 28,                 # 28GB ì´ìƒì‹œ ì•ŒëŒ (32GB ì¤‘)
    'processing_latency_p95_ms': 100,   # 100ms ì´ìƒì‹œ ì•ŒëŒ
    'queue_depth_p99_frames': 20        # 20í”„ë ˆì„ ì´ìƒì‹œ ì•ŒëŒ
}
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€ ë° ê²€ì¦ (ì •í™•í•œ ê¸°ì¤€)

### ì •ëŸ‰ì  ì„±ê³µ ê¸°ì¤€
1. **ì²˜ë¦¬ëŸ‰ í–¥ìƒ**: ê¸°ì¡´ ëŒ€ë¹„ **8ë°° ì´ìƒ** (Full GPU íš¨ê³¼)
2. **NVDEC í™œìš©ë¥ **: **85% ì´ìƒ** ìœ ì§€ (4ê°œ ì—”ì§„ í‰ê· )
3. **VRAM íš¨ìœ¨ì„±**: **87.5% ì´í•˜** ì‚¬ìš© (28GB/32GB)  
4. **End-to-End ì§€ì—°**: í”„ë ˆì„ë‹¹ **30ms ì´í•˜**
5. **ID ì•ˆì •ì„±**: ë¶„ë‹¹ ìŠ¤ìœ„ì¹˜ **0.05íšŒ ì´í•˜**

### ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤
1. **4ê°œ ì˜ìƒ ë™ì‹œ ì²˜ë¦¬**: 1080p x 4ìŠ¤íŠ¸ë¦¼ 15ë¶„ ì—°ì†
2. **ë©”ëª¨ë¦¬ ì•ˆì •ì„±**: 8ì‹œê°„ ì—°ì† ì²˜ë¦¬ without OOM
3. **ì¥ì•  ë³µêµ¬**: NVDEC ì‹¤íŒ¨ ì‹œ ìë™ ë³µêµ¬ í™•ì¸
4. **ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**: YOLOv8n vs YOLOv8s vs SCRFD ë²¤ì¹˜ë§ˆí‚¹

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ê´€ë¦¬ (êµ¬ì²´ì  ëŒ€ì‘ì±…)

### ê³ ìœ„í—˜ ìš”ì†Œ ë° ëŒ€ì‘
1. **PyAV NVDEC ì„¤ì¹˜ ì‹¤íŒ¨**: â†’ PyNvCodec ë°±ì—…, ìµœì¢… CPU ë””ì½”ë”©
2. **TensorRT ë³€í™˜ ì‹¤íŒ¨**: â†’ PyTorch FP16 ë°±ì—… ê²½ë¡œ  
3. **VRAM ë¶€ì¡±**: â†’ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •, ìŠ¤íŠ¸ë¦¼ ìˆ˜ ê°ì†Œ
4. **NVENC ë™ì‹œ í•œê³„**: â†’ 2ê°œ ì—”ì§„ ë¡œë“œ ë°¸ëŸ°ì‹±, ëŒ€ê¸°ì—´ ê´€ë¦¬

### ì„±ëŠ¥ ë°±ì—… ì‹œë‚˜ë¦¬ì˜¤
- **Plan A**: 4ìŠ¤íŠ¸ë¦¼ Full GPU (ëª©í‘œ)
- **Plan B**: 3ìŠ¤íŠ¸ë¦¼ Full GPU (ì•ˆì •)
- **Plan C**: 2ìŠ¤íŠ¸ë¦¼ + CPU ë°±ì—… (ìµœì†Œ)

---

**í”„ë¡œì íŠ¸ ìƒíƒœ**: Full GPU íŒŒì´í”„ë¼ì¸ ìƒì„¸ ì„¤ê³„ ì™„ë£Œ  
**ì˜ˆìƒ ì™„ë£Œ**: 10ì¼ ì´ë‚´  
**í•µì‹¬ í˜ì‹ **: PyAV NVDEC â†’ TensorRT â†’ NVENC ì œë¡œì¹´í”¼ íŒŒì´í”„ë¼ì¸  
**ì„±ëŠ¥ ëª©í‘œ**: 6.2ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ (2.6 â†’ 16ê°œ/ì‹œê°„) | ë‹¨ì¼ ì˜ìƒ: 6.1ë°° ë‹¨ì¶• (23ë¶„ â†’ 3.75ë¶„)  
**ê¸°ìˆ  ìŠ¤íƒ**: PyNvCodec(VPF) + TensorRT + CUDA Composition + NVENC H.264
