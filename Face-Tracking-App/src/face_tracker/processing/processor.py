"""
ìµœì í™”ëœ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° - FFmpeg ê¸°ë°˜
"""
import os
import time
import shutil
import subprocess
import warnings
import cv2
import numpy as np
from multiprocessing import Pool, cpu_count, set_start_method
from concurrent.futures import ThreadPoolExecutor

# pkg_resources ê²½ê³  ì–µì œ
warnings.filterwarnings("ignore", message="pkg_resources is deprecated")

# CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

from src.face_tracker.utils.audio import get_voice_segments_ffmpeg
from src.face_tracker.processing.analyzer import analyze_video_faces
from src.face_tracker.processing.timeline import generate_id_timeline
from src.face_tracker.processing.trimmer import (
    create_condensed_video_ffmpeg, 
    trim_by_face_timeline_ffmpeg, 
    slice_video_parallel_ffmpeg
)
from src.face_tracker.processing.tracker import track_and_crop_video
from src.face_tracker.processing.selector import TargetSelector
from src.face_tracker.utils.logging import logger
from src.face_tracker.utils.performance_reporter import start_video_report, get_current_reporter
from src.face_tracker.config import (
    DEVICE, INPUT_DIR, OUTPUT_ROOT, TEMP_ROOT, 
    SUPPORTED_VIDEO_EXTENSIONS, BATCH_SIZE_ANALYZE, 
    BATCH_SIZE_ID_TIMELINE, VIDEO_CODEC, AUDIO_CODEC, TRACKING_MODE
)


def process_single_segment_ffmpeg(task_data):
    """
    FFmpegë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)
    
    Args:
        task_data: ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ì— í•„ìš”í•œ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    try:
        seg_fname = task_data['seg_fname']
        seg_input = task_data['seg_input']
        seg_cropped = task_data['seg_cropped']
        output_path = task_data['output_path']
        
        logger.info(f"ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬: {seg_fname}")
        
        # 1) ëª¨ë¸ ì´ˆê¸°í™” (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ)
        from src.face_tracker.core.models import ModelManager
        model_manager = ModelManager(DEVICE)
        mtcnn = model_manager.get_mtcnn()
        resnet = model_manager.get_resnet()
        
        # 2) ì–¼êµ´ í¬ë¡­ (íŒŒì¼ ì¡´ì¬ í™•ì¸)
        if not os.path.exists(seg_input):
            logger.error(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {seg_input}")
            return
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
        seg_cropped_dir = os.path.dirname(seg_cropped)
        os.makedirs(seg_cropped_dir, exist_ok=True)
        logger.info(f"ì–¼êµ´ í¬ë¡­ ì‹œì‘: {seg_input} -> {seg_cropped}")
            
        track_and_crop_video(seg_input, seg_cropped, mtcnn, resnet, DEVICE)
        
        # í¬ë¡­ëœ íŒŒì¼ ìƒì„± í™•ì¸
        if not os.path.exists(seg_cropped):
            logger.error(f"ì–¼êµ´ í¬ë¡­ ì‹¤íŒ¨: {seg_fname} - í¬ë¡­ëœ íŒŒì¼ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            return
        
        # 3) FFmpegë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ë™ê¸°í™” (MoviePy ëŒ€ì‹ )
        cmd = [
            'ffmpeg', '-y',
            '-i', seg_cropped,  # í¬ë¡­ëœ ë¹„ë””ì˜¤
            '-i', seg_input,    # ì›ë³¸ ì˜¤ë””ì˜¤ê°€ ìˆëŠ” ë¹„ë””ì˜¤
            '-c:v', 'copy',     # ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”© ì—†ì´ ë³µì‚¬
            '-c:a', AUDIO_CODEC,
            '-map', '0:v:0',    # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤
            '-map', '1:a:0',    # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤
            '-shortest',        # ë” ì§§ì€ ìŠ¤íŠ¸ë¦¼ì— ë§ì¶¤
            output_path
        ]
        
        # 4) FFmpeg ì‹¤í–‰
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # 5) ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(seg_cropped):
            os.remove(seg_cropped)
            
        logger.success(f"ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ: {seg_fname}")
        
    except subprocess.CalledProcessError as e:
        logger.error(f"{seg_fname} FFmpeg ì˜¤ë¥˜: {e.stderr}")
    except Exception as e:
        logger.error(f"{seg_fname} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")


def process_single_video_optimized(fname: str):
    """
    ìµœì í™”ëœ ë‹¨ì¼ ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    
    Args:
        fname: ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ëª…
    """
    basename = os.path.splitext(fname)[0]
    temp_dir = os.path.join(TEMP_ROOT, basename)
    os.makedirs(temp_dir, exist_ok=True)

    input_path = os.path.join(INPUT_DIR, fname)
    condensed = os.path.join(temp_dir, f"condensed_{fname}")
    trimmed = os.path.join(temp_dir, f"trimmed_{fname}")

    logger.stage(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {fname}")
    start_time = time.time()
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì‹œì‘
    reporter = start_video_report(fname)
    
    try:
        # 0) FFmpegë¥¼ ì‚¬ìš©í•œ ê³ ì† ì˜¤ë””ì˜¤ VAD
        reporter.start_stage("ì˜¤ë””ì˜¤ VAD")
        logger.stage("ì˜¤ë””ì˜¤ VAD ì²˜ë¦¬...")
        voice_timeline = get_voice_segments_ffmpeg(input_path)
        logger.info(f"ìŒì„± êµ¬ê°„ {len(voice_timeline)}ê°œ ê°ì§€")
        reporter.end_stage("ì˜¤ë””ì˜¤ VAD", segments=len(voice_timeline))
        
        # 1) ì–¼êµ´ ê°ì§€ íƒ€ì„ë¼ì¸
        reporter.start_stage("ì–¼êµ´ ê°ì§€")
        logger.stage("ì–¼êµ´ ê°ì§€ ë¶„ì„...")
        timeline, fps = analyze_video_faces(input_path, batch_size=BATCH_SIZE_ANALYZE, device=DEVICE)
        reporter.end_stage("ì–¼êµ´ ê°ì§€", frames=len(timeline), batch_size=BATCH_SIZE_ANALYZE)
        
        # 2) FFmpegë¥¼ ì‚¬ìš©í•œ ê³ ì† ìš”ì•½ë³¸ ìƒì„±
        reporter.start_stage("ìš”ì•½ë³¸ ìƒì„±")
        logger.stage("ìš”ì•½ë³¸ ìƒì„±...")
        if create_condensed_video_ffmpeg(input_path, condensed, timeline, fps):
            logger.success("ìš”ì•½ë³¸ ìƒì„± ì™„ë£Œ")
            reporter.end_stage("ìš”ì•½ë³¸ ìƒì„±", frames=len(timeline))
            
            # 2a) ID íƒ€ì„ë¼ì¸ ìƒì„± ë° íƒ€ê²Ÿ ì¸ë¬¼ ìë™ ì„ íƒ
            reporter.start_stage("ì–¼êµ´ ì¸ì‹")
            logger.stage("ì–¼êµ´ ì¸ì‹ ë° íƒ€ê²Ÿ ì„ íƒ...")
            id_timeline, fps2 = generate_id_timeline(condensed, DEVICE, batch_size=BATCH_SIZE_ID_TIMELINE)
            reporter.end_stage("ì–¼êµ´ ì¸ì‹", frames=len(id_timeline), batch_size=BATCH_SIZE_ID_TIMELINE)
            
            # ëª¨ë“œë³„ ì²˜ë¦¬ ë¶„ê¸°
            from src.face_tracker.core.embeddings import SmartEmbeddingManager
            emb_manager = SmartEmbeddingManager()
            embeddings = emb_manager.get_all_embeddings()
            
            # ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ TRACKING_MODE ê°’ ê°€ì ¸ì˜¤ê¸°
            from src.face_tracker.config import TRACKING_MODE as current_mode
            
            print(f"ğŸ” DEBUG: í˜„ì¬ TRACKING_MODE: {current_mode}")
            logger.info(f"ğŸ” í˜„ì¬ TRACKING_MODE: {current_mode}")
            
            if current_mode == "dual":
                print("ğŸ” DEBUG: DUAL ëª¨ë“œ ë¶„ê¸° ì§„ì…")
                logger.info("ğŸ” DEBUG: DUAL ëª¨ë“œ ë¶„ê¸° ì§„ì…")
                # DUAL ëª¨ë“œ ì „ìš© ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
                process_dual_mode_segments(
                    id_timeline, fps2, condensed, 
                    temp_dir, basename, reporter, len(timeline)
                )
                
            elif current_mode == "dual_split":
                print("ğŸ¯ DEBUG: DUAL_SPLIT ëª¨ë“œ ë¶„ê¸° ì§„ì…!")
                logger.info("ğŸ¯ DUAL_SPLIT ëª¨ë“œ ë¶„ê¸° ì§„ì…!")
                # DUAL_SPLIT ëª¨ë“œ ì „ìš© ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
                try:
                    process_dual_split_mode_segments(
                        id_timeline, fps2, condensed, 
                        temp_dir, basename, reporter, len(timeline)
                    )
                    print("ğŸ¯ DEBUG: DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ!")
                    logger.info("ğŸ¯ DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ!")
                except Exception as e:
                    print(f"âŒ DEBUG: DUAL_SPLIT ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    logger.error(f"DUAL_SPLIT ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    import traceback
                    traceback.print_exc()
                
            else:
                print(f"ğŸ” DEBUG: SINGLE ëª¨ë“œ ë˜ëŠ” ê¸°íƒ€ ëª¨ë“œ ë¶„ê¸° ì§„ì…: {current_mode}")
                logger.info(f"ğŸ” DEBUG: SINGLE ëª¨ë“œ ë˜ëŠ” ê¸°íƒ€ ëª¨ë“œ ë¶„ê¸° ì§„ì…: {current_mode}")
                # SINGLE ëª¨ë“œ: ê¸°ì¡´ ë¡œì§
                target_id = TargetSelector.select_target(id_timeline, current_mode, embeddings)
                if target_id is not None:
                    stats = TargetSelector.get_target_stats(id_timeline, target_id)
                    logger.success(f"íƒ€ê²Ÿ ì„ íƒ: ID={target_id} ({stats['coverage_ratio']:.1%} ì»¤ë²„ë¦¬ì§€)")
                else:
                    logger.warning(f"íƒ€ê²Ÿ ì°¾ê¸° ì‹¤íŒ¨ (ëª¨ë“œ={current_mode})")
                
                # íƒ€ê²Ÿ ì•„ë‹Œ í”„ë ˆì„ì€ Noneìœ¼ë¡œ í‘œì‹œ
                if target_id is not None:
                    id_timeline_bool = [tid if tid == target_id else None for tid in id_timeline]
                else:
                    id_timeline_bool = id_timeline
            
                # SINGLE ëª¨ë“œ: ê¸°ì¡´ íŠ¸ë¦¬ë° ë° ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
                # FFmpegë¥¼ ì‚¬ìš©í•œ ê³ ì† íŠ¸ë¦¬ë°
                reporter.start_stage("ë¹„ë””ì˜¤ íŠ¸ë¦¬ë°")
                logger.stage("ë¹„ë””ì˜¤ íŠ¸ë¦¬ë°...")
                if trim_by_face_timeline_ffmpeg(condensed, id_timeline_bool, fps2, threshold_frames=30, output_path=trimmed):
                    source_for_crop = trimmed
                else:
                    source_for_crop = condensed
                reporter.end_stage("ë¹„ë””ì˜¤ íŠ¸ë¦¬ë°")
                
                # 3) FFmpegë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
                reporter.start_stage("ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• ")
                logger.stage("ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• ...")
                segment_temp_folder = os.path.join(temp_dir, "segments")
                os.makedirs(segment_temp_folder, exist_ok=True)
                slice_video_parallel_ffmpeg(source_for_crop, segment_temp_folder, segment_length=10)
                reporter.end_stage("ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• ")

                # 4) ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ì–¼êµ´ í¬ë¡­ ë° ì˜¤ë””ì˜¤ ë™ê¸° ë³‘í•© (ë³‘ë ¬ ì²˜ë¦¬)
                reporter.start_stage("ì–¼êµ´ í¬ë¡­")
                logger.stage("ì–¼êµ´ í¬ë¡­ ë° ì˜¤ë””ì˜¤ ë™ê¸°í™”...")
                final_segment_folder = os.path.join(OUTPUT_ROOT, basename)
                os.makedirs(final_segment_folder, exist_ok=True)
                
                segment_files = [f for f in os.listdir(segment_temp_folder) 
                               if f.lower().endswith(".mp4")]
                segment_files.sort()  # ìˆœì„œ ë³´ì¥
                
                logger.info(f"ì„¸ê·¸ë¨¼íŠ¸ {len(segment_files)}ê°œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
                
                # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•œ ì‘ì—… ë°ì´í„° ì¤€ë¹„
                segment_tasks = []
                for seg_fname in segment_files:
                    seg_input = os.path.join(segment_temp_folder, seg_fname)
                    seg_cropped = os.path.join(temp_dir, f"crop_{seg_fname}")
                    output_seg_path = os.path.join(final_segment_folder, seg_fname)
                    
                    segment_tasks.append({
                        'seg_fname': seg_fname,
                        'seg_input': seg_input,
                        'seg_cropped': seg_cropped,
                        'output_path': output_seg_path
                    })
                
                # CPU ì½”ì–´ ê¸°ë°˜ ìµœì  í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³„ì‚°
                num_processes = max(1, min(8, int(cpu_count() * 0.8)))
                
                # ì„±ëŠ¥ ë¦¬í¬íŠ¸ì— ì •ë³´ ì„¤ì •
                reporter.set_processing_info(len(timeline), len(segment_files), num_processes)
                
                with Pool(processes=num_processes) as pool:
                    pool.map(process_single_segment_ffmpeg, segment_tasks)
                        
                reporter.end_stage("ì–¼êµ´ í¬ë¡­", segments=len(segment_files))
                logger.success(f"ì„¸ê·¸ë¨¼íŠ¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ({len(segment_files)}ê°œ)")
        else:
            logger.error(f"ìš”ì•½ë³¸ ìƒì„± ì‹¤íŒ¨: {fname}")

    except Exception as e:
        logger.error(f"{fname} ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    finally:
        elapsed = time.time() - start_time
        logger.success(f"{fname} ì²˜ë¦¬ ì™„ë£Œ ({int(elapsed)}ì´ˆ)")
        
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        if reporter:
            reporter.generate_report()
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

def process_dual_mode_segments(id_timeline, fps, source_video, temp_dir, basename, reporter, total_frames):
    """
    DUAL ëª¨ë“œ ì „ìš©: ìƒìœ„ 2ëª…ì˜ í™”ìì— ëŒ€í•´ ê°ê° 10ì´ˆ ë‹¨ìœ„ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
    
    Args:
        id_timeline: ì–¼êµ´ ID íƒ€ì„ë¼ì¸
        fps: ë¹„ë””ì˜¤ FPS
        source_video: ì†ŒìŠ¤ ë¹„ë””ì˜¤ ê²½ë¡œ (ìš”ì•½ë³¸)
        temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬
        basename: ë² ì´ìŠ¤ íŒŒì¼ëª…
        reporter: ì„±ëŠ¥ ë¦¬í¬í„°
    """
    from src.face_tracker.dual_config import DUAL_PERSON_FOLDER_PREFIX
    from collections import Counter

    reporter.start_stage("DUAL ëª¨ë“œ ì²˜ë¦¬")
    logger.stage("DUAL ëª¨ë“œ: ìƒìœ„ 2ëª… í™”ìë³„ 10ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±...")

    # 1. L2 ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ ID ë³‘í•©ìœ¼ë¡œ ìƒìœ„ 2ëª… face_id ì¶”ì¶œ
    from src.face_tracker.core.embeddings import SmartEmbeddingManager
    from src.face_tracker.processing.selector import TargetSelector
    from src.face_tracker.config import L2_NORMALIZATION_ENABLED
    
    emb_manager = SmartEmbeddingManager()
    embeddings = emb_manager.get_all_embeddings()
    
    # ë™ì  ì„ê³„ê°’ ê³„ì‚° ë° L2 ì •ê·œí™” ì ìš©ëœ ID ë³‘í•©
    if embeddings:
        from src.face_tracker.utils.adaptive_threshold import (
            calculate_optimal_threshold, 
            log_threshold_optimization,
            should_apply_adaptive_threshold
        )
        from src.face_tracker.config import (
            DUAL_MODE_SIMILARITY_THRESHOLD,
            ENABLE_ADAPTIVE_THRESHOLD,
            ADAPTIVE_THRESHOLD_MIN_CONFIDENCE,
            ADAPTIVE_THRESHOLD_MIN_IMPROVEMENT,
            ADAPTIVE_THRESHOLD_SAFETY_RANGE,
            ADAPTIVE_THRESHOLD_MIN_SAMPLES
        )
        
        # ê¸°ë³¸ ì„ê³„ê°’ ì„¤ì •
        default_threshold = DUAL_MODE_SIMILARITY_THRESHOLD
        final_threshold = default_threshold
        optimal_threshold = None
        confidence = "disabled"
        statistics = {}
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚° (ì„¤ì •ì— ë”°ë¼)
        if ENABLE_ADAPTIVE_THRESHOLD:
            optimal_threshold, confidence, statistics = calculate_optimal_threshold(
                embeddings, 
                use_l2_norm=L2_NORMALIZATION_ENABLED,
                min_same_samples=ADAPTIVE_THRESHOLD_MIN_SAMPLES,
                min_different_samples=ADAPTIVE_THRESHOLD_MIN_SAMPLES,
                safety_range=ADAPTIVE_THRESHOLD_SAFETY_RANGE
            )
            
            # ì ìš©í•  ì„ê³„ê°’ ê²°ì •
            if optimal_threshold is not None and should_apply_adaptive_threshold(
                confidence, optimal_threshold, default_threshold,
                min_confidence_level=ADAPTIVE_THRESHOLD_MIN_CONFIDENCE,
                min_improvement_percent=ADAPTIVE_THRESHOLD_MIN_IMPROVEMENT
            ):
                final_threshold = optimal_threshold
                logger.success(f"ë™ì  ì„ê³„ê°’ ì ìš©: {final_threshold:.3f} (ì‹ ë¢°ë„: {confidence})")
            else:
                logger.info(f"ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©: {final_threshold:.3f} (ë™ì : {confidence})")
        else:
            logger.info(f"ë™ì  ì„ê³„ê°’ ë¹„í™œì„±í™”, ê¸°ë³¸ê°’ ì‚¬ìš©: {final_threshold:.3f}")
        
        # ìµœì í™” ê²°ê³¼ ë¡œê¹…
        log_threshold_optimization(
            optimal_threshold, confidence, statistics, 
            default_threshold, L2_NORMALIZATION_ENABLED
        )
        
        # ID ë³‘í•© ìˆ˜í–‰
        merged_timeline = TargetSelector._merge_similar_ids(
            id_timeline, embeddings, 
            similarity_threshold=final_threshold,
            use_l2_norm=L2_NORMALIZATION_ENABLED
        )
        logger.info(f"DUAL ëª¨ë“œ ID ë³‘í•© ì™„ë£Œ (ì„ê³„ê°’: {final_threshold:.3f}, L2: {L2_NORMALIZATION_ENABLED})")
        
        # ì„±ëŠ¥ ë¦¬í¬í„°ì— ì„ê³„ê°’ ìµœì í™” ì •ë³´ ì „ë‹¬
        if optimal_threshold is not None:
            reporter.set_threshold_optimization_info(
                final_threshold, default_threshold, confidence, statistics
            )
    else:
        merged_timeline = id_timeline
    
    valid_ids = [fid for fid in merged_timeline if fid is not None]
    if not valid_ids:
        logger.warning("DUAL ëª¨ë“œ: ì¸ì‹ëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        reporter.end_stage("DUAL ëª¨ë“œ ì²˜ë¦¬", segments=0)
        return

    face_id_counts = Counter(valid_ids)
    top_2_faces = face_id_counts.most_common(2)

    logger.info(f"DUAL ëª¨ë“œ: ìƒìœ„ {len(top_2_faces)}ëª… í™”ì ì„ íƒë¨")
    total_segments_processed = 0

    # 2. ê° í™”ìì— ëŒ€í•´ SINGLE ëª¨ë“œì™€ ë™ì¼í•œ ë¡œì§ ìˆ˜í–‰
    for i, (target_id, count) in enumerate(top_2_faces, 1):
        person_folder_name = f"{DUAL_PERSON_FOLDER_PREFIX}{i}"
        logger.info(f"{person_folder_name} (face_id={target_id}, ë“±ì¥ í”„ë ˆì„={count}) ì²˜ë¦¬ ì‹œì‘")

        # 2a. í•´ë‹¹ í™”ìì˜ íƒ€ì„ë¼ì¸ ìƒì„± (ë³‘í•©ëœ íƒ€ì„ë¼ì¸ ì‚¬ìš©)
        target_timeline_bool = [tid if tid == target_id else None for tid in merged_timeline]
        
        # 2b. í•´ë‹¹ í™”ì ì˜ìƒë§Œ íŠ¸ë¦¬ë°
        person_trimmed_path = os.path.join(temp_dir, f"trimmed_{person_folder_name}.mp4")
        if not trim_by_face_timeline_ffmpeg(source_video, target_timeline_bool, fps, threshold_frames=30, output_path=person_trimmed_path):
            logger.warning(f"{person_folder_name}: íŠ¸ë¦¬ë°í•  ì˜ìƒì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # 2c. 10ì´ˆ ë‹¨ìœ„ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
        person_segment_temp_folder = os.path.join(temp_dir, f"segments_{person_folder_name}")
        os.makedirs(person_segment_temp_folder, exist_ok=True)
        slice_video_parallel_ffmpeg(person_trimmed_path, person_segment_temp_folder, segment_length=10)

        # 2d. ê° ì„¸ê·¸ë¨¼íŠ¸ í¬ë¡­ ë° ì €ì¥
        final_person_output_dir = os.path.join(OUTPUT_ROOT, basename, person_folder_name)
        os.makedirs(final_person_output_dir, exist_ok=True)

        segment_files = [f for f in os.listdir(person_segment_temp_folder) if f.lower().endswith(".mp4")]
        segment_files.sort()

        if not segment_files:
            logger.warning(f"{person_folder_name}: ìƒì„±ëœ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        logger.info(f"{person_folder_name}: {len(segment_files)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹œì‘")

        segment_tasks = []
        for seg_fname in segment_files:
            # ì¶œë ¥ íŒŒì¼ëª…ì— person_folder_nameì„ í¬í•¨í•˜ì—¬ êµ¬ë¶„
            output_seg_fname = f"{person_folder_name}_{seg_fname}"
            seg_input = os.path.join(person_segment_temp_folder, seg_fname)
            seg_cropped = os.path.join(temp_dir, f"crop_{output_seg_fname}")
            output_seg_path = os.path.join(final_person_output_dir, output_seg_fname)

            segment_tasks.append({
                'seg_fname': output_seg_fname,
                'seg_input': seg_input,
                'seg_cropped': seg_cropped,
                'output_path': output_seg_path
            })
        
        # ìˆœì°¨ ì²˜ë¦¬
        for task in segment_tasks:
            process_single_segment_ffmpeg(task)
        
        logger.success(f"{person_folder_name}: {len(segment_tasks)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        total_segments_processed += len(segment_tasks)

        # ì„ì‹œ í´ë” ì •ë¦¬
        if os.path.exists(person_segment_temp_folder):
            shutil.rmtree(person_segment_temp_folder)
        if os.path.exists(person_trimmed_path):
            os.remove(person_trimmed_path)

    # ìµœì¢… ë¦¬í¬íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸
    num_processes = max(1, min(8, int(cpu_count() * 0.8)))
    reporter.set_processing_info(total_frames, total_segments_processed, num_processes)

    reporter.end_stage("DUAL ëª¨ë“œ ì²˜ë¦¬", segments=total_segments_processed)
    logger.success(f"DUAL ëª¨ë“œ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_segments_processed}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {len(top_2_faces)}ëª… ì¸ì‹")


def process_all_videos_optimized():
    """
    ìµœì í™”ëœ ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
    """
    # CUDA ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ìœ„í•œ spawn ë°©ì‹ ì„¤ì •
    try:
        set_start_method('spawn', force=True)
    except RuntimeError:
        # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° ë¬´ì‹œ
        pass
    
    os.makedirs(OUTPUT_ROOT, exist_ok=True)
    os.makedirs(TEMP_ROOT, exist_ok=True)

    # ë¡œê·¸ ì´ˆê¸°í™”
    logger.clear_log()
    logger.info("ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘")

    for fname in os.listdir(INPUT_DIR):
        if not fname.lower().endswith(SUPPORTED_VIDEO_EXTENSIONS):
            continue
        
        process_single_video_optimized(fname)

    logger.success("ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ì „ì²´ ì„ì‹œ í´ë” ì •ë¦¬
    if os.path.exists(TEMP_ROOT):
        shutil.rmtree(TEMP_ROOT)
        logger.info("ì„ì‹œ í´ë” ì •ë¦¬ ì™„ë£Œ")


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ëª… ìœ ì§€
def process_single_video(fname: str):
    """ìµœì í™”ëœ ë²„ì „ í˜¸ì¶œ"""
    return process_single_video_optimized(fname)


def process_all_videos():
    """ìµœì í™”ëœ ë²„ì „ í˜¸ì¶œ"""
    return process_all_videos_optimized()


def process_dual_split_mode_segments(id_timeline, fps, source_video, temp_dir, basename, reporter, total_frames):
    """
    DUAL_SPLIT ëª¨ë“œ ì „ìš©: í™”ë©´ì„ ì¢Œìš°ë¡œ ë¶„í• í•˜ì—¬ 2ëª…ì„ ë™ì‹œì— ì¶”ì 
    
    Args:
        id_timeline: ì–¼êµ´ ID íƒ€ì„ë¼ì¸
        fps: ë¹„ë””ì˜¤ FPS
        source_video: ì†ŒìŠ¤ ë¹„ë””ì˜¤ ê²½ë¡œ (ìš”ì•½ë³¸)
        temp_dir: ì„ì‹œ ë””ë ‰í† ë¦¬
        basename: ë² ì´ìŠ¤ íŒŒì¼ëª…
        reporter: ì„±ëŠ¥ ë¦¬í¬í„°
        total_frames: ì´ í”„ë ˆì„ ìˆ˜
    """
    from collections import Counter
    import cv2
    from moviepy import VideoFileClip  # MoviePy v2
    import numpy as np
    from src.face_tracker.utils.logging import logger
    from datetime import datetime

    print(f"ğŸ”„ CONSOLE: DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬ ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
    reporter.start_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬")
    logger.stage("DUAL_SPLIT ëª¨ë“œ: í™”ë©´ ë¶„í•  2ì¸ ë™ì‹œ ì¶”ì ...")
    logger.debug("ğŸ” DUAL_SPLIT: process_dual_split_mode_segments() í•¨ìˆ˜ ì‹œì‘")
    print(f"ğŸ”„ CONSOLE: process_dual_split_mode_segments() ì§„ì… ì™„ë£Œ - íŒŒë¼ë¯¸í„°: fps={fps}, total_frames={total_frames}")

    try:
        # 1. í•˜ì´ë¸Œë¦¬ë“œ Person í• ë‹¹ ì „ëµ ìˆ˜í–‰
        print(f"ğŸ”„ CONSOLE: DUAL_SPLIT ì˜ì¡´ì„± ë¡œë“œ ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
        logger.debug("ğŸ” DUAL_SPLIT: import êµ¬ë¬¸ ë¡œë“œ ì¤‘...")
        try:
            from src.face_tracker.core.embeddings import SmartEmbeddingManager
            from src.face_tracker.processing.selector import TargetSelector
            from src.face_tracker.config import L2_NORMALIZATION_ENABLED
            logger.debug("âœ… DUAL_SPLIT: import êµ¬ë¬¸ ë¡œë“œ ì™„ë£Œ")
            print(f"âœ… CONSOLE: DUAL_SPLIT ì˜ì¡´ì„± ë¡œë“œ ì™„ë£Œ ({datetime.now().strftime('%H:%M:%S')})")
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: import êµ¬ë¬¸ ë¡œë“œ ì‹¤íŒ¨ - {e}")
            print(f"âŒ CONSOLE: DUAL_SPLIT ì˜ì¡´ì„± ë¡œë“œ ì‹¤íŒ¨: {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        logger.debug("ğŸ” DUAL_SPLIT: SmartEmbeddingManager ì´ˆê¸°í™” ì¤‘...")
        try:
            emb_manager = SmartEmbeddingManager()
            logger.debug("âœ… DUAL_SPLIT: SmartEmbeddingManager ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: SmartEmbeddingManager ì´ˆê¸°í™” ì‹¤íŒ¨ - {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        logger.debug("ğŸ” DUAL_SPLIT: embeddings ë¡œë“œ ì¤‘...")
        try:
            embeddings = emb_manager.get_all_embeddings()
            logger.debug(f"âœ… DUAL_SPLIT: embeddings ë¡œë“œ ì™„ë£Œ - {len(embeddings) if embeddings else 0}ê°œ")
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: embeddings ë¡œë“œ ì‹¤íŒ¨ - {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        # L2 ì •ê·œí™” ì ìš©ëœ ID ë³‘í•© (ê°„ë‹¨í•œ ì„ê³„ê°’ ì‚¬ìš©)
        if embeddings:
            logger.debug("ğŸ” DUAL_SPLIT: ID ë³‘í•© ì‹œì‘...")
            try:
                from src.face_tracker.config import DUAL_MODE_SIMILARITY_THRESHOLD
                
                logger.debug(f"ğŸ” DUAL_SPLIT: ID ë³‘í•© íŒŒë¼ë¯¸í„° - ì„ê³„ê°’: {DUAL_MODE_SIMILARITY_THRESHOLD}, L2: {L2_NORMALIZATION_ENABLED}")
                merged_timeline = TargetSelector._merge_similar_ids(
                    id_timeline, embeddings, 
                    similarity_threshold=DUAL_MODE_SIMILARITY_THRESHOLD,
                    use_l2_norm=L2_NORMALIZATION_ENABLED
                )
                logger.info(f"DUAL_SPLIT ëª¨ë“œ ID ë³‘í•© ì™„ë£Œ (ì„ê³„ê°’: {DUAL_MODE_SIMILARITY_THRESHOLD:.3f}, L2: {L2_NORMALIZATION_ENABLED})")
                logger.debug("âœ… DUAL_SPLIT: ID ë³‘í•© ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ DUAL_SPLIT: ID ë³‘í•© ì‹¤íŒ¨ - {e}")
                logger.warning("ğŸ”„ DUAL_SPLIT: ID ë³‘í•© ì‹¤íŒ¨ë¡œ ì›ë³¸ íƒ€ì„ë¼ì¸ ì‚¬ìš©")
                merged_timeline = id_timeline
        else:
            logger.debug("ğŸ” DUAL_SPLIT: embeddingsê°€ ì—†ì–´ ID ë³‘í•© ìŠ¤í‚µ")
            merged_timeline = id_timeline
        
        # 2. í•˜ì´ë¸Œë¦¬ë“œ Person í• ë‹¹
        print(f"ğŸ”„ CONSOLE: Person í• ë‹¹ ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
        logger.debug("ğŸ” DUAL_SPLIT: assign_persons_hybrid() í˜¸ì¶œ ì‹œì‘...")
        try:
            person1_id, person2_id = assign_persons_hybrid(merged_timeline, source_video, fps)
            logger.debug(f"âœ… DUAL_SPLIT: assign_persons_hybrid() ì™„ë£Œ - Person1: {person1_id}, Person2: {person2_id}")
            print(f"âœ… CONSOLE: Person í• ë‹¹ ì™„ë£Œ - Person1: {person1_id}, Person2: {person2_id} ({datetime.now().strftime('%H:%M:%S')})")
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: assign_persons_hybrid() ì‹¤íŒ¨ - {e}")
            print(f"âŒ CONSOLE: Person í• ë‹¹ ì‹¤íŒ¨: {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        if person1_id is None or person2_id is None:
            logger.warning("DUAL_SPLIT ëª¨ë“œ: 2ëª…ì˜ ì£¼ìš” ì¸ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        logger.info(f"ê°œì„ ëœ DUAL_SPLIT ëª¨ë“œ: Person1(ì¢Œì¸¡)={person1_id}, Person2(ìš°ì¸¡)={person2_id}")
        
        # 3. ë¶„í•  ì˜ìƒ ìƒì„±
        logger.debug("ğŸ” DUAL_SPLIT: ì¶œë ¥ í´ë” ìƒì„± ì¤‘...")
        try:
            from src.face_tracker.config import OUTPUT_ROOT
            output_folder = os.path.join(OUTPUT_ROOT, basename)
            os.makedirs(output_folder, exist_ok=True)
            
            split_video_path = os.path.join(output_folder, f"{basename}_dual_split.mp4")
            logger.debug(f"âœ… DUAL_SPLIT: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± ì™„ë£Œ: {split_video_path}")
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: ì¶œë ¥ í´ë” ìƒì„± ì‹¤íŒ¨ - {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        # 4. í™”ë©´ ë¶„í•  ë° ì¤‘ì•™ ì •ë ¬ ì²˜ë¦¬
        print(f"ğŸ”„ CONSOLE: í™”ë©´ ë¶„í•  ì˜ìƒ ìƒì„± ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
        logger.debug("ğŸ” DUAL_SPLIT: create_split_screen_video() í˜¸ì¶œ ì‹œì‘...")
        try:
            split_result = create_split_screen_video(
                source_video, merged_timeline, fps, 
                person1_id, person2_id, split_video_path
            )
            logger.debug("âœ… DUAL_SPLIT: create_split_screen_video() í˜¸ì¶œ ì™„ë£Œ")
            print(f"âœ… CONSOLE: í™”ë©´ ë¶„í•  ì˜ìƒ ìƒì„± ì™„ë£Œ ({datetime.now().strftime('%H:%M:%S')})")
            
            # ì²˜ë¦¬ ê²°ê³¼ ì •ë³´ ë¡œê·¸
            if split_result:
                logger.info(f"ğŸ¬ DUAL_SPLIT ì²˜ë¦¬ ê²°ê³¼:")
                logger.info(f"  â€¢ ì²˜ë¦¬ëœ í”„ë ˆì„: {split_result['processed_frames']:,}ê°œ")
                logger.info(f"  â€¢ ì´ í”„ë ˆì„: {split_result['total_frames']:,}ê°œ")
                logger.info(f"  â€¢ Person1 ê²€ì¶œë¥ : {split_result['person1_ratio']:.1f}%")
                logger.info(f"  â€¢ Person2 ê²€ì¶œë¥ : {split_result['person2_ratio']:.1f}%")
                logger.info(f"  â€¢ ì „ì²´ í’ˆì§ˆ: {split_result['quality_status']}")
                
                # ì„±ëŠ¥ ë¦¬í¬í„°ì— ì •í™•í•œ ì •ë³´ ì „ë‹¬
                reporter.set_dual_split_stats(
                    processed_frames=split_result['processed_frames'],
                    total_frames=split_result['total_frames'],
                    segments=1  # DUAL_SPLIT ëª¨ë“œëŠ” ë‹¨ì¼ ì˜ìƒ ìƒì„±
                )
            else:
                logger.warning("ğŸ” DUAL_SPLIT: create_split_screen_video()ì—ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")
                
        except Exception as e:
            logger.error(f"âŒ DUAL_SPLIT: create_split_screen_video() ì‹¤íŒ¨ - {e}")
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
            return
        
        logger.success(f"DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬ ì™„ë£Œ: {split_video_path}")
        
        # ìµœì¢… ê²°ê³¼ë¥¼ ë¦¬í¬í„°ì— ì „ë‹¬
        if split_result:
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", 
                            segments=1, 
                            frames=split_result['processed_frames'])
        else:
            reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=1)
        
    except Exception as e:
        logger.error(f"âŒ DUAL_SPLIT: process_dual_split_mode_segments() ì „ì²´ ì‹¤í–‰ ì‹¤íŒ¨ - {e}")
        reporter.end_stage("DUAL_SPLIT ëª¨ë“œ ì²˜ë¦¬", segments=0)
        return


def assign_persons_hybrid(id_timeline, source_video, fps):
    """
    ê°œì„ ëœ Person í• ë‹¹ ì „ëµ
    ê°€ì¥ ë§ì´ ë‚˜ì˜¨ 2ëª…ì„ ì¼ê´€ë˜ê²Œ Person1/Person2ë¡œ í• ë‹¹
    Person1 = ë” ë§ì´ ë‚˜ì˜¨ ì‚¬ëŒ (ì¢Œì¸¡ ì¶œë ¥ ì˜ì—­)
    Person2 = ë‘ ë²ˆì§¸ë¡œ ë§ì´ ë‚˜ì˜¨ ì‚¬ëŒ (ìš°ì¸¡ ì¶œë ¥ ì˜ì—­)
    
    Returns:
        tuple: (person1_id, person2_id)
    """
    from collections import Counter
    from src.face_tracker.utils.logging import logger
    from datetime import datetime
    
    print(f"ğŸ”„ CONSOLE: assign_persons_hybrid() ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
    logger.info("ê°œì„ ëœ Person í• ë‹¹ ì‹œì‘...")
    logger.debug("ğŸ” ASSIGN: assign_persons_hybrid() í•¨ìˆ˜ ì‹œì‘")
    
    # 1. ë¹ˆë„ ë¶„ì„ìœ¼ë¡œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ 2ëª… ì°¾ê¸°
    print(f"ğŸ”„ CONSOLE: ID íƒ€ì„ë¼ì¸ ë¶„ì„ ì‹œì‘ - ì „ì²´ ê¸¸ì´: {len(id_timeline)} ({datetime.now().strftime('%H:%M:%S')})")
    logger.debug("ğŸ” ASSIGN: ID íƒ€ì„ë¼ì¸ ë¶„ì„ ì‹œì‘...")
    valid_ids = [fid for fid in id_timeline if fid is not None]
    logger.debug(f"ğŸ” ASSIGN: ìœ íš¨í•œ ID ê°œìˆ˜: {len(valid_ids)}")
    print(f"ğŸ”„ CONSOLE: ìœ íš¨í•œ ID ê°œìˆ˜: {len(valid_ids)}ê°œ")
    
    if not valid_ids:
        logger.warning("ìœ íš¨í•œ ì–¼êµ´ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
        logger.debug("ğŸ” ASSIGN: ìœ íš¨í•œ IDê°€ ì—†ì–´ì„œ ì¢…ë£Œ")
        return None, None
    
    logger.debug("ğŸ” ASSIGN: Counterë¥¼ ì´ìš©í•œ ë¹ˆë„ ë¶„ì„ ì¤‘...")
    face_counts = Counter(valid_ids)
    logger.debug(f"ğŸ” ASSIGN: ì „ì²´ ì–¼êµ´ IDë³„ ì¹´ìš´íŠ¸: {dict(face_counts.most_common())}")
    
    top_2_faces = face_counts.most_common(2)
    logger.debug(f"ğŸ” ASSIGN: ìƒìœ„ 2ëª… ê²°ê³¼: {top_2_faces}")
    
    if len(top_2_faces) < 2:
        logger.warning("2ëª… ë¯¸ë§Œì˜ ì–¼êµ´ë§Œ ê°ì§€ë¨")
        logger.debug("ğŸ” ASSIGN: 2ëª… ë¯¸ë§Œìœ¼ë¡œ ì¸í•œ ì¡°ê¸° ì¢…ë£Œ")
        return top_2_faces[0][0] if top_2_faces else None, None
    
    # 2. ì¼ê´€ëœ Person í• ë‹¹
    logger.debug("ğŸ” ASSIGN: Person1/Person2 í• ë‹¹ ì¤‘...")
    # Person1 = ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ì‚¬ëŒ (ì¢Œì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹)
    # Person2 = ë‘ ë²ˆì§¸ë¡œ ë§ì´ ë‚˜ì˜¨ ì‚¬ëŒ (ìš°ì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹)
    person1_id = top_2_faces[0][0]  # ìµœë‹¤ ì¶œí˜„ì
    person2_id = top_2_faces[1][0]  # ì°¨ìˆœ ì¶œí˜„ì
    logger.debug(f"ğŸ” ASSIGN: í• ë‹¹ ê²°ê³¼ - Person1: {person1_id}, Person2: {person2_id}")
    
    person1_count = top_2_faces[0][1]
    person2_count = top_2_faces[1][1]
    
    logger.info(f"Person í• ë‹¹ ì™„ë£Œ:")
    logger.info(f"  Person1 (ì¢Œì¸¡): ID={person1_id} ({person1_count}íšŒ ì¶œí˜„)")
    logger.info(f"  Person2 (ìš°ì¸¡): ID={person2_id} ({person2_count}íšŒ ì¶œí˜„)")
    print(f"âœ… CONSOLE: Person í• ë‹¹ ê²°ê³¼ - Person1: {person1_id}({person1_count}íšŒ), Person2: {person2_id}({person2_count}íšŒ)")
    
    # 3. í• ë‹¹ ë¹„ìœ¨ ê²€ì¦
    total_appearances = person1_count + person2_count
    person1_ratio = person1_count / total_appearances
    person2_ratio = person2_count / total_appearances
    
    logger.info(f"ì¶œí˜„ ë¹„ìœ¨: Person1={person1_ratio:.1%}, Person2={person2_ratio:.1%}")
    print(f"ğŸ”„ CONSOLE: ì¶œí˜„ ë¹„ìœ¨ - Person1: {person1_ratio:.1%}, Person2: {person2_ratio:.1%} ({datetime.now().strftime('%H:%M:%S')})")
    
    # ê·¹ë‹¨ì ì¸ ë¶ˆê· í˜• ê²½ê³  (í•œ ì‚¬ëŒì´ 90% ì´ìƒ)
    if person1_ratio > 0.9:
        logger.warning(f"Person1ì´ {person1_ratio:.1%} ë¹„ìœ¨ë¡œ ì••ë„ì  ì¶œí˜„ - ë‹¨ì¼ ì¸ë¬¼ ì˜ìƒì¼ ê°€ëŠ¥ì„±")
    
    print(f"âœ… CONSOLE: assign_persons_hybrid() ì™„ë£Œ ({datetime.now().strftime('%H:%M:%S')})")
    return person1_id, person2_id

class DualPersonTracker:
    """
    dual_split ì „ìš© Person íŠ¸ë˜ì»¤
    ë²¡í„° ê¸°ë°˜ Person1/Person2 ì‹ë³„ + ìœ„ì¹˜ ê¸°ë°˜ ì—°ì†ì„± ë³´ì¥
    """
    
    def __init__(self, person1_id, person2_id, embeddings_manager):
        """
        Args:
            person1_id: ì¢Œì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹ Person ID
            person2_id: ìš°ì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹ Person ID
            embeddings_manager: SmartEmbeddingManager ì¸ìŠ¤í„´ìŠ¤
        """
        from src.face_tracker.utils.logging import logger
        import numpy as np
        
        self.person1_id = person1_id  # ì¢Œì¸¡ ì˜ì—­ ë‹´ë‹¹
        self.person2_id = person2_id  # ìš°ì¸¡ ì˜ì—­ ë‹´ë‹¹
        
        # ê¸°ì¤€ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ ë° ë©”ëª¨ë¦¬ ë³´ê´€
        embeddings = embeddings_manager.get_all_embeddings()
        self.person1_embedding = embeddings.get(person1_id)
        self.person2_embedding = embeddings.get(person2_id)
        
        # L2 ì •ê·œí™” ì ìš©
        if self.person1_embedding is not None:
            self.person1_embedding = self._l2_normalize(self.person1_embedding)
        if self.person2_embedding is not None:
            self.person2_embedding = self._l2_normalize(self.person2_embedding)
        
        # ìœ„ì¹˜ íŠ¸ë˜í‚¹ ìƒíƒœ
        self.person1_last_pos = None  # (x, y, w, h)
        self.person2_last_pos = None
        self.person1_lost_frames = 0  # íŠ¸ë˜í‚¹ ì‹¤íŒ¨ ì¹´ìš´í„°
        self.person2_lost_frames = 0
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìºì‹œ ê´€ë¦¬
        self._embedding_cache = {}  # ì„ë² ë”© ê³„ì‚° ê²°ê³¼ ìºì‹œ
        self._cache_size_limit = 100  # ìºì‹œ ìµœëŒ€ í¬ê¸°
        
        logger.info(f"DualPersonTracker ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"  Person1 (ì¢Œì¸¡): ID={person1_id}, ì„ë² ë”©={'âœ…' if self.person1_embedding is not None else 'âŒ'}")
        logger.info(f"  Person2 (ìš°ì¸¡): ID={person2_id}, ì„ë² ë”©={'âœ…' if self.person2_embedding is not None else 'âŒ'}")
    
    def _l2_normalize(self, embedding):
        """L2 ì •ê·œí™” ì ìš©"""
        import numpy as np
        try:
            # ì„ë² ë”©ì„ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
            emb_flat = np.array(embedding).flatten()
            
            # L2 norm ê³„ì‚°
            norm = np.linalg.norm(emb_flat)
            
            # ì •ê·œí™” (normì´ 0ì¸ ê²½ìš° ì›ë³¸ ë°˜í™˜)
            if norm > 1e-8:  # ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ 0 ì²´í¬
                return emb_flat / norm
            else:
                print(f"âš ï¸ CONSOLE: L2 normì´ 0ì— ê°€ê¹Œì›€ - norm: {norm}")
                return emb_flat
        except Exception as e:
            print(f"âŒ CONSOLE: L2 ì •ê·œí™” ì—ëŸ¬ - {e}")
            return np.array(embedding).flatten()  # ì—ëŸ¬ ì‹œ í‰íƒ„í™”ë§Œ ìˆ˜í–‰
    
    def _calculate_cosine_similarity(self, emb1, emb2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        try:
            # ì„ë² ë”©ì„ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜ (ì•ˆì „ì„± í™•ë³´)
            emb1_flat = np.array(emb1).flatten()
            emb2_flat = np.array(emb2).flatten()
            
            # ë‚´ì  ê³„ì‚° í›„ ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ë³€í™˜
            dot_product = np.dot(emb1_flat, emb2_flat)
            
            # ë°°ì—´ì¸ ê²½ìš° ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
            if isinstance(dot_product, np.ndarray):
                dot_product = float(dot_product.item())
            
            return dot_product
        except Exception as e:
            print(f"âŒ CONSOLE: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì—ëŸ¬ - {e}")
            return 0.0  # ì—ëŸ¬ ì‹œ 0 ë°˜í™˜  # L2 ì •ê·œí™”ëœ ë²¡í„°ëŠ” ë‚´ì ì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    
    def _calculate_position_distance(self, pos1, pos2):
        """ìœ„ì¹˜ ê°„ ê±°ë¦¬ ê³„ì‚°"""
        import numpy as np
        
        if pos1 is None or pos2 is None:
            return float('inf')
        
        try:
            # íŠœí”Œ/ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ë³€í™˜
            pos1 = np.array(pos1).flatten()
            pos2 = np.array(pos2).flatten()
            
            if len(pos1) < 4 or len(pos2) < 4:
                return float('inf')
            
            # ì¤‘ì‹¬ì  ê¸°ì¤€ ê±°ë¦¬ ê³„ì‚° (ìŠ¤ì¹¼ë¼ ê°’ ë³´ì¥)
            center1_x = float(pos1[0] + pos1[2] // 2)
            center1_y = float(pos1[1] + pos1[3] // 2)
            center2_x = float(pos2[0] + pos2[2] // 2)
            center2_y = float(pos2[1] + pos2[3] // 2)
            
            distance = np.sqrt((center1_x - center2_x)**2 + (center1_y - center2_y)**2)
            
            # ìŠ¤ì¹¼ë¼ ê°’ìœ¼ë¡œ ë³€í™˜
            if isinstance(distance, np.ndarray):
                distance = float(distance.item())
            
            return distance
        except Exception as e:
            print(f"âŒ CONSOLE: ìœ„ì¹˜ ê±°ë¦¬ ê³„ì‚° ì—ëŸ¬ - {e}")
            return float('inf')
    
    def match_faces_to_persons(self, faces, face_embeddings):
        """
        ì—¬ëŸ¬ ì–¼êµ´ ì¤‘ì—ì„œ Person1/Person2ì— í•´ë‹¹í•˜ëŠ” ì–¼êµ´ ì°¾ê¸° (ì™„ì „ ì•ˆì „ ë²„ì „)
        
        Args:
            faces: MTCNN ê²€ì¶œëœ ì–¼êµ´ bbox ë¦¬ìŠ¤íŠ¸ [(x1,y1,x2,y2), ...]
            face_embeddings: ê° ì–¼êµ´ì˜ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            tuple: (person1_face_box, person2_face_box)
        """
        from src.face_tracker.dual_split_config import (
            TRACKING_POSITION_THRESHOLD, 
            TRACKING_SIMILARITY_THRESHOLD,
            TRACKING_MAX_LOST_FRAMES
        )
        
        # ì•ˆì „í•œ ì…ë ¥ ê²€ì¦
        try:
            if faces is None:
                self.person1_lost_frames += 1
                self.person2_lost_frames += 1
                return None, None
            
            # facesë¥¼ ì•ˆì „í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            import numpy as np
            if isinstance(faces, np.ndarray):
                if faces.size == 0:
                    self.person1_lost_frames += 1
                    self.person2_lost_frames += 1
                    return None, None
                faces_list = faces.tolist()
            elif isinstance(faces, list):
                if len(faces) == 0:
                    self.person1_lost_frames += 1
                    self.person2_lost_frames += 1
                    return None, None
                faces_list = faces
            else:
                self.person1_lost_frames += 1
                self.person2_lost_frames += 1
                return None, None
            
            face_count = len(faces_list)
            if face_count == 0:
                self.person1_lost_frames += 1
                self.person2_lost_frames += 1
                return None, None
                
        except Exception as e:
            print(f"âŒ CONSOLE: ì…ë ¥ ê²€ì¦ ì—ëŸ¬ - {e}")
            self.person1_lost_frames += 1
            self.person2_lost_frames += 1
            return None, None
        
        # 1ë‹¨ê³„: ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­ (ì—°ì†ì„± ìš°ì„ )
        person1_candidates = []
        person2_candidates = []
        
        for i in range(face_count):
            try:
                face = faces_list[i]
                # ì•ˆì „í•œ ì¢Œí‘œ ì¶”ì¶œ
                x1, y1, x2, y2 = float(face[0]), float(face[1]), float(face[2]), float(face[3])
                face_pos = (x1, y1, x2 - x1, y2 - y1)  # (x, y, w, h)
                
                # Person1 ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­
                if self.person1_last_pos is not None:
                    try:
                        distance = self._calculate_position_distance(face_pos, self.person1_last_pos)
                        distance_val = float(distance)
                        threshold_val = float(TRACKING_POSITION_THRESHOLD)
                        
                        if distance_val < threshold_val:
                            person1_candidates.append((i, face, distance_val))
                    except Exception as e:
                        print(f"âŒ CONSOLE: Person1 ìœ„ì¹˜ ë§¤ì¹­ ì—ëŸ¬ - {e}")
                        continue
                
                # Person2 ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­  
                if self.person2_last_pos is not None:
                    try:
                        distance = self._calculate_position_distance(face_pos, self.person2_last_pos)
                        distance_val = float(distance)
                        threshold_val = float(TRACKING_POSITION_THRESHOLD)
                        
                        if distance_val < threshold_val:
                            person2_candidates.append((i, face, distance_val))
                    except Exception as e:
                        print(f"âŒ CONSOLE: Person2 ìœ„ì¹˜ ë§¤ì¹­ ì—ëŸ¬ - {e}")
                        continue
                        
            except Exception as e:
                print(f"âŒ CONSOLE: ì–¼êµ´ {i} ìœ„ì¹˜ ì²˜ë¦¬ ì—ëŸ¬ - {e}")
                continue
        
        # 2ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ë„ ê²€ì¦
        person1_face = None
        person2_face = None
        
        # Person1 ë§¤ì¹­
        if len(person1_candidates) > 0 and self.person1_embedding is not None:
            best_candidate = None
            best_similarity_val = -1.0
            
            for idx, face, pos_distance in person1_candidates:
                try:
                    if idx < len(face_embeddings) and face_embeddings[idx] is not None:
                        similarity = self._calculate_cosine_similarity(
                            self.person1_embedding, 
                            self._l2_normalize(face_embeddings[idx])
                        )
                        
                        # ì•ˆì „í•œ ìŠ¤ì¹¼ë¼ ë³€í™˜
                        similarity_val = float(similarity)
                        threshold_val = float(TRACKING_SIMILARITY_THRESHOLD)
                        
                        # ì•ˆì „í•œ ì¡°ê±´ ë¹„êµ
                        if similarity_val > best_similarity_val and similarity_val > threshold_val:
                            best_similarity_val = similarity_val
                            best_candidate = (idx, face)
                            
                except Exception as e:
                    print(f"âŒ CONSOLE: Person1 ìœ ì‚¬ë„ ê³„ì‚° ì—ëŸ¬ - {e}")
                    continue
            
            if best_candidate:
                person1_face = best_candidate[1]
                # ì•ˆì „í•œ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                try:
                    x1, y1, x2, y2 = float(person1_face[0]), float(person1_face[1]), float(person1_face[2]), float(person1_face[3])
                    self.person1_last_pos = (x1, y1, x2 - x1, y2 - y1)
                    self.person1_lost_frames = 0
                except Exception as e:
                    print(f"âŒ CONSOLE: Person1 ìœ„ì¹˜ ì—…ë°ì´íŠ¸ ì—ëŸ¬ - {e}")
        
        # Person2 ë§¤ì¹­
        if len(person2_candidates) > 0 and self.person2_embedding is not None:
            best_candidate = None
            best_similarity_val = -1.0
            
            for idx, face, pos_distance in person2_candidates:
                try:
                    if idx < len(face_embeddings) and face_embeddings[idx] is not None:
                        similarity = self._calculate_cosine_similarity(
                            self.person2_embedding, 
                            self._l2_normalize(face_embeddings[idx])
                        )
                        
                        # ì•ˆì „í•œ ìŠ¤ì¹¼ë¼ ë³€í™˜
                        similarity_val = float(similarity)
                        threshold_val = float(TRACKING_SIMILARITY_THRESHOLD)
                        
                        # ì•ˆì „í•œ ì¡°ê±´ ë¹„êµ
                        if similarity_val > best_similarity_val and similarity_val > threshold_val:
                            best_similarity_val = similarity_val
                            best_candidate = (idx, face)
                            
                except Exception as e:
                    print(f"âŒ CONSOLE: Person2 ìœ ì‚¬ë„ ê³„ì‚° ì—ëŸ¬ - {e}")
                    continue
            
            if best_candidate:
                person2_face = best_candidate[1]
                # ì•ˆì „í•œ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
                try:
                    x1, y1, x2, y2 = float(person2_face[0]), float(person2_face[1]), float(person2_face[2]), float(person2_face[3])
                    self.person2_last_pos = (x1, y1, x2 - x1, y2 - y1)
                    self.person2_lost_frames = 0
                except Exception as e:
                    print(f"âŒ CONSOLE: Person2 ìœ„ì¹˜ ì—…ë°ì´íŠ¸ ì—ëŸ¬ - {e}")
        
        # 3ë‹¨ê³„: Fallback - ì´ˆê¸° í”„ë ˆì„ì´ê±°ë‚˜ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
        if person1_face is None and self.person1_last_pos is None and face_count > 0:
            try:
                person1_face = faces_list[0]
                x1, y1, x2, y2 = float(person1_face[0]), float(person1_face[1]), float(person1_face[2]), float(person1_face[3])
                self.person1_last_pos = (x1, y1, x2 - x1, y2 - y1)
            except Exception as e:
                print(f"âŒ CONSOLE: Person1 Fallback ì—ëŸ¬ - {e}")
        
        if person2_face is None and self.person2_last_pos is None and face_count > 1:
            try:
                person2_face = faces_list[1]
                x1, y1, x2, y2 = float(person2_face[0]), float(person2_face[1]), float(person2_face[2]), float(person2_face[3])
                self.person2_last_pos = (x1, y1, x2 - x1, y2 - y1)
            except Exception as e:
                print(f"âŒ CONSOLE: Person2 Fallback ì—ëŸ¬ - {e}")
        
        return person1_face, person2_face

    def _get_cached_embedding(self, face_hash):
        """ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ"""
        return self._embedding_cache.get(face_hash)
    
    def _cache_embedding(self, face_hash, embedding):
        """ì„ë² ë”© ìºì‹œ ì €ì¥"""
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self._embedding_cache) >= self._cache_size_limit:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (ë‹¨ìˆœ êµ¬í˜„)
            oldest_key = next(iter(self._embedding_cache))
            del self._embedding_cache[oldest_key]
        
        self._embedding_cache[face_hash] = embedding
    
    def get_tracking_stats(self):
        """íŠ¸ë˜í‚¹ í†µê³„ ë°˜í™˜"""
        return {
            'person1_lost_frames': self.person1_lost_frames,
            'person2_lost_frames': self.person2_lost_frames,
            'cache_size': len(self._embedding_cache),
            'person1_has_embedding': self.person1_embedding is not None,
            'person2_has_embedding': self.person2_embedding is not None
        }


# ì¶”ê°€ import êµ¬ë¬¸ë“¤
from PIL import Image

def check_gpu_compatibility():
    """GPU/CPU í˜¸í™˜ì„± ë° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸"""
    import torch
    from src.face_tracker.config import DEVICE
    
    print(f"ğŸ”§ CONSOLE: GPU í˜¸í™˜ì„± ê²€ì‚¬ ì‹œì‘...")
    
    # CUDA ê°€ìš©ì„± í™•ì¸
    cuda_available = torch.cuda.is_available()
    print(f"ğŸ”§ CONSOLE: CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}")
    
    # ì„¤ì •ëœ ë””ë°”ì´ìŠ¤ í™•ì¸
    device_is_cuda = 'cuda' in str(DEVICE)
    print(f"ğŸ”§ CONSOLE: ì„¤ì •ëœ ë””ë°”ì´ìŠ¤: {DEVICE} ({'CUDA' if device_is_cuda else 'CPU'})")
    
    if device_is_cuda and not cuda_available:
        print(f"âš ï¸ CONSOLE: CUDAê°€ ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ëŒ€ì²´ ê¶Œì¥")
        return False, "CUDA ë¶ˆê°€"
    
    # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (CUDA ì‚¬ìš©ì‹œ)
    if cuda_available and device_is_cuda:
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            total_memory = torch.cuda.get_device_properties(0).total_memory
            allocated_memory = torch.cuda.memory_allocated(0)
            free_memory = total_memory - allocated_memory
            
            print(f"ğŸ”§ CONSOLE: GPU ë©”ëª¨ë¦¬ - ì´: {total_memory/1024**3:.1f}GB, ì‚¬ìš©: {allocated_memory/1024**3:.1f}GB, ì—¬ìœ : {free_memory/1024**3:.1f}GB")
            
            # ìµœì†Œ 1GB ì—¬ìœ  ë©”ëª¨ë¦¬ í•„ìš”
            if free_memory < 1024**3:
                print(f"âš ï¸ CONSOLE: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ({free_memory/1024**3:.1f}GB < 1.0GB)")
                return False, "GPU ë©”ëª¨ë¦¬ ë¶€ì¡±"
            
            return True, "GPU í˜¸í™˜ ê°€ëŠ¥"
            
        except Exception as e:
            print(f"âŒ CONSOLE: GPU ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨ - {e}")
            return False, f"GPU í™•ì¸ ì‹¤íŒ¨: {e}"
    
    # CPU ëª¨ë“œ
    print(f"ğŸ”§ CONSOLE: CPU ëª¨ë“œë¡œ ë™ì‘")
    return True, "CPU ëª¨ë“œ"

def create_split_screen_video(source_video, id_timeline, fps, person1_id, person2_id, output_path):
    """
    ê°œì„ ëœ ë¶„í•  í™”ë©´ ì˜ìƒ ìƒì„±
    - íŠ¸ë˜ì»¤ ê¸°ë°˜ ì •í™•í•œ Person1/Person2 ë§¤ì¹­
    - í”„ë ˆì„ ìŠ¤í‚µ ì—†ì´ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ (ì—°ì†ì„± ë³´ì¥)
    - ë²¡í„° ìœ ì‚¬ë„ + ìœ„ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­
    
    Args:
        source_video: ì†ŒìŠ¤ ë¹„ë””ì˜¤ ê²½ë¡œ (ìš”ì•½ë³¸)
        id_timeline: ì–¼êµ´ ID íƒ€ì„ë¼ì¸
        fps: ë¹„ë””ì˜¤ FPS
        person1_id: ì¢Œì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹ Person ID
        person2_id: ìš°ì¸¡ ì¶œë ¥ ì˜ì—­ ë‹´ë‹¹ Person ID
        output_path: ì¶œë ¥ ê²½ë¡œ
    """
    # í•„ìˆ˜ importë“¤ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
    try:
        from datetime import datetime
        import cv2
        import numpy as np
        from src.face_tracker.core.models import ModelManager
        from src.face_tracker.core.embeddings import SmartEmbeddingManager
        from src.face_tracker.config import DEVICE
        from src.face_tracker.utils.logging import logger
        from src.face_tracker.dual_split_config import SKIP_NO_FACE_FRAMES
        import torch
        
        # ì´ì œ loggerë¥¼ ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
        print(f"ğŸš€ CONSOLE: create_split_screen_video() ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')})")
        logger.debug("ğŸš€ CREATE_SPLIT: create_split_screen_video() í•¨ìˆ˜ ì§„ì…")
        logger.debug(f"ğŸš€ CREATE_SPLIT: ì…ë ¥ íŒŒë¼ë¯¸í„° - source_video: {source_video}")
        logger.debug(f"ğŸš€ CREATE_SPLIT: ì…ë ¥ íŒŒë¼ë¯¸í„° - person1_id: {person1_id}, person2_id: {person2_id}")
        logger.debug(f"ğŸš€ CREATE_SPLIT: ì…ë ¥ íŒŒë¼ë¯¸í„° - output_path: {output_path}")
        logger.debug(f"ğŸš€ CREATE_SPLIT: ì…ë ¥ íŒŒë¼ë¯¸í„° - fps: {fps}, timeline ê¸¸ì´: {len(id_timeline) if id_timeline else 'None'}")
        print(f"ğŸ”„ CONSOLE: ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì • - Person1: {person1_id}, Person2: {person2_id}, FPS: {fps}")
        logger.debug("ğŸ”§ CREATE_SPLIT: ëª¨ë“  import ì™„ë£Œ")
        
    except ImportError as e:
        print(f"âŒ CONSOLE: import ì˜¤ë¥˜ - {e}")
        print("âŒ CONSOLE: ê¸°ë³¸ importë§Œ ì‚¬ìš©í•˜ì—¬ ì§„í–‰")
        # ê¸°ë³¸ importë§Œ ì‚¬ìš©
        from datetime import datetime
        import cv2
        import numpy as np
        print(f"ğŸš€ CONSOLE: create_split_screen_video() ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')}) - ì œí•œëœ ëª¨ë“œ")
        # logger ì—†ì´ ì§„í–‰
        logger = None
    
    except Exception as e:
        print(f"âŒ CONSOLE: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {e}")
        from datetime import datetime
        logger = None
        print(f"ğŸš€ CONSOLE: create_split_screen_video() ì‹œì‘ ({datetime.now().strftime('%H:%M:%S')}) - ë¹„ìƒ ëª¨ë“œ")
        
        # logger ì—†ì´ëŠ” ê¸°ë³¸ ì²˜ë¦¬ ê²°ê³¼ë§Œ ë°˜í™˜
        return {
            'processed_frames': 0,
            'total_frames': 0,
            'person1_detected_count': 0,
            'person2_detected_count': 0,
            'person1_ratio': 0.0,
            'person2_ratio': 0.0,
            'overall_detection_rate': 0.0,
            'person_balance': 0.0,
            'quality_status': 'ğŸ”´ ì‹œìŠ¤í…œ ì˜¤ë¥˜',
            'tracking_stats': {},
            'output_path': output_path
        }
    
    # logger ì‚¬ìš© ì‹œ ì•ˆì „ ì²˜ë¦¬
    if logger:
        logger.info(f"ê°œì„ ëœ ë¶„í•  í™”ë©´ ì˜ìƒ ìƒì„± ì‹œì‘: {output_path}")
        logger.info(f"Person1(ì¢Œì¸¡)={person1_id}, Person2(ìš°ì¸¡)={person2_id}")
    else:
        print(f"ğŸ¬ CONSOLE: ë¶„í•  í™”ë©´ ìƒì„± ì‹œì‘: {output_path}")
        print(f"ğŸ¬ CONSOLE: Person1(ì¢Œì¸¡)={person1_id}, Person2(ìš°ì¸¡)={person2_id}")
    
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: ëª¨ë¸ ë° íŠ¸ë˜ì»¤ ì´ˆê¸°í™” ì‹œì‘...")
    
    # GPU/CPU í˜¸í™˜ì„± ê²€ì‚¬
    gpu_compatible, gpu_status = check_gpu_compatibility()
    if not gpu_compatible:
        print(f"âš ï¸ CONSOLE: GPU í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€ - {gpu_status}")
        if logger:
            logger.warning(f"GPU í˜¸í™˜ì„± ë¬¸ì œ: {gpu_status}")
    else:
        print(f"âœ… CONSOLE: GPU í˜¸í™˜ì„± í™•ì¸ - {gpu_status}")
        if logger:
            logger.info(f"GPU í˜¸í™˜ì„±: {gpu_status}")
    
    # ëª¨ë¸ ë° íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: ModelManager ì´ˆê¸°í™”...")
    model_manager = ModelManager(DEVICE)
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: ModelManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: MTCNN ëª¨ë¸ ë¡œë“œ...")
    mtcnn = model_manager.get_mtcnn()
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: MTCNN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: FaceNet ëª¨ë¸ ë¡œë“œ...")
    facenet = model_manager.get_resnet()
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: FaceNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: SmartEmbeddingManager ì´ˆê¸°í™”...")
    emb_manager = SmartEmbeddingManager()
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: SmartEmbeddingManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    # DualPersonTracker ì´ˆê¸°í™”
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: DualPersonTracker ì´ˆê¸°í™”...")
    tracker = DualPersonTracker(person1_id, person2_id, emb_manager)
    logger.debug("ğŸ—ï¸ CREATE_SPLIT: DualPersonTracker ì´ˆê¸°í™” ì™„ë£Œ")
    
    logger.debug("ğŸ¬ CREATE_SPLIT: ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì‹œì‘...")
    # ë¹„ë””ì˜¤ ì„¤ì •
    cap = cv2.VideoCapture(source_video)
    if not cap.isOpened():
        logger.error(f"ğŸ¬ CREATE_SPLIT: ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨ - {source_video}")
        return
    logger.debug("ğŸ¬ CREATE_SPLIT: ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸° ì™„ë£Œ")
    
    logger.debug("ğŸ¬ CREATE_SPLIT: ë¹„ë””ì˜¤ ì†ì„± ì½ê¸°...")
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    logger.debug(f"ğŸ¬ CREATE_SPLIT: ë¹„ë””ì˜¤ ì†ì„± - í¬ê¸°: {frame_width}x{frame_height}, ì´ í”„ë ˆì„: {total_frames}")
    
    logger.debug("ğŸ¬ CREATE_SPLIT: VideoWriter ì´ˆê¸°í™”...")
    # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì • (1920x1080)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (1920, 1080))
    if not out.isOpened():
        logger.error(f"ğŸ¬ CREATE_SPLIT: VideoWriter ì´ˆê¸°í™” ì‹¤íŒ¨ - {output_path}")
        cap.release()
        return
    logger.debug("ğŸ¬ CREATE_SPLIT: VideoWriter ì´ˆê¸°í™” ì™„ë£Œ")
    
    frame_idx = 0
    processed_frames = 0
    person1_detected_count = 0
    person2_detected_count = 0
    
    logger.info(f"ì´ {total_frames}í”„ë ˆì„ ì²˜ë¦¬ ì‹œì‘ (ìŠ¤í‚µ ì—†ìŒ: {not SKIP_NO_FACE_FRAMES})")
    logger.debug("ğŸ”„ CREATE_SPLIT: í”„ë ˆì„ë³„ ì²˜ë¦¬ ë£¨í”„ ì‹œì‘...")
    
    while True:
        logger.debug(f"ğŸ”„ CREATE_SPLIT: Frame {frame_idx} ì½ê¸°...")
        ret, frame = cap.read()
        if not ret or frame_idx >= len(id_timeline):
            logger.debug(f"ğŸ”„ CREATE_SPLIT: í”„ë ˆì„ ì½ê¸° ì¢…ë£Œ - ret: {ret}, frame_idx: {frame_idx}, timeline ê¸¸ì´: {len(id_timeline)}")
            break
        
        current_id = id_timeline[frame_idx] if frame_idx < len(id_timeline) else None
        logger.debug(f"ğŸ”„ CREATE_SPLIT: Frame {frame_idx} - current_id: {current_id}")
        
        # ğŸš€ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ (ìŠ¤í‚µ ì—†ìŒ)
        split_frame = np.zeros((1080, 1920, 3), dtype=np.uint8)
        
        try:
            logger.debug(f"ğŸ” CREATE_SPLIT: Frame {frame_idx} MTCNN ì–¼êµ´ ê²€ì¶œ...")
            # ì–¼êµ´ ê²€ì¶œ
            faces, _ = mtcnn.detect(frame)
            face_embeddings = []
            
            # ì–¼êµ´ì´ ê²€ì¶œëœ ê²½ìš° ì„ë² ë”© ì¶”ì¶œ (ì•ˆì „í•œ ë°°ì—´ ê²€ì‚¬)
            has_faces = False
            if faces is not None:
                # NumPy ë°°ì—´ì¸ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                import numpy as np
                if isinstance(faces, np.ndarray):
                    has_faces = faces.size > 0 and len(faces.shape) >= 2
                elif isinstance(faces, list):
                    has_faces = len(faces) > 0
                else:
                    # ê¸°íƒ€ íƒ€ì…ì˜ ê²½ìš° ê¸¸ì´ í™•ì¸
                    try:
                        has_faces = len(faces) > 0
                    except:
                        has_faces = False
            
            if has_faces:
                logger.debug(f"ğŸ” CREATE_SPLIT: Frame {frame_idx} - {len(faces)}ê°œ ì–¼êµ´ ê²€ì¶œë¨, ì„ë² ë”© ì¶”ì¶œ ì‹œì‘...")
                for face_idx, face in enumerate(faces):
                    try:
                        # ì–¼êµ´ í¬ë¡­ í›„ ì„ë² ë”© ì¶”ì¶œ
                        face_crop = extract_face_for_embedding(frame, face)
                        if face_crop is not None:
                            with torch.no_grad():
                                # GPU/CPU ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ì¸
                                if hasattr(face_crop, 'device') and face_crop.device != facenet.device:
                                    face_crop = face_crop.to(facenet.device)
                                
                                embedding = facenet(face_crop.unsqueeze(0)).detach().cpu().numpy().flatten()
                                face_embeddings.append(embedding)
                                logger.debug(f"ğŸ” CREATE_SPLIT: Frame {frame_idx} - ì–¼êµ´ {face_idx} ì„ë² ë”© ì¶”ì¶œ ì„±ê³µ")
                        else:
                            face_embeddings.append(None)
                            logger.debug(f"ğŸ” CREATE_SPLIT: Frame {frame_idx} - ì–¼êµ´ {face_idx} í¬ë¡­ ì‹¤íŒ¨")
                    except Exception as e:
                        logger.warning(f"Frame {frame_idx} ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        face_embeddings.append(None)
                
                logger.debug(f"ğŸ¯ CREATE_SPLIT: Frame {frame_idx} - íŠ¸ë˜ì»¤ë¡œ Person ë§¤ì¹­ ì‹œì‘...")
                # ğŸ¯ íŠ¸ë˜ì»¤ë¡œ ì •í™•í•œ Person1/Person2 ë§¤ì¹­ (ì•ˆì „í•œ í˜¸ì¶œ)
                try:
                    person1_face, person2_face = tracker.match_faces_to_persons(faces, face_embeddings)
                except Exception as tracker_error:
                    print(f"âŒ CONSOLE: íŠ¸ë˜ì»¤ ë§¤ì¹­ ì—ëŸ¬ - {tracker_error}")
                    person1_face, person2_face = None, None
                logger.debug(f"ğŸ¯ CREATE_SPLIT: Frame {frame_idx} - ë§¤ì¹­ ê²°ê³¼: Person1={person1_face is not None}, Person2={person2_face is not None}")
            else:
                logger.debug(f"ğŸ” CREATE_SPLIT: Frame {frame_idx} - ì–¼êµ´ ê²€ì¶œë˜ì§€ ì•ŠìŒ")
                person1_face, person2_face = None, None
            
            # Person1 ì²˜ë¦¬ (ì¢Œì¸¡ 960x1080)
            if person1_face is not None:
                logger.debug(f"âœ‚ï¸ CREATE_SPLIT: Frame {frame_idx} - Person1 ì–¼êµ´ í¬ë¡­ ì‹œì‘...")
                person1_crop = create_centered_face_crop(frame, person1_face, 960, 1080)
                split_frame[:, :960] = person1_crop
                person1_detected_count += 1
                logger.debug(f"âœ‚ï¸ CREATE_SPLIT: Frame {frame_idx} - Person1 í¬ë¡­ ì™„ë£Œ")
            # Person1ì´ ì—†ì–´ë„ ë¹ˆ ì˜ì—­ ìœ ì§€ (ê²€ì€ìƒ‰)
            
            # Person2 ì²˜ë¦¬ (ìš°ì¸¡ 960x1080)
            if person2_face is not None:
                logger.debug(f"âœ‚ï¸ CREATE_SPLIT: Frame {frame_idx} - Person2 ì–¼êµ´ í¬ë¡­ ì‹œì‘...")
                person2_crop = create_centered_face_crop(frame, person2_face, 960, 1080)
                split_frame[:, 960:] = person2_crop
                person2_detected_count += 1
                logger.debug(f"âœ‚ï¸ CREATE_SPLIT: Frame {frame_idx} - Person2 í¬ë¡­ ì™„ë£Œ")
            # Person2ê°€ ì—†ì–´ë„ ë¹ˆ ì˜ì—­ ìœ ì§€ (ê²€ì€ìƒ‰)
            
        except Exception as e:
            logger.warning(f"Frame {frame_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒí•´ë„ ë¹ˆ í”„ë ˆì„ ì¶œë ¥ (ì—°ì†ì„± ë³´ì¥)
        
        # âœ… ëª¨ë“  í”„ë ˆì„ ì¶œë ¥ (1920x1080 ë³´ì¥)
        logger.debug(f"ğŸ’¾ CREATE_SPLIT: Frame {frame_idx} - ë¹„ë””ì˜¤ ì¶œë ¥...")
        out.write(split_frame)
        processed_frames += 1
        frame_idx += 1
        
        # ì§„í–‰ë¥  ë¡œê·¸ (1000í”„ë ˆì„ë§ˆë‹¤)
        if frame_idx % 1000 == 0:
            progress = (frame_idx / total_frames) * 100
            logger.info(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {progress:.1f}% ({frame_idx}/{total_frames})")
    
    logger.debug("ğŸ”š CREATE_SPLIT: í”„ë ˆì„ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ, ë¦¬ì†ŒìŠ¤ ì •ë¦¬...")
    cap.release()
    out.release()
    logger.debug("ğŸ”š CREATE_SPLIT: ë¹„ë””ì˜¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    
    # ê²°ê³¼ í†µê³„
    person1_ratio = (person1_detected_count / processed_frames) * 100
    person2_ratio = (person2_detected_count / processed_frames) * 100
    
    # íŠ¸ë˜í‚¹ í†µê³„ ìˆ˜ì§‘
    tracking_stats = tracker.get_tracking_stats()
    
    # ê²°ê³¼ í†µê³„ ê³„ì‚°
    person1_ratio = (person1_detected_count / processed_frames) * 100 if processed_frames > 0 else 0
    person2_ratio = (person2_detected_count / processed_frames) * 100 if processed_frames > 0 else 0
    
    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
    overall_detection_rate = ((person1_detected_count + person2_detected_count) / (processed_frames * 2)) * 100
    person_balance = abs(person1_ratio - person2_ratio)  # ê· í˜•ë„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    
    # ì¢…í•© í’ˆì§ˆ í‰ê°€
    if overall_detection_rate >= 80 and person_balance <= 30:
        quality_status = "ğŸŸ¢ ìš°ìˆ˜"
    elif overall_detection_rate >= 60 and person_balance <= 50:
        quality_status = "ğŸŸ¡ ì–‘í˜¸"
    else:
        quality_status = "ğŸ”´ ê°œì„  í•„ìš”"
    
    logger.success(f"ë¶„í•  í™”ë©´ ì˜ìƒ ìƒì„± ì™„ë£Œ - í’ˆì§ˆ: {quality_status}")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
    logger.info(f"  â€¢ ì²˜ë¦¬ëœ í”„ë ˆì„: {processed_frames:,}")
    logger.info(f"  â€¢ ì „ì²´ ê²€ì¶œë¥ : {overall_detection_rate:.1f}%")
    logger.info(f"  â€¢ Person1(ì¢Œì¸¡) ê²€ì¶œë¥ : {person1_ratio:.1f}% ({person1_detected_count:,}/{processed_frames:,})")
    logger.info(f"  â€¢ Person2(ìš°ì¸¡) ê²€ì¶œë¥ : {person2_ratio:.1f}% ({person2_detected_count:,}/{processed_frames:,})")
    logger.info(f"  â€¢ Person ê· í˜•ë„: {person_balance:.1f}% (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    
    logger.info(f"ğŸ¯ íŠ¸ë˜í‚¹ ì„±ëŠ¥:")
    logger.info(f"  â€¢ Person1 íŠ¸ë˜í‚¹ ì‹¤íŒ¨: {tracking_stats['person1_lost_frames']:,}í”„ë ˆì„")
    logger.info(f"  â€¢ Person2 íŠ¸ë˜í‚¹ ì‹¤íŒ¨: {tracking_stats['person2_lost_frames']:,}í”„ë ˆì„") 
    logger.info(f"  â€¢ ì„ë² ë”© ìºì‹œ í¬ê¸°: {tracking_stats['cache_size']}")
    logger.info(f"  â€¢ Person1 ê¸°ì¤€ ì„ë² ë”©: {'âœ…' if tracking_stats['person1_has_embedding'] else 'âŒ'}")
    logger.info(f"  â€¢ Person2 ê¸°ì¤€ ì„ë² ë”©: {'âœ…' if tracking_stats['person2_has_embedding'] else 'âŒ'}")
    
    logger.info(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_path}")
    
    # í’ˆì§ˆ ê°œì„  ì œì•ˆ
    if person_balance > 50:
        logger.warning("âš ï¸ Person ë¶ˆê· í˜•ì´ í½ë‹ˆë‹¤. assign_persons_hybrid() ë¡œì§ ê²€í†  í•„ìš”")
    if overall_detection_rate < 60:
        logger.warning("âš ï¸ ì „ì²´ ê²€ì¶œë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì„ë² ë”© ì„ê³„ê°’ ì¡°ì • ê³ ë ¤")
    if tracking_stats['person1_lost_frames'] > processed_frames * 0.1 or tracking_stats['person2_lost_frames'] > processed_frames * 0.1:
        logger.warning("âš ï¸ íŠ¸ë˜í‚¹ ì‹¤íŒ¨ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ìœ„ì¹˜ ì„ê³„ê°’ ì¡°ì • ê³ ë ¤")
    
    logger.debug("ğŸ‰ CREATE_SPLIT: create_split_screen_video() í•¨ìˆ˜ ì™„ë£Œ")
    
    # ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜
    return {
        'processed_frames': processed_frames,
        'total_frames': total_frames,
        'person1_detected_count': person1_detected_count,
        'person2_detected_count': person2_detected_count,
        'person1_ratio': person1_ratio,
        'person2_ratio': person2_ratio,
        'overall_detection_rate': overall_detection_rate,
        'person_balance': person_balance,
        'quality_status': quality_status,
        'tracking_stats': tracking_stats,
        'output_path': output_path
    }


def extract_face_for_embedding(frame, face_box):
    """
    ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œìš© í¬ë¡­ ìƒì„±
    
    Args:
        frame: ì›ë³¸ í”„ë ˆì„
        face_box: ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ [x1, y1, x2, y2]

    Returns:
        torch.Tensor: 160x160 í¬ê¸°ì˜ ì •ê·œí™”ëœ ì–¼êµ´ ì´ë¯¸ì§€ í…ì„œ (GPUì— ìœ„ì¹˜)
    """
    import cv2
    import torch
    from torchvision import transforms
    from src.face_tracker.config import DEVICE
    
    try:
        x1, y1, x2, y2 = face_box.astype(int)
        
        # ì–¼êµ´ ì˜ì—­ í¬ë¡­
        face_crop = frame[y1:y2, x1:x2]
        
        # 160x160ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (FaceNet ì…ë ¥ í¬ê¸°)
        face_resized = cv2.resize(face_crop, (160, 160))
        
        # RGB ë³€í™˜ ë° ì •ê·œí™”
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        
        # PIL Imageë¡œ ë³€í™˜ í›„ í…ì„œ ë³€í™˜
        from PIL import Image
        face_pil = Image.fromarray(face_rgb)
        
        # í…ì„œ ë³€í™˜ ë° ì •ê·œí™” (-1 ~ 1 ë²”ìœ„)
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        face_tensor = transform(face_pil)
        
        # GPUë¡œ ì´ë™ (CUDA ê°€ìš©ì„± í™•ì¸) - ì•ˆì „í•œ íƒ€ì… ê²€ì‚¬
        if torch.cuda.is_available():
            device_str = str(DEVICE)
            if 'cuda' in device_str:
                face_tensor = face_tensor.to(DEVICE)
        
        return face_tensor
    
    except Exception as e:
        print(f"âŒ CONSOLE: extract_face_for_embedding ì—ëŸ¬ - {e}")
        return None


def create_centered_face_crop(frame, face_box, target_width, target_height):
    """
    ì–¼êµ´ì„ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¨ í¬ë¡­ ìƒì„±
    
    Args:
        frame: ì›ë³¸ í”„ë ˆì„
        face_box: ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ [x1, y1, x2, y2]
        target_width: ëª©í‘œ ë„ˆë¹„
        target_height: ëª©í‘œ ë†’ì´
    
    Returns:
        numpy.ndarray: ì¤‘ì•™ ì •ë ¬ëœ í¬ë¡­ ì´ë¯¸ì§€
    """
    import cv2
    h, w = frame.shape[:2]
    x1, y1, x2, y2 = face_box.astype(int)
    
    # ì–¼êµ´ ì¤‘ì‹¬ì 
    face_center_x = (x1 + x2) // 2
    face_center_y = (y1 + y2) // 2
    
    # ì–¼êµ´ í¬ê¸° ê¸°ë°˜ í¬ë¡­ ì˜ì—­ ê³„ì‚°
    face_width = x2 - x1
    face_height = y2 - y1
    face_size = max(face_width, face_height)
    
    # ì—¬ìœ  ê³µê°„ì„ ê³ ë ¤í•œ í¬ë¡­ í¬ê¸°
    crop_size = int(face_size * 2.5)  # ì–¼êµ´ì˜ 2.5ë°° í¬ê¸°ë¡œ í¬ë¡­
    
    # í¬ë¡­ ì˜ì—­ ê³„ì‚°
    crop_x1 = max(0, face_center_x - crop_size // 2)
    crop_y1 = max(0, face_center_y - crop_size // 2)
    crop_x2 = min(w, face_center_x + crop_size // 2)
    crop_y2 = min(h, face_center_y + crop_size // 2)
    
    # í¬ë¡­ ì¶”ì¶œ
    cropped = frame[crop_y1:crop_y2, crop_x1:crop_x2]
    
    # ëª©í‘œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    resized = cv2.resize(cropped, (target_width, target_height))
    
    return resized