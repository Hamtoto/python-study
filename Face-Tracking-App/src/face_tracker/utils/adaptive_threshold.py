"""
ë™ì  ì„ê³„ê°’ ìë™ ê³„ì‚° ëª¨ë“ˆ
ì´ë¯¸ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ì„ë² ë”© ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ê° ë¹„ë””ì˜¤ì— ìµœì í™”ëœ ì„ê³„ê°’ì„ ì‹¤ì‹œê°„ ê³„ì‚°
"""
import numpy as np
import torch
from typing import Dict, List, Tuple, Optional, Any
from src.face_tracker.utils.similarity import calculate_face_similarity
from src.face_tracker.utils.logging import logger


def calculate_optimal_threshold(
    embeddings_dict: Dict[int, List[torch.Tensor]], 
    use_l2_norm: bool = True,
    min_same_samples: int = 5,
    min_different_samples: int = 5,
    safety_range: Tuple[float, float] = (0.6, 0.95)
) -> Tuple[Optional[float], str, Dict[str, Any]]:
    """
    ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆëŠ” ì„ë² ë”©ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì‹¤ì‹œê°„ ê³„ì‚°
    
    Args:
        embeddings_dict: {person_id: [embeddings]} í˜•íƒœì˜ ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        use_l2_norm: L2 ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        min_same_samples: ê°™ì€ ì‚¬ëŒ ìœ ì‚¬ë„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        min_different_samples: ë‹¤ë¥¸ ì‚¬ëŒ ìœ ì‚¬ë„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        safety_range: ì•ˆì „ ë²”ìœ„ (min_threshold, max_threshold)
        
    Returns:
        tuple: (optimal_threshold, confidence, statistics)
    """
    
    if len(embeddings_dict) < 2:
        return None, "insufficient_ids", {}
    
    same_person_similarities = []
    different_person_similarities = []
    
    person_ids = list(embeddings_dict.keys())
    
    # 1. ê°™ì€ ì‚¬ëŒë¼ë¦¬ ìœ ì‚¬ë„ ìˆ˜ì§‘ (ID ë‚´ë¶€)
    for person_id, embedding_list in embeddings_dict.items():
        if len(embedding_list) >= 2:
            for i in range(len(embedding_list)):
                for j in range(i + 1, len(embedding_list)):
                    try:
                        similarity = calculate_face_similarity(
                            embedding_list[i], 
                            embedding_list[j], 
                            use_l2_norm=use_l2_norm
                        )
                        same_person_similarities.append(similarity)
                    except Exception as e:
                        logger.warning(f"ê°™ì€ ì‚¬ëŒ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                        continue
    
    # 2. ë‹¤ë¥¸ ì‚¬ëŒë¼ë¦¬ ìœ ì‚¬ë„ ìˆ˜ì§‘ (ID ê°„)
    for i in range(len(person_ids)):
        for j in range(i + 1, len(person_ids)):
            try:
                # ê° IDì˜ ëŒ€í‘œ ì„ë² ë”© (ì²« ë²ˆì§¸) ì‚¬ìš©
                emb1 = embeddings_dict[person_ids[i]][0]
                emb2 = embeddings_dict[person_ids[j]][0]
                
                similarity = calculate_face_similarity(emb1, emb2, use_l2_norm=use_l2_norm)
                different_person_similarities.append(similarity)
            except Exception as e:
                logger.warning(f"ë‹¤ë¥¸ ì‚¬ëŒ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                continue
    
    # 3. ë°ì´í„° ì¶©ë¶„ì„± ê²€ì¦
    if (len(same_person_similarities) < min_same_samples or 
        len(different_person_similarities) < min_different_samples):
        return None, "insufficient_samples", {
            'same_person_count': len(same_person_similarities),
            'different_person_count': len(different_person_similarities)
        }
    
    # 4. í†µê³„ ë¶„ì„
    same_array = np.array(same_person_similarities)
    different_array = np.array(different_person_similarities)
    
    same_mean = np.mean(same_array)
    same_std = np.std(same_array)
    same_min = np.min(same_array)
    same_max = np.max(same_array)
    
    different_mean = np.mean(different_array)
    different_std = np.std(different_array)
    different_min = np.min(different_array)
    different_max = np.max(different_array)
    
    # 5. ìµœì  ì„ê³„ê°’ ê³„ì‚°
    if same_min > different_max:
        # ëª…í™•í•˜ê²Œ êµ¬ë¶„ ê°€ëŠ¥í•œ ê²½ìš°
        optimal_threshold = (same_min + different_max) / 2
        confidence = "high"
        confidence_score = 0.9
    elif same_mean - same_std > different_mean + different_std:
        # ë¶„í¬ê°€ ì•½ê°„ ê²¹ì¹˜ì§€ë§Œ êµ¬ë¶„ ê°€ëŠ¥
        optimal_threshold = (same_mean - same_std + different_mean + different_std) / 2
        confidence = "medium"
        confidence_score = 0.7
    else:
        # ë¶„í¬ê°€ ë§ì´ ê²¹ì¹¨, ë³´ìˆ˜ì  ì ‘ê·¼
        # ê°™ì€ ì‚¬ëŒ ë¶„í¬ì˜ í•˜ìœ„ 15% ì§€ì  ì‚¬ìš©
        optimal_threshold = np.percentile(same_array, 15)
        confidence = "low"
        confidence_score = 0.5
    
    # 6. ì•ˆì „ ë²”ìœ„ ì œí•œ
    optimal_threshold = np.clip(optimal_threshold, safety_range[0], safety_range[1])
    
    # 7. í†µê³„ ì •ë³´ ìƒì„±
    statistics = {
        'same_person': {
            'count': len(same_person_similarities),
            'mean': float(same_mean),
            'std': float(same_std),
            'min': float(same_min),
            'max': float(same_max),
            'percentile_10': float(np.percentile(same_array, 10)),
            'percentile_90': float(np.percentile(same_array, 90))
        },
        'different_person': {
            'count': len(different_person_similarities),
            'mean': float(different_mean),
            'std': float(different_std),
            'min': float(different_min),
            'max': float(different_max),
            'percentile_10': float(np.percentile(different_array, 10)),
            'percentile_90': float(np.percentile(different_array, 90))
        },
        'separation_quality': float(same_min - different_max),  # ì–‘ìˆ˜ë©´ ì™„ì „ ë¶„ë¦¬
        'confidence_score': confidence_score,
        'total_ids': len(embeddings_dict),
        'total_embeddings': sum(len(embs) for embs in embeddings_dict.values())
    }
    
    return optimal_threshold, confidence, statistics


def log_threshold_optimization(
    optimal_threshold: Optional[float],
    confidence: str,
    statistics: Dict[str, Any],
    current_threshold: float,
    use_l2_norm: bool
) -> None:
    """ì„ê³„ê°’ ìµœì í™” ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥"""
    
    if optimal_threshold is None:
        logger.warning(f"ğŸ”§ ì„ê³„ê°’ ìµœì í™” ì‹¤íŒ¨: {confidence}")
        if 'same_person_count' in statistics:
            logger.info(f"   - ê°™ì€ ì‚¬ëŒ ìƒ˜í”Œ: {statistics['same_person_count']}ê°œ")
            logger.info(f"   - ë‹¤ë¥¸ ì‚¬ëŒ ìƒ˜í”Œ: {statistics['different_person_count']}ê°œ")
        logger.info(f"   - ê¸°ë³¸ê°’ ì‚¬ìš©: {current_threshold:.3f}")
        return
    
    logger.info("=" * 60)
    logger.info("ğŸ”§ ì‹¤ì‹œê°„ ì„ê³„ê°’ ìµœì í™”")
    logger.info("=" * 60)
    
    # ê¸°ë³¸ ì •ë³´
    logger.info(f"ğŸ“Š ì„ë² ë”© ë°ì´í„°: {statistics['total_ids']}ê°œ ID, "
                f"ì´ {statistics['total_embeddings']}ê°œ ì„ë² ë”©")
    
    # ê°™ì€ ì‚¬ëŒ í†µê³„
    same_stats = statistics['same_person']
    logger.info(f"ğŸ‘¥ ê°™ì€ ì‚¬ëŒ ìœ ì‚¬ë„: {same_stats['mean']:.3f} Â± {same_stats['std']:.3f} "
                f"({same_stats['count']}ê°œ)")
    logger.info(f"   ë²”ìœ„: {same_stats['min']:.3f} ~ {same_stats['max']:.3f}")
    
    # ë‹¤ë¥¸ ì‚¬ëŒ í†µê³„
    diff_stats = statistics['different_person']
    logger.info(f"ğŸš« ë‹¤ë¥¸ ì‚¬ëŒ ìœ ì‚¬ë„: {diff_stats['mean']:.3f} Â± {diff_stats['std']:.3f} "
                f"({diff_stats['count']}ê°œ)")
    logger.info(f"   ë²”ìœ„: {diff_stats['min']:.3f} ~ {diff_stats['max']:.3f}")
    
    # ë¶„ë¦¬ í’ˆì§ˆ
    separation = statistics['separation_quality']
    if separation > 0:
        logger.info(f"ğŸ¯ ë¶„ë¦¬ í’ˆì§ˆ: ì™„ì „ ë¶„ë¦¬ (+{separation:.3f})")
    else:
        logger.info(f"âš ï¸ ë¶„ë¦¬ í’ˆì§ˆ: ë¶„í¬ ê²¹ì¹¨ ({separation:.3f})")
    
    # ìµœì í™” ê²°ê³¼
    improvement = abs(optimal_threshold - current_threshold) / current_threshold * 100
    logger.info(f"ğŸ’¡ ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f} (ì‹ ë¢°ë„: {confidence})")
    logger.info(f"ğŸ“ ê¸°ì¡´ â†’ ì‹ ê·œ: {current_threshold:.3f} â†’ {optimal_threshold:.3f} "
                f"({improvement:+.1f}%)")
    logger.info(f"ğŸ” L2 ì •ê·œí™”: {'ì ìš©' if use_l2_norm else 'ë¯¸ì ìš©'}")
    logger.info("=" * 60)


def should_apply_adaptive_threshold(
    confidence: str, 
    optimal_threshold: float, 
    current_threshold: float,
    min_confidence_level: str = "medium",
    min_improvement_percent: float = 2.0
) -> bool:
    """
    ë™ì  ì„ê³„ê°’ì„ ì ìš©í• ì§€ ì—¬ë¶€ ê²°ì •
    
    Args:
        confidence: ì‹ ë¢°ë„ ë ˆë²¨ ("high", "medium", "low")
        optimal_threshold: ê³„ì‚°ëœ ìµœì  ì„ê³„ê°’
        current_threshold: í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ê³„ê°’
        min_confidence_level: ìµœì†Œ ì‹ ë¢°ë„ ë ˆë²¨
        min_improvement_percent: ìµœì†Œ ê°œì„ ìœ¨ (%)
        
    Returns:
        bool: ì ìš© ì—¬ë¶€
    """
    confidence_levels = {"high": 3, "medium": 2, "low": 1}
    
    # ì‹ ë¢°ë„ í™•ì¸
    if confidence_levels.get(confidence, 0) < confidence_levels.get(min_confidence_level, 2):
        return False
    
    # ê°œì„ ìœ¨ í™•ì¸
    improvement = abs(optimal_threshold - current_threshold) / current_threshold * 100
    if improvement < min_improvement_percent:
        return False
    
    return True